{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from setting.db import SessionLocal\n",
    "\n",
    "from graph.graph_knowledge_base import GraphKnowledgeBase\n",
    "\n",
    "gkb = GraphKnowledgeBase(\"entities_150001\", \"relationships_150001\", \"chunks_150001\")\n",
    "session = SessionLocal()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"what is the latest version of tidb\"\n",
    "model_kwargs = {\n",
    "    \"num_ctx\": 8092,\n",
    "    \"num_gpu\": 40,\n",
    "    \"num_predict\": 10000,\n",
    "    \"temperature\": 0.1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis Result:\n",
      "Reasoning: To deeply analyze the query 'what is the latest version of tidb' using first principles thinking, we start by understanding the user's fundamental need. The user is seeking information about the current state of a specific software product, TiDB. This indicates a need for up-to-date information, which is crucial in the context of software development and deployment. The fundamental components needed to answer this question include identifying what TiDB is, understanding how software versions are tracked and released, and determining where the most reliable and current information about TiDB's version can be found. The user might be asking this question for several reasons: they could be considering an upgrade, evaluating compatibility with other systems, or simply staying informed about the software they use. We would know if we've fully answered the question by providing the most recent version number of TiDB, along with a source or method for verifying this information, ensuring the user can trust the accuracy and recency of the data provided.\n",
      "Intent:\n",
      "    Action: retrieve\n",
      "    Target: latest version information\n",
      "    Context: TiDB software\n",
      "Initial Queries:\n",
      "    current TiDB version\n",
      "    latest TiDB release\n",
      "    TiDB version history\n"
     ]
    }
   ],
   "source": [
    "from llm_inference.base import LLMInterface\n",
    "from graph.query_analyzer import DeepUnderstandingAnalyzer\n",
    "\n",
    "llm_client = LLMInterface(\"openai\", \"gpt-4o\")\n",
    "analyzer = DeepUnderstandingAnalyzer(llm_client)\n",
    "analysis_res = analyzer.perform(query)\n",
    "print(analysis_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graph.knowledge_synthesizer import KnowledgeSynthesizer, SearchAction\n",
    "\n",
    "synthesizer = KnowledgeSynthesizer(llm_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_history = []\n",
    "current_findings = []\n",
    "docs = {}\n",
    "\n",
    "next_actions = [SearchAction(\n",
    "    tool=\"retrieve_knowledge\",\n",
    "    query=a\n",
    ") for a in analysis_res.initial_queries]\n",
    "\n",
    "reasoning = analysis_res.reasoning\n",
    "queries = analysis_res.initial_queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM generation failed: llm_inference.base.BaseLLMProvider._retry_with_exponential_backoff() got multiple values for keyword argument 'temperature'\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "llm_inference.base.BaseLLMProvider._retry_with_exponential_backoff() got multiple values for keyword argument 'temperature'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m current_time \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow()\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCurrent Time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent_time\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mThe question: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mThe Reasoning: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreasoning\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mSo the answer is:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 7\u001b[0m answer_1 \u001b[38;5;241m=\u001b[39m \u001b[43mllm_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m answer_1\n",
      "File \u001b[0;32m~/Work/graph_toolkit/llm_inference/base.py:258\u001b[0m, in \u001b[0;36mLLMInterface.generate\u001b[0;34m(self, prompt, context, **kwargs)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    257\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLLM generation failed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 258\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[0;32m~/Work/graph_toolkit/llm_inference/base.py:255\u001b[0m, in \u001b[0;36mLLMInterface.generate\u001b[0;34m(self, prompt, context, **kwargs)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate\u001b[39m(\n\u001b[1;32m    252\u001b[0m     \u001b[38;5;28mself\u001b[39m, prompt: \u001b[38;5;28mstr\u001b[39m, context: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    253\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m    254\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 255\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprovider\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    257\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLLM generation failed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Work/graph_toolkit/llm_inference/base.py:77\u001b[0m, in \u001b[0;36mOpenAIProvider.generate\u001b[0;34m(self, prompt, context, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate\u001b[39m(\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28mself\u001b[39m, prompt: \u001b[38;5;28mstr\u001b[39m, context: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m     75\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m     76\u001b[0m     full_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcontext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mprompt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m context \u001b[38;5;28;01melse\u001b[39;00m prompt\n\u001b[0;32m---> 77\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retry_with_exponential_backoff(\n\u001b[1;32m     78\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mcreate,\n\u001b[1;32m     79\u001b[0m         model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel,\n\u001b[1;32m     80\u001b[0m         messages\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m     81\u001b[0m             {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are a helpful assistant.\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m     82\u001b[0m             {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: full_prompt},\n\u001b[1;32m     83\u001b[0m         ],\n\u001b[1;32m     84\u001b[0m         temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m     85\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m     86\u001b[0m     )\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent\u001b[38;5;241m.\u001b[39mstrip()\n",
      "\u001b[0;31mTypeError\u001b[0m: llm_inference.base.BaseLLMProvider._retry_with_exponential_backoff() got multiple values for keyword argument 'temperature'"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "# Fact 1: \n",
    "current_time = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "prompt = f\"Current Time: {current_time}.\\n\\nThe question: {query}.\\n\\nThe Reasoning: {reasoning}.\\n\\nSo the answer is:\"\n",
    "\n",
    "answer_1 = llm_client.generate(prompt)\n",
    "answer_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "knowledge_retrieved = {}\n",
    "for action in next_actions:\n",
    "    if action.tool == 'retrieve_knowledge':\n",
    "        data = gkb.retrieve_graph_data(session, action.query)\n",
    "    elif action.tool == 'retrieve_neighbors':\n",
    "        data = gkb.retrieve_neighbors(session, action.entity_ids, action.query)\n",
    "    knowledge_retrieved[str(action)] = data\n",
    "    action_history.append(action)\n",
    "\n",
    "knowledge_retrieved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_findings = synthesizer.extract_findings(reasoning, query, knowledge_retrieved, current_findings)\n",
    "current_findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for finding in current_findings:\n",
    "    for quote in finding.source_quotes:\n",
    "        refer_chunks = {}\n",
    "        refer_relationships = {}\n",
    "        doc_link = quote.get('doc_link')\n",
    "        if doc_link is None:\n",
    "            print(f\"doc_link is None for quote: {quote}\")\n",
    "        \n",
    "        for chunk_id in quote.get('chunk_ids', []):\n",
    "            is_found = False\n",
    "            for action, data in knowledge_retrieved.items():\n",
    "                for ck in data.get('chunks', []):\n",
    "                    if ck.get('id') == chunk_id:\n",
    "                        refer_chunks[chunk_id] = ck\n",
    "                        is_found = True\n",
    "                        break\n",
    "                if is_found:\n",
    "                    break\n",
    "\n",
    "        for relationship_id in quote.get('relationship_ids', []):\n",
    "            is_found = False\n",
    "            for action, data in knowledge_retrieved.items():\n",
    "                for rel in data.get('relationships', []):\n",
    "                    if rel.get('id') == relationship_id:\n",
    "                        refer_relationships[relationship_id] = rel\n",
    "                        is_found = True\n",
    "                        break\n",
    "                if is_found:\n",
    "                    break\n",
    "\n",
    "        if doc_link not in docs:\n",
    "            docs[doc_link] = {\n",
    "                'refer_chunks': refer_chunks,\n",
    "                'refer_relationships': refer_relationships,\n",
    "            }\n",
    "        else:\n",
    "            docs[doc_link]['refer_chunks'].update(refer_chunks)\n",
    "            docs[doc_link]['refer_relationships'].update(refer_relationships)\n",
    "\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_res = synthesizer.evaluate_findings(query, reasoning, current_findings, docs, action_history)\n",
    "eval_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_actions = []\n",
    "if eval_res.is_sufficient is False:\n",
    "    print(f\"Not sufficient, re-synthesizing...\\n    Reasoning: {eval_res.reasoning}\\n    missing_aspects: {eval_res.missing_aspects}\")\n",
    "    for action in eval_res.next_actions:\n",
    "        next_actions.append(action)\n",
    "next_actions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graph",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
