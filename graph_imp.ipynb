{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from setting.db import SessionLocal\n",
    "\n",
    "from graph.graph_knowledge_base import GraphKnowledgeBase, SearchAction\n",
    "\n",
    "gkb = GraphKnowledgeBase(\"entities_150001\", \"relationships_150001\", \"chunks_150001\")\n",
    "session = SessionLocal()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Where can I find the latest version of tidb\"\n",
    "model_kwargs = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis Result:\n",
      "Reasoning: To deeply analyze the query 'How to find the latest version of tidb' using first principles thinking, we start by understanding the user's fundamental need. The user is likely seeking the most recent version of TiDB, a distributed SQL database, to ensure they have access to the latest features, improvements, and security updates. This need arises from the broader context of software maintenance and development, where staying updated is crucial for performance, security, and compatibility. The fundamental components needed to answer this question include identifying reliable sources where TiDB versions are published, understanding how versioning works, and knowing how to verify the authenticity and recency of the information. The user is asking this question because they want to ensure they are using the most current and supported version of TiDB, which is essential for optimal operation and support. We would know if we've fully answered the question if the user can confidently identify and access the latest version of TiDB from a reliable source.\n",
      "Intent:\n",
      "    Action: identify\n",
      "    Target: latest version of TiDB\n",
      "    Context: software version management and updates\n",
      "Initial Queries:\n",
      "    TiDB latest version release\n",
      "    current TiDB version\n",
      "    TiDB version history\n"
     ]
    }
   ],
   "source": [
    "from llm_inference.base import LLMInterface\n",
    "from graph.query_analyzer import DeepUnderstandingAnalyzer\n",
    "\n",
    "llm_client = LLMInterface(\"openai\", \"gpt-4o\")\n",
    "analyzer = DeepUnderstandingAnalyzer(llm_client)\n",
    "analysis_res = analyzer.perform(query)\n",
    "print(analysis_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_history = []\n",
    "current_findings = []\n",
    "docs = {}\n",
    "\n",
    "next_actions = [SearchAction(\n",
    "    tool=\"retrieve_documents\",\n",
    "    query=a\n",
    ") for a in analysis_res.initial_queries]\n",
    "\n",
    "reasoning = analysis_res.reasoning\n",
    "queries = analysis_res.initial_queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{15758: DocumentData(id=15758, chunks={}, content='---\\ntitle: TiDB 1.0.2 Release Notes\\nsummary: TiDB 1.0.2 was released on November 13, 2017. Updates include optimized cost estimation for index point query, support for Alter Table Add Column syntax, and improved query optimization. Placement Driver (PD) scheduling stability was enhanced, and TiKV now supports table splitting and limits key length to 4 KB. Other improvements include more accurate read traffic statistics and bug fixes for LIKE behavior and do_div_mod bug.\\n---\\n\\n# TiDB 1.0.2 Release Notes\\n\\nOn November 13, 2017, TiDB 1.0.2 is released with the following updates:\\n\\n## TiDB\\n\\n- Optimize the cost estimation of index point query\\n- Support the `Alter Table Add Column (ColumnDef ColumnPosition)` syntax\\n- Optimize the queries whose `where` conditions are contradictory\\n- Optimize the `Add Index` operation to rectify the progress and reduce repetitive operations\\n- Optimize the `Index Look Join` operator to accelerate the query speed for small data size\\n- Fix the issue with prefix index judgment\\n\\n## Placement Driver (PD)\\n\\n- Improve the stability of scheduling under exceptional situations\\n\\n## TiKV\\n\\n- Support splitting table to ensure one region does not contain data from multiple tables\\n- Limit the length of a key to be no more than 4 KB\\n- More accurate read traffic statistics\\n- Implement deep protection on the coprocessor stack\\n- Fix the `LIKE` behavior and the `do_div_mod` bug\\n', doc_link='https://docs.pingcap.com/tidb/v8.1/release-1.0.2'),\n",
       " 15911: DocumentData(id=15911, chunks={}, content='---\\ntitle: TiDB 1.0.3 Release Notes\\nsummary: TiDB 1.0.3 was released on November 28, 2017. Updates include performance optimization, new configuration options, and bug fixes. PD now supports adding more schedulers using API, and TiKV has fixed deadlock and leader value issues. To upgrade from 1.0.2 to 1.0.3, follow the rolling upgrade order of PD, TiKV, and TiDB.\\n---\\n\\n# TiDB 1.0.3 Release Notes\\n\\nOn November 28, 2017, TiDB 1.0.3 is released with the following updates:\\n\\n## TiDB\\n\\n- [Optimize the performance in transaction conflicts scenario](https://github.com/pingcap/tidb/pull/5051)\\n- [Add the `TokenLimit` option in the config file](https://github.com/pingcap/tidb/pull/5107)\\n- [Output the default database in slow query logs](https://github.com/pingcap/tidb/pull/5107)\\n- [Remove the DDL statement from query duration metrics](https://github.com/pingcap/tidb/pull/5107)\\n- [Optimize the query cost estimation](https://github.com/pingcap/tidb/pull/5140)\\n- [Fix the index prefix issue when creating tables](https://github.com/pingcap/tidb/pull/5149)\\n- [Support pushing down the expressions for the Float type to TiKV](https://github.com/pingcap/tidb/pull/5153)\\n- [Fix the issue that it is slow to add index for tables with discrete integer primary index](https://github.com/pingcap/tidb/pull/5155)\\n- [Reduce the unnecessary statistics updates](https://github.com/pingcap/tidb/pull/5164)\\n- [Fix a potential issue during the transaction retry](https://github.com/pingcap/tidb/pull/5219)\\n\\n## PD\\n\\n- Support adding more types of schedulers using API\\n\\n## TiKV\\n\\n- Fix the deadlock issue with the PD client\\n- Fix the issue that the wrong leader value is prompted for `NotLeader`\\n- Fix the issue that the chunk size is too large in the coprocessor\\n\\nTo upgrade from 1.0.2 to 1.0.3, follow the rolling upgrade order of PD -> TiKV -> TiDB.\\n', doc_link='https://docs.pingcap.com/tidb/v8.1/release-1.0.3'),\n",
       " 15909: DocumentData(id=15909, chunks={}, content=\"---\\ntitle: TiDB 2.1.16 Release Notes\\nsummary: TiDB 2.1.16 was released on August 15, 2019. It includes various fixes and improvements to the SQL optimizer, SQL execution engine, server, DDL, TiKV, TiDB Binlog, TiDB Lightning, and TiDB Ansible. Some notable changes include support for subqueries within SHOW statements, fixing issues with DATE_ADD function, and adding configuration items in Drainer for TiDB Binlog.\\n---\\n\\n# TiDB 2.1.16 Release Notes\\n\\nRelease date: August 15, 2019\\n\\nTiDB version: 2.1.16\\n\\nTiDB Ansible version: 2.1.16\\n\\n## TiDB\\n\\n+ SQL Optimizer\\n    - Fix the issue that row count is estimated inaccurately for the equal condition on the time column [#11526](https://github.com/pingcap/tidb/pull/11526)\\n    - Fix the issue that `TIDB_INLJ` Hint does not take effect or take effect on the specified table [#11361](https://github.com/pingcap/tidb/pull/11361)\\n    - Change the implementation of `NOT EXISTS` in a query from OUTER JOIN to ANTI JOIN to find a more optimized execution plan [#11291](https://github.com/pingcap/tidb/pull/11291)\\n    - Support subqueries within `SHOW` statements, allowing syntaxes such as `SHOW COLUMNS FROM tbl WHERE FIELDS IN (SELECT 'a')` [#11461](https://github.com/pingcap/tidb/pull/11461)\\n    - Fix the issue that the `SELECT … CASE WHEN … ELSE NULL ...` query gets an incorrect result caused by the constant folding optimization [#11441](https://github.com/pingcap/tidb/pull/11441)\\n+ SQL Execution Engine\\n    - Fix the issue that the `DATE_ADD` function gets a wrong result when `INTERVAL` is negative [#11616](https://github.com/pingcap/tidb/pull/11616)\\n    - Fix the issue that the `DATE_ADD` function might get an incorrect result because it performs type conversion wrongly when it accepts an argument of the `FLOAT`, `DOUBLE`, or `DECIMAL` type [#11628](https://github.com/pingcap/tidb/pull/11628)\\n    - Fix the issue that the error message is inaccurate when CAST(JSON AS SIGNED) overflows [#11562](https://github.com/pingcap/tidb/pull/11562)\\n    - Fix the issue that other child nodes are not closed when one child node fails to be closed and returns an error during the process of closing Executor [#11598](https://github.com/pingcap/tidb/pull/11598)\\n    - Support `SPLIT TABLE` statements that return the number of Regions that are successfully split and a finished percentage rather than an error when the scheduling is not finished for Region scatter before the timeout [#11487](https://github.com/pingcap/tidb/pull/11487)\\n    - Make `REGEXP BINARY` function case sensitive to be compatible with MySQL [#11505](https://github.com/pingcap/tidb/pull/11505)\\n    - Fix the issue that `NULL` is not returned correctly because the value of `YEAR` in the `DATE_ADD`/`DATE_SUB` result overflows when it is smaller than 0 or larger than 65535 [#11477](https://github.com/pingcap/tidb/pull/11477)\\n    - Add in the slow query table a `Succ` field that indicates whether the execution succeeds [#11412](https://github.com/pingcap/tidb/pull/11421)\\n    - Fix the MySQL incompatibility issue caused by fetching the current timestamp multiple times when a SQL statement involves calculations of the current time (such as `CURRENT_TIMESTAMP` or `NOW`) [#11392](https://github.com/pingcap/tidb/pull/11392)\\n    - Fix the issue that the AUTO_INCREMENT columns do not handle the FLOAT or DOUBLE type [#11389](https://github.com/pingcap/tidb/pull/11389)\\n    - Fix the issue that `NULL` is not returned correctly when the `CONVERT_TZ` function accepts an invalid argument [#11357](https://github.com/pingcap/tidb/pull/11357)\\n    - Fix the issue that an error is reported by the `PARTITION BY LIST` statement. (Currently only the syntax is supported; when TiDB executes the statement, a regular table is created and a prompting message is provided) [#11236](https://github.com/pingcap/tidb/pull/11236)\\n    - Fix the issue that `Mod(%)`, `Multiple(*)`, and `Minus(-)` operations return an inconsistent `0` result with that in MySQL when there are many decimal digits (such as `select 0.000 % 0.11234500000000000000`) [#11353](https://github.com/pingcap/tidb/pull/11353)\\n+ Server\\n    - Fix the issue that the plugin gets a `NULL` domain when `OnInit` is called back [#11426](https://github.com/pingcap/tidb/pull/11426)\\n    - Fix the issue that the table information in a schema can still be obtained through the HTTP interface after the schema has been deleted [#11586](https://github.com/pingcap/tidb/pull/11586)\\n+ DDL\\n    - Disallow dropping indexes on auto-increment columns to avoid incorrect results of the auto-increment columns caused by this operation [#11402](https://github.com/pingcap/tidb/pull/11402)\\n    - Fix the issue that the character set of the column is not correct when creating and modifying the table with different character sets and collations [#11423](https://github.com/pingcap/tidb/pull/11423)\\n    - Fix the issue that the column schema might get wrong when `alter table ... set default...` and another DDL statement that modifies this column are executed in parallel [#11374](https://github.com/pingcap/tidb/pull/11374)\\n    - Fix the issue that data fails to be backfilled when Generated Column A depends on Generated Column B and A is used to create an index [#11538](https://github.com/pingcap/tidb/pull/11538)\\n    - Speed up `ADMIN CHECK TABLE` operations [#11538](https://github.com/pingcap/tidb/pull/11676)\\n\\n## TiKV\\n\\n+ Support returning an error message when the client accesses a TiKV Region that is being closed [#4820](https://github.com/tikv/tikv/pull/4820)\\n+ Support reverse `raw_scan` and `raw_batch_scan` interfaces [#5148](https://github.com/tikv/tikv/pull/5148)\\n\\n## Tools\\n\\n+ TiDB Binlog\\n    - Add the `ignore-txn-commit-ts` configuration item in Drainer to skip executing some statements in a transaction [#697](https://github.com/pingcap/tidb-binlog/pull/697)\\n    - Add the configuration item check on startup, which stops Pump and Drainer from running and returns an error message when meeting invalid configuration items [#708](https://github.com/pingcap/tidb-binlog/pull/708)\\n    - Add the `node-id` configuration in Drainer to specify Drainer's node ID [#706](https://github.com/pingcap/tidb-binlog/pull/706)\\n+ TiDB Lightning\\n    - Fix the issue that `tikv_gc_life_time` fails to be changed back to its original value when 2 checksums are running at the same time [#224](https://github.com/pingcap/tidb-lightning/pull/224)\\n\\n## TiDB Ansible\\n\\n+ Add the `log4j` configuration file in Spark [#842](https://github.com/pingcap/tidb-ansible/pull/842)\\n+ Update the tispark jar package to v2.1.2 [#863](https://github.com/pingcap/tidb-ansible/pull/863)\\n+ Fix the issue that the Prometheus configuration file is generated in the wrong format when TiDB Binlog uses Kafka or ZooKeeper [#845](https://github.com/pingcap/tidb-ansible/pull/845)\\n+ Fix the bug that PD fails to switch the Leader when executing the `rolling_update.yml` operation [#888](https://github.com/pingcap/tidb-ansible/pull/888)\\n+ Optimize the logic of rolling updating PD nodes - upgrade Followers first and then the Leader - to improve stability [#895](https://github.com/pingcap/tidb-ansible/pull/895)\\n\", doc_link='https://docs.pingcap.com/tidb/v8.1/release-2.1.16'),\n",
       " 15854: DocumentData(id=15854, chunks={}, content='---\\ntitle: TiDB 3.0.0-rc.3 Release Notes\\nsummary: TiDB 3.0.0-rc.3 was released on June 21, 2019, with improvements in stability, usability, features, SQL optimizer, statistics, and execution engine. Fixes and new features were added to TiDB, PD, TiKV, and TiDB Ansible. Notable improvements include automatic loading statistics, manual splitting of table and index regions, and support for pessimistic transactions in TiKV.\\n---\\n\\n# TiDB 3.0.0-rc.3 Release Notes\\n\\nRelease date: June 21, 2019\\n\\nTiDB version: 3.0.0-rc.3\\n\\nTiDB Ansible version: 3.0.0-rc.3\\n\\n## Overview\\n\\nOn June 21, 2019, TiDB 3.0.0-rc.3 is released. The corresponding TiDB Ansible version is 3.0.0-rc.3. Compared with TiDB 3.0.0-rc.2, this release has greatly improved the stability, usability, features, the SQL optimizer, statistics, and the execution engine.\\n\\n## TiDB\\n\\n+ SQL Optimizer\\n    - Remove the feature of collecting virtual generated column statistics [#10629](https://github.com/pingcap/tidb/pull/10629)\\n    - Fix the issue that the primary key constant overflows during point queries [#10699](https://github.com/pingcap/tidb/pull/10699)\\n    - Fix the issue that using uninitialized information in `fast analyze` causes panic [#10691](https://github.com/pingcap/tidb/pull/10691)\\n    - Fix the issue that executing the `create view` statement using `prepare` causes panic because of wrong column information [#10713](https://github.com/pingcap/tidb/pull/10713)\\n    - Fix the issue that the column information is not cloned when handling window functions [#10720](https://github.com/pingcap/tidb/pull/10720)\\n    - Fix the wrong estimation for the selectivity rate of the inner table selection in index join [#10854](https://github.com/pingcap/tidb/pull/10854)\\n    - Support automatic loading statistics when the `stats-lease` variable value is 0 [#10811](https://github.com/pingcap/tidb/pull/10811)\\n\\n+ Execution Engine\\n    - Fix the issue that resources are not correctly released when calling the `Close` function in `StreamAggExec` [#10636](https://github.com/pingcap/tidb/pull/10636)\\n    - Fix the issue that the order of `table_option` and `partition_options` is incorrect in the result of executing the `show create table` statement for partitioned tables [#10689](https://github.com/pingcap/tidb/pull/10689)\\n    - Improve the performance of `admin show ddl jobs` by supporting scanning data in reverse order [#10687](https://github.com/pingcap/tidb/pull/10687)\\n    - Fix the issue that the result of the `show grants` statement in RBAC is incompatible with that of MySQL when this statement has the `current_user` field [#10684](https://github.com/pingcap/tidb/pull/10684)\\n    - Fix the issue that UUIDs might generate duplicate values on multiple nodes [#10712](https://github.com/pingcap/tidb/pull/10712)\\n    - Fix the issue that the `show view` privilege is not considered in `explain` [#10635](https://github.com/pingcap/tidb/pull/10635)\\n    - Add the `split table region` statement to manually split the table Region to alleviate the hotspot issue [#10765](https://github.com/pingcap/tidb/pull/10765)\\n    - Add the `split index region` statement to manually split the index Region to alleviate the hotspot issue [#10764](https://github.com/pingcap/tidb/pull/10764)\\n    - Fix the incorrect execution issue when you execute multiple statements such as `create user`, `grant`, or `revoke` consecutively [#10737](https://github.com/pingcap/tidb/pull/10737)\\n    - Add a blocklist to prohibit pushing down expressions to Coprocessor [#10791](https://github.com/pingcap/tidb/pull/10791)\\n    - Add the feature of printing the `expensive query` log when a query exceeds the memory configuration limit [#10849](https://github.com/pingcap/tidb/pull/10849)\\n    - Add the `bind-info-lease` configuration item to control the update time of the modified binding execution plan [#10727](https://github.com/pingcap/tidb/pull/10727)\\n    - Fix the OOM issue in high concurrent scenarios caused by the failure to quickly release Coprocessor resources, resulted from the `execdetails.ExecDetails` pointer [#10832](https://github.com/pingcap/tidb/pull/10832)\\n    - Fix the panic issue caused by the `kill` statement in some cases [#10876](https://github.com/pingcap/tidb/pull/10876)\\n\\n+ Server\\n    - Fix the issue that goroutine might leak when repairing GC [#10683](https://github.com/pingcap/tidb/pull/10683)\\n    - Support displaying the `host` information in slow queries [#10693](https://github.com/pingcap/tidb/pull/10693)\\n    - Support reusing idle links that interact with TiKV [#10632](https://github.com/pingcap/tidb/pull/10632)\\n    - Fix the support for enabling the `skip-grant-table` option in RBAC [#10738](https://github.com/pingcap/tidb/pull/10738)\\n    - Fix the issue that `pessimistic-txn` configuration goes invalid [#10825](https://github.com/pingcap/tidb/pull/10825)\\n    - Fix the issue that the actively canceled ticlient requests are still retried [#10850](https://github.com/pingcap/tidb/pull/10850)\\n    - Improve performance in the case where pessimistic transactions conflict with optimistic transactions [#10881](https://github.com/pingcap/tidb/pull/10881)\\n\\n+ DDL\\n    - Fix the issue that modifying charset using `alter table` causes the `blob` type change [#10698](https://github.com/pingcap/tidb/pull/10698)\\n    - Add a feature to use `SHARD_ROW_ID_BITS` to scatter row IDs when the column contains an `AUTO_INCREMENT` attribute to alleviate the hotspot issue [#10794](https://github.com/pingcap/tidb/pull/10794)\\n    - Prohibit adding stored generated columns by using the `alter table` statement [#10808](https://github.com/pingcap/tidb/pull/10808)\\n    - Optimize the invalid survival time of DDL metadata to shorten the period during which the DDL operation is slower after cluster upgrade [#10795](https://github.com/pingcap/tidb/pull/10795)\\n\\n## PD\\n\\n- Add the `enable-two-way-merge` configuration item to allow only one-way merging [#1583](https://github.com/pingcap/pd/pull/1583)\\n- Add scheduling operations for `AddLightLearner` and `AddLightPeer` to make Region Scatter scheduling unrestricted by the limit mechanism [#1563](https://github.com/pingcap/pd/pull/1563)\\n- Fix the issue of insufficient reliability because the data might only have one replica replication when the system is started [#1581](https://github.com/pingcap/pd/pull/1581)\\n- Optimize configuration check logic to avoid configuration item errors [#1585](https://github.com/pingcap/pd/pull/1585)\\n- Adjust the definition of the `store-balance-rate` configuration to the upper limit of the number of balance operators generated per minute [#1591](https://github.com/pingcap/pd/pull/1591)\\n- Fix the issue that the store might have been unable to generate scheduled operations [#1590](https://github.com/pingcap/pd/pull/1590)\\n\\n## TiKV\\n\\n+ Engine\\n    - Fix the issue that incomplete snapshots are generated in the system caused by the iterator not checking the status [#4936](https://github.com/tikv/tikv/pull/4936)\\n    - Fix the data loss issue caused by a delay of flushing data to the disk when receiving snapshots after a power failure in abnormal conditions [#4850](https://github.com/tikv/tikv/pull/4850)\\n\\n+ Server\\n    - Add a feature to check the validity of the `block-size` configuration [#4928](https://github.com/tikv/tikv/pull/4928)\\n    - Add `READ_INDEX`-related monitoring metrics [#4830](https://github.com/tikv/tikv/pull/4830)\\n    - Add GC worker-related monitoring metrics [#4922](https://github.com/tikv/tikv/pull/4922)\\n\\n+ Raftstore\\n    - Fix the issue that the cache of the local reader is not cleared correctly [#4778](https://github.com/tikv/tikv/pull/4778)\\n    - Fix the issue that the request delay might be increased when transferring the leader and changing `conf` [#4734](https://github.com/tikv/tikv/pull/4734)\\n    - Fix the issue that a stale command is wrongly reported [#4682](https://github.com/tikv/tikv/pull/4682)\\n    - Fix the issue that the command might be pending for a long time [#4810](https://github.com/tikv/tikv/pull/4810)\\n    - Fix the issue that files are damaged after a power failure, which is caused by a delay of synchronizing the snapshot file to the disk [#4807](https://github.com/tikv/tikv/pull/4807), [#4850](https://github.com/tikv/tikv/pull/4850)\\n\\n+ Coprocessor\\n    - Support Top-N in vector calculation [#4827](https://github.com/tikv/tikv/pull/4827)\\n    - Support `Stream` aggregation in vector calculation [#4786](https://github.com/tikv/tikv/pull/4786)\\n    - Support the `AVG` aggregate function in vector calculation [#4777](https://github.com/tikv/tikv/pull/4777)\\n    - Support the `First` aggregate function in vector calculation [#4771](https://github.com/tikv/tikv/pull/4771)\\n    - Support the `SUM` aggregate function in vector calculation [#4797](https://github.com/tikv/tikv/pull/4797)\\n    - Support the `MAX`/`MIN` aggregate function in vector calculation [#4837](https://github.com/tikv/tikv/pull/4837)\\n    - Support the `Like` expression in vector calculation [#4747](https://github.com/tikv/tikv/pull/4747)\\n    - Support the `MultiplyDecimal` expression in vector calculation [#4849](https://github.com/tikv/tikv/pull/4849 )\\n    - Support the `BitAnd`/`BitOr`/`BitXor` expression in vector calculation [#4724](https://github.com/tikv/tikv/pull/4724)\\n    - Support the `UnaryNot` expression in vector calculation [#4808](https://github.com/tikv/tikv/pull/4808)\\n\\n+ Transaction\\n    - Fix the issue that an error occurs caused by non-pessimistic locking conflicts in pessimistic transactions [#4801](https://github.com/tikv/tikv/pull/4801), [#4883](https://github.com/tikv/tikv/pull/4883)\\n    - Reduce unnecessary calculation for optimistic transactions after enabling pessimistic transactions to improve the performance [#4813](https://github.com/tikv/tikv/pull/4813)\\n    - Add a feature of single statement rollback to ensure that the whole transaction does not need a rollback operation in a deadlock situation [#4848](https://github.com/tikv/tikv/pull/4848)\\n    - Add pessimistic transaction-related monitoring items [#4852](https://github.com/tikv/tikv/pull/4852)\\n    - Support using the `ResolveLockLite` command to resolve lightweight locks to improve the performance when severe conflicts exist [#4882](https://github.com/tikv/tikv/pull/4882)\\n\\n+ tikv-ctl\\n    - Add the `bad-regions` command to support checking more abnormal conditions [#4862](https://github.com/tikv/tikv/pull/4862)\\n    - Add a feature of forcibly executing the `tombstone` command [#4862](https://github.com/tikv/tikv/pull/4862)\\n\\n+ Misc\\n    - Add the `dist_release` compiling command [#4841](https://github.com/tikv/tikv/pull/4841)\\n\\n## Tools\\n\\n+ TiDB Binlog\\n    - Fix the wrong offset issue caused by Pump not checking the returned value when it fails to write data [#640](https://github.com/pingcap/tidb-binlog/pull/640)\\n    - Add the `advertise-addr` configuration in Drainer to support the bridge mode in the container environment [#634](https://github.com/pingcap/tidb-binlog/pull/634)\\n    - Add the `GetMvccByEncodeKey` function in Pump to speed up querying the transaction status [#632](https://github.com/pingcap/tidb-binlog/pull/632)\\n\\n## TiDB Ansible\\n\\n- Add a monitoring item to predict the maximum QPS value of the cluster (\"hide\" by default) [#f5cfa4d](https://github.com/pingcap/tidb-ansible/commit/f5cfa4d903bbcd77e01eddc8d31eabb6e6157f73)\\n', doc_link='https://docs.pingcap.com/tidb/v8.1/release-3.0.0-rc.3'),\n",
       " 15868: DocumentData(id=15868, chunks={}, content='---\\ntitle: Release Notes\\nsummary: TiDB has released multiple versions, including 8.1.0, 8.0.0-DMR, 7.6.0-DMR, 7.5.1, 7.5.0, 7.4.0-DMR, 7.3.0-DMR, 7.2.0-DMR, 7.1.4, 7.1.3, 7.1.2, 7.1.1, 7.1.0, 7.0.0-DMR, 6.6.0-DMR, 6.5.9, 6.5.8, 6.5.7, 6.5.6, 6.5.5, 6.5.4, 6.5.3, 6.5.2, 6.5.1, 6.5.0, 6.4.0-DMR, 6.3.0-DMR, 6.2.0-DMR, 6.1.7, 6.1.6, 6.1.5, 6.1.4, 6.1.3, 6.1.2, 6.1.1, 6.1.0, 6.0.0-DMR, 5.4.3, 5.4.2, 5.4.1, 5.4.0, 5.3.4, 5.3.3, 5.3.2, 5.3.1, 5.3.0, 5.2.4, 5.2.3, 5.2.2, 5.2.1, 5.2.0, 5.1.5, 5.1.4, 5.1.3, 5.1.2, 5.1.1, 5.1.0, 5.0.6, 5.0.5, 5.0.4, 5.0.3, 5.0.2, 5.0.1, 5.0.0, 5.0.0-rc, 4.0.16, 4.0.15, 4.0.14, 4.0.13, 4.0.12, 4.0.11, 4.0.10, 4.0.9, 4.0.8, 4.0.7, 4.0.6, 4.0.5, 4.0.4, 4.0.3, 4.0.2, 4.0.1, 4.0.0, 4.0.0-rc.2, 4.0.0-rc.1, 4.0.0-rc, 4.0.0-beta.2, 4.0.0-beta.1, 4.0.0-beta, 3.1.2, 3.1.1, 3.1.0, 3.1.0-rc, 3.1.0-beta.2, 3.1.0-beta.1, 3.1.0-beta, 3.0.20, 3.0.19, 3.0.18, 3.0.17, 3.0.16, 3.0.15, 3.0.14, 3.0.13, 3.0.12, 3.0.11, 3.0.10, 3.0.9, 3.0.8, 3.0.7, 3.0.6, 3.0.5, 3.0.4, 3.0.3, 3.0.2, 3.0.1, 3.0.0, 3.0.0-rc.3, 3.0.0-rc.2, 3.0.0-rc.1, 3.0.0-beta.1, 3.0.0-beta, 2.1.19, 2.1.18, 2.1.17, 2.1.16, 2.1.15, 2.1.14, 2.1.13, 2.1.12, 2.1.11, 2.1.10, 2.1.9, 2.1.8, 2.1.7, 2.1.6, 2.1.5, 2.1.4, 2.1.3, 2.1.2, 2.1.1, 2.1.0, 2.1.0-rc.5, 2.1.0-rc.4, 2.1.0-rc.3, 2.1.0-rc.2, 2.1.0-rc.1, 2.1.0-beta, 2.0.11, 2.0.10, 2.0.9, 2.0.8, 2.0.7, 2.0.6, 2.0.5, 2.0.4, 2.0.3, 2.0.2, 2.0.1, 2.0.0, 2.0.0-rc.5, 2.0.0-rc.4, 2.0.0-rc.3, 2.0.0-rc.1, 1.1.0-beta, 1.1.0-alpha, 1.0.8, 1.0.7, 1.0.6, 1.0.5, 1.0.4, 1.0.3, 1.0.2, 1.0.1, 1.0.0, Pre-GA, rc4, rc3, rc2, rc1.\\n---\\n\\n# TiDB Release Notes\\n\\n<EmailSubscriptionWrapper />\\n\\n## 8.1\\n\\n- [8.1.1](/releases/release-8.1.1.md): 2024-08-27\\n- [8.1.0](/releases/release-8.1.0.md): 2024-05-24\\n\\n## 8.0\\n\\n- [8.0.0-DMR](/releases/release-8.0.0.md): 2024-03-29\\n\\n## 7.6\\n\\n- [7.6.0-DMR](/releases/release-7.6.0.md): 2024-01-25\\n\\n## 7.5\\n\\n- [7.5.4](/releases/release-7.5.4.md): 2024-10-15\\n- [7.5.3](/releases/release-7.5.3.md): 2024-08-05\\n- [7.5.2](/releases/release-7.5.2.md): 2024-06-13\\n- [7.5.1](/releases/release-7.5.1.md): 2024-02-29\\n- [7.5.0](/releases/release-7.5.0.md): 2023-12-01\\n\\n## 7.4\\n\\n- [7.4.0-DMR](/releases/release-7.4.0.md): 2023-10-12\\n\\n## 7.3\\n\\n- [7.3.0-DMR](/releases/release-7.3.0.md): 2023-08-14\\n\\n## 7.2\\n\\n- [7.2.0-DMR](/releases/release-7.2.0.md): 2023-06-29\\n\\n## 7.1\\n\\n- [7.1.5](/releases/release-7.1.5.md): 2024-04-26\\n- [7.1.4](/releases/release-7.1.4.md): 2024-03-11\\n- [7.1.3](/releases/release-7.1.3.md): 2023-12-21\\n- [7.1.2](/releases/release-7.1.2.md): 2023-10-25\\n- [7.1.1](/releases/release-7.1.1.md): 2023-07-24\\n- [7.1.0](/releases/release-7.1.0.md): 2023-05-31\\n\\n## 7.0\\n\\n- [7.0.0-DMR](/releases/release-7.0.0.md): 2023-03-30\\n\\n## 6.6\\n\\n- [6.6.0-DMR](/releases/release-6.6.0.md): 2023-02-20\\n\\n## 6.5\\n\\n- [6.5.11](/releases/release-6.5.11.md): 2024-09-20\\n- [6.5.10](/releases/release-6.5.10.md): 2024-06-20\\n- [6.5.9](/releases/release-6.5.9.md): 2024-04-12\\n- [6.5.8](/releases/release-6.5.8.md): 2024-02-02\\n- [6.5.7](/releases/release-6.5.7.md): 2024-01-08\\n- [6.5.6](/releases/release-6.5.6.md): 2023-12-07\\n- [6.5.5](/releases/release-6.5.5.md): 2023-09-21\\n- [6.5.4](/releases/release-6.5.4.md): 2023-08-28\\n- [6.5.3](/releases/release-6.5.3.md): 2023-06-14\\n- [6.5.2](/releases/release-6.5.2.md): 2023-04-21\\n- [6.5.1](/releases/release-6.5.1.md): 2023-03-10\\n- [6.5.0](/releases/release-6.5.0.md): 2022-12-29\\n\\n## 6.4\\n\\n- [6.4.0-DMR](/releases/release-6.4.0.md): 2022-11-17\\n\\n## 6.3\\n\\n- [6.3.0-DMR](/releases/release-6.3.0.md): 2022-09-30\\n\\n## 6.2\\n\\n- [6.2.0-DMR](/releases/release-6.2.0.md): 2022-08-23\\n\\n## 6.1\\n\\n- [6.1.7](/releases/release-6.1.7.md): 2023-07-12\\n- [6.1.6](/releases/release-6.1.6.md): 2023-04-12\\n- [6.1.5](/releases/release-6.1.5.md): 2023-02-28\\n- [6.1.4](/releases/release-6.1.4.md): 2023-02-08\\n- [6.1.3](/releases/release-6.1.3.md): 2022-12-05\\n- [6.1.2](/releases/release-6.1.2.md): 2022-10-24\\n- [6.1.1](/releases/release-6.1.1.md): 2022-09-01\\n- [6.1.0](/releases/release-6.1.0.md): 2022-06-13\\n\\n## 6.0\\n\\n- [6.0.0-DMR](/releases/release-6.0.0-dmr.md): 2022-04-07\\n\\n## 5.4\\n\\n- [5.4.3](/releases/release-5.4.3.md): 2022-10-13\\n- [5.4.2](/releases/release-5.4.2.md): 2022-07-08\\n- [5.4.1](/releases/release-5.4.1.md): 2022-05-13\\n- [5.4.0](/releases/release-5.4.0.md): 2022-02-15\\n\\n## 5.3\\n\\n- [5.3.4](/releases/release-5.3.4.md): 2022-11-24\\n- [5.3.3](/releases/release-5.3.3.md): 2022-09-14\\n- [5.3.2](/releases/release-5.3.2.md): 2022-06-29\\n- [5.3.1](/releases/release-5.3.1.md): 2022-03-03\\n- [5.3.0](/releases/release-5.3.0.md): 2021-11-30\\n\\n## 5.2\\n\\n- [5.2.4](/releases/release-5.2.4.md): 2022-04-26\\n- [5.2.3](/releases/release-5.2.3.md): 2021-12-03\\n- [5.2.2](/releases/release-5.2.2.md): 2021-10-29\\n- [5.2.1](/releases/release-5.2.1.md): 2021-09-09\\n- [5.2.0](/releases/release-5.2.0.md): 2021-08-27\\n\\n## 5.1\\n\\n- [5.1.5](/releases/release-5.1.5.md): 2022-12-28\\n- [5.1.4](/releases/release-5.1.4.md): 2022-02-22\\n- [5.1.3](/releases/release-5.1.3.md): 2021-12-03\\n- [5.1.2](/releases/release-5.1.2.md): 2021-09-27\\n- [5.1.1](/releases/release-5.1.1.md): 2021-07-30\\n- [5.1.0](/releases/release-5.1.0.md): 2021-06-24\\n\\n## 5.0\\n\\n- [5.0.6](/releases/release-5.0.6.md): 2021-12-31\\n- [5.0.5](/releases/release-5.0.5.md): 2021-12-03\\n- [5.0.4](/releases/release-5.0.4.md): 2021-09-27\\n- [5.0.3](/releases/release-5.0.3.md): 2021-07-02\\n- [5.0.2](/releases/release-5.0.2.md): 2021-06-10\\n- [5.0.1](/releases/release-5.0.1.md): 2021-04-24\\n- [5.0.0](/releases/release-5.0.0.md): 2021-04-07\\n- [5.0.0-rc](/releases/release-5.0.0-rc.md): 2021-01-12\\n\\n## 4.0\\n\\n- [4.0.16](/releases/release-4.0.16.md): 2021-12-17\\n- [4.0.15](/releases/release-4.0.15.md): 2021-09-27\\n- [4.0.14](/releases/release-4.0.14.md): 2021-07-27\\n- [4.0.13](/releases/release-4.0.13.md): 2021-05-28\\n- [4.0.12](/releases/release-4.0.12.md): 2021-04-02\\n- [4.0.11](/releases/release-4.0.11.md): 2021-02-26\\n- [4.0.10](/releases/release-4.0.10.md): 2021-01-15\\n- [4.0.9](/releases/release-4.0.9.md): 2020-12-21\\n- [4.0.8](/releases/release-4.0.8.md): 2020-10-30\\n- [4.0.7](/releases/release-4.0.7.md): 2020-09-29\\n- [4.0.6](/releases/release-4.0.6.md): 2020-09-15\\n- [4.0.5](/releases/release-4.0.5.md): 2020-08-31\\n- [4.0.4](/releases/release-4.0.4.md): 2020-07-31\\n- [4.0.3](/releases/release-4.0.3.md): 2020-07-24\\n- [4.0.2](/releases/release-4.0.2.md): 2020-07-01\\n- [4.0.1](/releases/release-4.0.1.md): 2020-06-12\\n- [4.0.0](/releases/release-4.0-ga.md): 2020-05-28\\n- [4.0.0-rc.2](/releases/release-4.0.0-rc.2.md): 2020-05-15\\n- [4.0.0-rc.1](/releases/release-4.0.0-rc.1.md): 2020-04-28\\n- [4.0.0-rc](/releases/release-4.0.0-rc.md): 2020-04-08\\n- [4.0.0-beta.2](/releases/release-4.0.0-beta.2.md): 2020-03-18\\n- [4.0.0-beta.1](/releases/release-4.0.0-beta.1.md): 2020-02-28\\n- [4.0.0-beta](/releases/release-4.0.0-beta.md): 2020-01-17\\n\\n## 3.1\\n\\n- [3.1.2](/releases/release-3.1.2.md): 2020-06-04\\n- [3.1.1](/releases/release-3.1.1.md): 2020-04-30\\n- [3.1.0](/releases/release-3.1.0-ga.md): 2020-04-16\\n- [3.1.0-rc](/releases/release-3.1.0-rc.md): 2020-04-02\\n- [3.1.0-beta.2](/releases/release-3.1.0-beta.2.md): 2020-03-09\\n- [3.1.0-beta.1](/releases/release-3.1.0-beta.1.md): 2020-01-10\\n- [3.1.0-beta](/releases/release-3.1.0-beta.md): 2019-12-20\\n\\n## 3.0\\n\\n- [3.0.20](/releases/release-3.0.20.md): 2020-12-25\\n- [3.0.19](/releases/release-3.0.19.md): 2020-09-25\\n- [3.0.18](/releases/release-3.0.18.md): 2020-08-21\\n- [3.0.17](/releases/release-3.0.17.md): 2020-08-03\\n- [3.0.16](/releases/release-3.0.16.md): 2020-07-03\\n- [3.0.15](/releases/release-3.0.15.md): 2020-06-05\\n- [3.0.14](/releases/release-3.0.14.md): 2020-05-09\\n- [3.0.13](/releases/release-3.0.13.md): 2020-04-22\\n- [3.0.12](/releases/release-3.0.12.md): 2020-03-16\\n- [3.0.11](/releases/release-3.0.11.md): 2020-03-04\\n- [3.0.10](/releases/release-3.0.10.md): 2020-02-20\\n- [3.0.9](/releases/release-3.0.9.md): 2020-01-14\\n- [3.0.8](/releases/release-3.0.8.md): 2019-12-31\\n- [3.0.7](/releases/release-3.0.7.md): 2019-12-04\\n- [3.0.6](/releases/release-3.0.6.md): 2019-11-28\\n- [3.0.5](/releases/release-3.0.5.md): 2019-10-25\\n- [3.0.4](/releases/release-3.0.4.md): 2019-10-08\\n- [3.0.3](/releases/release-3.0.3.md): 2019-08-29\\n- [3.0.2](/releases/release-3.0.2.md): 2019-08-07\\n- [3.0.1](/releases/release-3.0.1.md): 2019-07-16\\n- [3.0.0](/releases/release-3.0-ga.md): 2019-06-28\\n- [3.0.0-rc.3](/releases/release-3.0.0-rc.3.md): 2019-06-21\\n- [3.0.0-rc.2](/releases/release-3.0.0-rc.2.md): 2019-05-28\\n- [3.0.0-rc.1](/releases/release-3.0.0-rc.1.md): 2019-05-10\\n- [3.0.0-beta.1](/releases/release-3.0.0-beta.1.md): 2019-03-26\\n- [3.0.0-beta](/releases/release-3.0-beta.md): 2019-01-19\\n\\n## 2.1\\n\\n- [2.1.19](/releases/release-2.1.19.md): 2019-12-27\\n- [2.1.18](/releases/release-2.1.18.md): 2019-11-04\\n- [2.1.17](/releases/release-2.1.17.md): 2019-09-11\\n- [2.1.16](/releases/release-2.1.16.md): 2019-08-15\\n- [2.1.15](/releases/release-2.1.15.md): 2019-07-18\\n- [2.1.14](/releases/release-2.1.14.md): 2019-07-04\\n- [2.1.13](/releases/release-2.1.13.md): 2019-06-21\\n- [2.1.12](/releases/release-2.1.12.md): 2019-06-13\\n- [2.1.11](/releases/release-2.1.11.md): 2019-06-03\\n- [2.1.10](/releases/release-2.1.10.md): 2019-05-22\\n- [2.1.9](/releases/release-2.1.9.md): 2019-05-06\\n- [2.1.8](/releases/release-2.1.8.md): 2019-04-12\\n- [2.1.7](/releases/release-2.1.7.md): 2019-03-28\\n- [2.1.6](/releases/release-2.1.6.md): 2019-03-15\\n- [2.1.5](/releases/release-2.1.5.md): 2019-02-28\\n- [2.1.4](/releases/release-2.1.4.md): 2019-02-15\\n- [2.1.3](/releases/release-2.1.3.md): 2019-01-28\\n- [2.1.2](/releases/release-2.1.2.md): 2018-12-22\\n- [2.1.1](/releases/release-2.1.1.md): 2018-12-12\\n- [2.1.0](/releases/release-2.1-ga.md): 2018-11-30\\n- [2.1.0-rc.5](/releases/release-2.1-rc.5.md): 2018-11-12\\n- [2.1.0-rc.4](/releases/release-2.1-rc.4.md): 2018-10-23\\n- [2.1.0-rc.3](/releases/release-2.1-rc.3.md): 2018-09-29\\n- [2.1.0-rc.2](/releases/release-2.1-rc.2.md): 2018-09-14\\n- [2.1.0-rc.1](/releases/release-2.1-rc.1.md): 2018-08-24\\n- [2.1.0-beta](/releases/release-2.1-beta.md): 2018-06-29\\n\\n## 2.0\\n\\n- [2.0.11](/releases/release-2.0.11.md): 2019-01-03\\n- [2.0.10](/releases/release-2.0.10.md): 2018-12-18\\n- [2.0.9](/releases/release-2.0.9.md): 2018-11-19\\n- [2.0.8](/releases/release-2.0.8.md): 2018-10-16\\n- [2.0.7](/releases/release-2.0.7.md): 2018-09-07\\n- [2.0.6](/releases/release-2.0.6.md): 2018-08-06\\n- [2.0.5](/releases/release-2.0.5.md): 2018-07-06\\n- [2.0.4](/releases/release-2.0.4.md): 2018-06-15\\n- [2.0.3](/releases/release-2.0.3.md): 2018-06-01\\n- [2.0.2](/releases/release-2.0.2.md): 2018-05-21\\n- [2.0.1](/releases/release-2.0.1.md): 2018-05-16\\n- [2.0.0](/releases/release-2.0-ga.md): 2018-04-27\\n- [2.0.0-rc.5](/releases/release-2.0-rc.5.md): 2018-04-17\\n- [2.0.0-rc.4](/releases/release-2.0-rc.4.md): 2018-03-30\\n- [2.0.0-rc.3](/releases/release-2.0-rc.3.md): 2018-03-23\\n- [2.0.0-rc.1](/releases/release-2.0-rc.1.md): 2018-03-09\\n- [1.1.0-beta](/releases/release-1.1-beta.md): 2018-02-24\\n- [1.1.0-alpha](/releases/release-1.1-alpha.md): 2018-01-19\\n\\n## 1.0\\n\\n- [1.0.8](/releases/release-1.0.8.md): 2018-02-11\\n- [1.0.7](/releases/release-1.0.7.md): 2018-01-22\\n- [1.0.6](/releases/release-1.0.6.md): 2018-01-08\\n- [1.0.5](/releases/release-1.0.5.md): 2017-12-26\\n- [1.0.4](/releases/release-1.0.4.md): 2017-12-11\\n- [1.0.3](/releases/release-1.0.3.md): 2017-11-28\\n- [1.0.2](/releases/release-1.0.2.md): 2017-11-13\\n- [1.0.1](/releases/release-1.0.1.md): 2017-11-01\\n- [1.0.0](/releases/release-1.0-ga.md): 2017-10-16\\n- [Pre-GA](/releases/release-pre-ga.md): 2017-08-30\\n- [rc4](/releases/release-rc.4.md): 2017-08-04\\n- [rc3](/releases/release-rc.3.md): 2017-06-16\\n- [rc2](/releases/release-rc.2.md): 2017-03-01\\n- [rc1](/releases/release-rc.1.md): 2016-12-23\\n', doc_link='https://docs.pingcap.com/tidb/v8.1/release-notes'),\n",
       " 15821: DocumentData(id=15821, chunks={}, content=\"---\\ntitle: TiDB 3.0.11 Release Notes\\nsummary: TiDB 3.0.11 was released on March 4, 2020. It includes compatibility changes, new features, bug fixes, and updates for TiDB, TiDB Binlog, TiDB Lightning, TiKV, and TiDB Ansible. Some known issues are fixed in new versions, so it is recommended to use the latest 3.0.x version.\\n---\\n\\n# TiDB 3.0.11 Release Notes\\n\\nRelease date: March 4, 2020\\n\\nTiDB version: 3.0.11\\n\\nTiDB Ansible version: 3.0.11\\n\\n> **Warning:**\\n>\\n> Some known issues are found in this version, and these issues are fixed in new versions. It is recommended that you use the latest 3.0.x version.\\n\\n## Compatibility Changes\\n\\n* TiDB\\n    + Add the `max-index-length` configuration item to control the maximum index length, which is compatible with the behavior of TiDB versions before 3.0.7 or of MySQL [#15057](https://github.com/pingcap/tidb/pull/15057)\\n\\n## New Features\\n\\n* TiDB\\n    + Support showing the meta information of partitioned tables in the `information_schema.PARTITIONS` table [#14849](https://github.com/pingcap/tidb/pull/14849)\\n\\n* TiDB Binlog\\n    + Support the bidirectional data replication between TiDB clusters [#884](https://github.com/pingcap/tidb-binlog/pull/884) [#909](https://github.com/pingcap/tidb-binlog/pull/909)\\n\\n* TiDB Lightning\\n    + Support the TLS configuration [#44](https://github.com/tikv/importer/pull/44) [#270](https://github.com/pingcap/tidb-lightning/pull/270)\\n\\n* TiDB Ansible\\n    + Modify the logic of `create_users.yml` so that users of the control machine do not have to be consistent with `ansible_user` [#1184](https://github.com/pingcap/tidb-ansible/pull/1184)\\n\\n## Bug Fixes\\n\\n* TiDB\\n    + Fix the issue of Goroutine leaks when retrying an optimistic transaction because queries using `Union` are not marked read-only [#15076](https://github.com/pingcap/tidb/pull/15076)\\n    + Fix the issue that `SHOW TABLE STATUS` fails to correctly output the table status at the snapshot time because the value of the `tidb_snapshot` parameter is not correctly used when executing the `SET SESSION tidb_snapshot = 'xxx';` statement [#14391](https://github.com/pingcap/tidb/pull/14391)\\n    + Fix the incorrect result caused by a SQL statement that contains `Sort Merge Join` and `ORDER BY DESC` at the same time [#14664](https://github.com/pingcap/tidb/pull/14664)\\n    + Fix the panic of TiDB server when creating partition tables using the unsupported expression. The error information `This partition function is not allowed` is returned after fixing this panic. [#14769](https://github.com/pingcap/tidb/pull/14769)\\n    + Fix the incorrect result occurred when executing the `select max() from subquery` statement with the subquery containing `Union` [#14944](https://github.com/pingcap/tidb/pull/14944)\\n    + Fix the issue that an error message is returned when executing the `SHOW BINDINGS` statement after executing `DROP BINDING` that drops the execution binding [#14865](https://github.com/pingcap/tidb/pull/14865)\\n    + Fix the issue that the connection is broken because the maximum length of an alias in a query is 256 characters in the MySQL protocol, but TiDB does not [cut the alias](https://dev.mysql.com/doc/refman/8.0/en/identifier-length.html) in the query results according to this protocol [#14940](https://github.com/pingcap/tidb/pull/14940)\\n    + Fix the incorrect query result that might occur when using the string type in `DIV`. For instance, now you can correctly execute the `select 1 / '2007' div 1` statement [#14098](https://github.com/pingcap/tidb/pull/14098)\\n\\n* TiKV\\n    + Optimize the log output by removing unnecessary logs [#6657](https://github.com/tikv/tikv/pull/6657)\\n    + Fix the panic that might occur when the peer is removed under high loads [#6704](https://github.com/tikv/tikv/pull/6704)\\n    + Fix the issue that Hibernate Regions are not waken up in some cases [#6732](https://github.com/tikv/tikv/pull/6732) [#6738](https://github.com/tikv/tikv/pull/6738)\\n\\n* TiDB Ansible\\n    + Update outdated document links in `tidb-ansible` [#1169](https://github.com/pingcap/tidb-ansible/pull/1169)\\n    + Fix the issue that undefined variables might occur in the `wait for region replication complete` task [#1173](https://github.com/pingcap/tidb-ansible/pull/1173)\\n\", doc_link='https://docs.pingcap.com/tidb/v8.1/release-3.0.11'),\n",
       " 15789: DocumentData(id=15789, chunks={}, content='---\\ntitle: TiDB Release Timeline\\nsummary: Learn about the TiDB release timeline.\\n---\\n\\n# TiDB Release Timeline\\n\\n<EmailSubscriptionWrapper />\\n\\nThis document shows all the released TiDB versions in reverse chronological order.\\n\\n| Version | Release Date |\\n| :--- | :--- |\\n| [7.5.4](/releases/release-7.5.4.md) | 2024-10-15 |\\n| [6.5.11](/releases/release-6.5.11.md) | 2024-09-20 |\\n| [8.1.1](/releases/release-8.1.1.md) | 2024-08-27 |\\n| [7.5.3](/releases/release-7.5.3.md) | 2024-08-05 |\\n| [6.5.10](/releases/release-6.5.10.md) | 2024-06-20 |\\n| [7.5.2](/releases/release-7.5.2.md) | 2024-06-13 |\\n| [8.1.0](/releases/release-8.1.0.md) | 2024-05-24 |\\n| [7.1.5](/releases/release-7.1.5.md) | 2024-04-26 |\\n| [6.5.9](/releases/release-6.5.9.md) | 2024-04-12 |\\n| [8.0.0-DMR](/releases/release-8.0.0.md) | 2024-03-29 |\\n| [7.1.4](/releases/release-7.1.4.md) | 2024-03-11 |\\n| [7.5.1](/releases/release-7.5.1.md) | 2024-02-29 |\\n| [6.5.8](/releases/release-6.5.8.md) | 2024-02-02 |\\n| [7.6.0-DMR](/releases/release-7.6.0.md) | 2024-01-25 |\\n| [6.5.7](/releases/release-6.5.7.md) | 2024-01-08 |\\n| [7.1.3](/releases/release-7.1.3.md) | 2023-12-21 |\\n| [6.5.6](/releases/release-6.5.6.md) | 2023-12-07 |\\n| [7.5.0](/releases/release-7.5.0.md) | 2023-12-01 |\\n| [7.1.2](/releases/release-7.1.2.md) | 2023-10-25 |\\n| [7.4.0-DMR](/releases/release-7.4.0.md) | 2023-10-12 |\\n| [6.5.5](/releases/release-6.5.5.md) | 2023-09-21 |\\n| [6.5.4](/releases/release-6.5.4.md) | 2023-08-28 |\\n| [7.3.0-DMR](/releases/release-7.3.0.md) | 2023-08-14 |\\n| [7.1.1](/releases/release-7.1.1.md) | 2023-07-24 |\\n| [6.1.7](/releases/release-6.1.7.md) | 2023-07-12 |\\n| [7.2.0-DMR](/releases/release-7.2.0.md) | 2023-06-29 |\\n| [6.5.3](/releases/release-6.5.3.md) | 2023-06-14 |\\n| [7.1.0](/releases/release-7.1.0.md) | 2023-05-31 |\\n| [6.5.2](/releases/release-6.5.2.md) | 2023-04-21 |\\n| [6.1.6](/releases/release-6.1.6.md) | 2023-04-12 |\\n| [7.0.0-DMR](/releases/release-7.0.0.md) | 2023-03-30 |\\n| [6.5.1](/releases/release-6.5.1.md) | 2023-03-10 |\\n| [6.1.5](/releases/release-6.1.5.md) | 2023-02-28 |\\n| [6.6.0-DMR](/releases/release-6.6.0.md) | 2023-02-20 |\\n| [6.1.4](/releases/release-6.1.4.md) | 2023-02-08 |\\n| [6.5.0](/releases/release-6.5.0.md) | 2022-12-29 |\\n| [5.1.5](/releases/release-5.1.5.md) | 2022-12-28 |\\n| [6.1.3](/releases/release-6.1.3.md) | 2022-12-05 |\\n| [5.3.4](/releases/release-5.3.4.md) | 2022-11-24 |\\n| [6.4.0-DMR](/releases/release-6.4.0.md) | 2022-11-17 |\\n| [6.1.2](/releases/release-6.1.2.md) | 2022-10-24 |\\n| [5.4.3](/releases/release-5.4.3.md) | 2022-10-13 |\\n| [6.3.0-DMR](/releases/release-6.3.0.md) | 2022-09-30 |\\n| [5.3.3](/releases/release-5.3.3.md) | 2022-09-14 |\\n| [6.1.1](/releases/release-6.1.1.md) | 2022-09-01 |\\n| [6.2.0-DMR](/releases/release-6.2.0.md) | 2022-08-23 |\\n| [5.4.2](/releases/release-5.4.2.md) | 2022-07-08 |\\n| [5.3.2](/releases/release-5.3.2.md) | 2022-06-29 |\\n| [6.1.0](/releases/release-6.1.0.md) | 2022-06-13 |\\n| [5.4.1](/releases/release-5.4.1.md) | 2022-05-13 |\\n| [5.2.4](/releases/release-5.2.4.md) | 2022-04-26 |\\n| [6.0.0-DMR](/releases/release-6.0.0-dmr.md) | 2022-04-07 |\\n| [5.3.1](/releases/release-5.3.1.md) | 2022-03-03 |\\n| [5.1.4](/releases/release-5.1.4.md) | 2022-02-22 |\\n| [5.4.0](/releases/release-5.4.0.md) | 2022-02-15 |\\n| [5.0.6](/releases/release-5.0.6.md) | 2021-12-31 |\\n| [4.0.16](/releases/release-4.0.16.md) | 2021-12-17 |\\n| [5.1.3](/releases/release-5.1.3.md) | 2021-12-03 |\\n| [5.0.5](/releases/release-5.0.5.md) | 2021-12-03 |\\n| [5.2.3](/releases/release-5.2.3.md) | 2021-12-03 |\\n| [5.3.0](/releases/release-5.3.0.md) | 2021-11-30 |\\n| [5.2.2](/releases/release-5.2.2.md) | 2021-10-29 |\\n| [5.1.2](/releases/release-5.1.2.md) | 2021-09-27 |\\n| [5.0.4](/releases/release-5.0.4.md) | 2021-09-27 |\\n| [4.0.15](/releases/release-4.0.15.md) | 2021-09-27 |\\n| [5.2.1](/releases/release-5.2.1.md) | 2021-09-09 |\\n| [5.2.0](/releases/release-5.2.0.md) | 2021-08-27 |\\n| [5.1.1](/releases/release-5.1.1.md) | 2021-07-30 |\\n| [4.0.14](/releases/release-4.0.14.md) | 2021-07-27 |\\n| [5.0.3](/releases/release-5.0.3.md) | 2021-07-02 |\\n| [5.1.0](/releases/release-5.1.0.md) | 2021-06-24 |\\n| [5.0.2](/releases/release-5.0.2.md) | 2021-06-10 |\\n| [4.0.13](/releases/release-4.0.13.md) | 2021-05-28 |\\n| [5.0.1](/releases/release-5.0.1.md) | 2021-04-24 |\\n| [5.0.0](/releases/release-5.0.0.md) | 2021-04-07 |\\n| [4.0.12](/releases/release-4.0.12.md) | 2021-04-02 |\\n| [4.0.11](/releases/release-4.0.11.md) | 2021-02-26 |\\n| [4.0.10](/releases/release-4.0.10.md) | 2021-01-15 |\\n| [5.0.0-rc](/releases/release-5.0.0-rc.md) | 2021-01-12 |\\n| [3.0.20](/releases/release-3.0.20.md) | 2020-12-25 |\\n| [4.0.9](/releases/release-4.0.9.md) | 2020-12-21 |\\n| [4.0.8](/releases/release-4.0.8.md) | 2020-10-30 |\\n| [4.0.7](/releases/release-4.0.7.md) | 2020-09-29 |\\n| [3.0.19](/releases/release-3.0.19.md) | 2020-09-25 |\\n| [4.0.6](/releases/release-4.0.6.md) | 2020-09-15 |\\n| [4.0.5](/releases/release-4.0.5.md) | 2020-08-31 |\\n| [3.0.18](/releases/release-3.0.18.md) | 2020-08-21 |\\n| [3.0.17](/releases/release-3.0.17.md) | 2020-08-03 |\\n| [4.0.4](/releases/release-4.0.4.md) | 2020-07-31 |\\n| [4.0.3](/releases/release-4.0.3.md) | 2020-07-24 |\\n| [3.0.16](/releases/release-3.0.16.md) | 2020-07-03 |\\n| [4.0.2](/releases/release-4.0.2.md) | 2020-07-01 |\\n| [4.0.1](/releases/release-4.0.1.md) | 2020-06-12 |\\n| [3.0.15](/releases/release-3.0.15.md) | 2020-06-05 |\\n| [3.1.2](/releases/release-3.1.2.md) | 2020-06-04 |\\n| [4.0.0](/releases/release-4.0-ga.md) | 2020-05-28 |\\n| [4.0.0-rc.2](/releases/release-4.0.0-rc.2.md) | 2020-05-15 |\\n| [3.0.14](/releases/release-3.0.14.md) | 2020-05-09 |\\n| [3.1.1](/releases/release-3.1.1.md) | 2020-04-30 |\\n| [4.0.0-rc.1](/releases/release-4.0.0-rc.1.md) | 2020-04-28 |\\n| [3.0.13](/releases/release-3.0.13.md) | 2020-04-22 |\\n| [3.1.0](/releases/release-3.1.0-ga.md) | 2020-04-16 |\\n| [4.0.0-rc](/releases/release-4.0.0-rc.md) | 2020-04-08 |\\n| [3.1.0-rc](/releases/release-3.1.0-rc.md) | 2020-04-02 |\\n| [4.0.0-beta.2](/releases/release-4.0.0-beta.2.md) | 2020-03-18 |\\n| [3.0.12](/releases/release-3.0.12.md) | 2020-03-16 |\\n| [3.1.0-beta.2](/releases/release-3.1.0-beta.2.md) | 2020-03-09 |\\n| [3.0.11](/releases/release-3.0.11.md) | 2020-03-04 |\\n| [4.0.0-beta.1](/releases/release-4.0.0-beta.1.md) | 2020-02-28 |\\n| [3.0.10](/releases/release-3.0.10.md) | 2020-02-20 |\\n| [4.0.0-beta](/releases/release-4.0.0-beta.md) | 2020-01-17 |\\n| [3.0.9](/releases/release-3.0.9.md) | 2020-01-14 |\\n| [3.1.0-beta.1](/releases/release-3.1.0-beta.1.md) | 2020-01-10 |\\n| [3.0.8](/releases/release-3.0.8.md) | 2019-12-31 |\\n| [2.1.19](/releases/release-2.1.19.md) | 2019-12-27 |\\n| [3.1.0-beta](/releases/release-3.1.0-beta.md) | 2019-12-20 |\\n| [3.0.7](/releases/release-3.0.7.md) | 2019-12-04 |\\n| [3.0.6](/releases/release-3.0.6.md) | 2019-11-28 |\\n| [2.1.18](/releases/release-2.1.18.md) | 2019-11-04 |\\n| [3.0.5](/releases/release-3.0.5.md) | 2019-10-25 |\\n| [3.0.4](/releases/release-3.0.4.md) | 2019-10-08 |\\n| [2.1.17](/releases/release-2.1.17.md) | 2019-09-11 |\\n| [3.0.3](/releases/release-3.0.3.md) | 2019-08-29 |\\n| [2.1.16](/releases/release-2.1.16.md) | 2019-08-15 |\\n| [3.0.2](/releases/release-3.0.2.md) | 2019-08-07 |\\n| [2.1.15](/releases/release-2.1.15.md) | 2019-07-18 |\\n| [3.0.1](/releases/release-3.0.1.md) | 2019-07-16 |\\n| [2.1.14](/releases/release-2.1.14.md) | 2019-07-04 |\\n| [3.0.0](/releases/release-3.0-ga.md) | 2019-06-28 |\\n| [3.0.0-rc.3](/releases/release-3.0.0-rc.3.md) | 2019-06-21 |\\n| [2.1.13](/releases/release-2.1.13.md) | 2019-06-21 |\\n| [2.1.12](/releases/release-2.1.12.md) | 2019-06-13 |\\n| [2.1.11](/releases/release-2.1.11.md) | 2019-06-03 |\\n| [3.0.0-rc.2](/releases/release-3.0.0-rc.2.md) | 2019-05-28 |\\n| [2.1.10](/releases/release-2.1.10.md) | 2019-05-22 |\\n| [3.0.0-rc.1](/releases/release-3.0.0-rc.1.md) | 2019-05-10 |\\n| [2.1.9](/releases/release-2.1.9.md) | 2019-05-06 |\\n| [2.1.8](/releases/release-2.1.8.md) | 2019-04-12 |\\n| [2.1.7](/releases/release-2.1.7.md) | 2019-03-28 |\\n| [3.0.0-beta.1](/releases/release-3.0.0-beta.1.md) | 2019-03-26 |\\n| [2.1.6](/releases/release-2.1.6.md) | 2019-03-15 |\\n| [2.1.5](/releases/release-2.1.5.md) | 2019-02-28 |\\n| [2.1.4](/releases/release-2.1.4.md) | 2019-02-15 |\\n| [2.1.3](/releases/release-2.1.3.md) | 2019-01-28 |\\n| [3.0.0-beta](/releases/release-3.0-beta.md) | 2019-01-19 |\\n| [2.0.11](/releases/release-2.0.11.md) | 2019-01-03 |\\n| [2.1.2](/releases/release-2.1.2.md) | 2018-12-22 |\\n| [2.0.10](/releases/release-2.0.10.md) | 2018-12-18 |\\n| [2.1.1](/releases/release-2.1.1.md) | 2018-12-12 |\\n| [2.1.0](/releases/release-2.1-ga.md) | 2018-11-30 |\\n| [2.0.9](/releases/release-2.0.9.md) | 2018-11-19 |\\n| [2.1.0-rc.5](/releases/release-2.1-rc.5.md) | 2018-11-12 |\\n| [2.1.0-rc.4](/releases/release-2.1-rc.4.md) | 2018-10-23 |\\n| [2.0.8](/releases/release-2.0.8.md) | 2018-10-16 |\\n| [2.1.0-rc.3](/releases/release-2.1-rc.3.md) | 2018-09-29 |\\n| [2.1.0-rc.2](/releases/release-2.1-rc.2.md) | 2018-09-14 |\\n| [2.0.7](/releases/release-2.0.7.md) | 2018-09-07 |\\n| [2.1.0-rc.1](/releases/release-2.1-rc.1.md) | 2018-08-24 |\\n| [2.0.6](/releases/release-2.0.6.md) | 2018-08-06 |\\n| [2.0.5](/releases/release-2.0.5.md) | 2018-07-06 |\\n| [2.1.0-beta](/releases/release-2.1-beta.md) | 2018-06-29 |\\n| [2.0.4](/releases/release-2.0.4.md) | 2018-06-15 |\\n| [2.0.3](/releases/release-2.0.3.md) | 2018-06-01 |\\n| [2.0.2](/releases/release-2.0.2.md) | 2018-05-21 |\\n| [2.0.1](/releases/release-2.0.1.md) | 2018-05-16 |\\n| [2.0.0](/releases/release-2.0-ga.md) | 2018-04-27 |\\n| [2.0.0-rc.5](/releases/release-2.0-rc.5.md) | 2018-04-17 |\\n| [2.0.0-rc.4](/releases/release-2.0-rc.4.md) | 2018-03-30 |\\n| [2.0.0-rc.3](/releases/release-2.0-rc.3.md) | 2018-03-23 |\\n| [2.0.0-rc.1](/releases/release-2.0-rc.1.md) | 2018-03-09 |\\n| [1.1.0-beta](/releases/release-1.1-beta.md) | 2018-02-24 |\\n| [1.0.8](/releases/release-1.0.8.md) | 2018-02-11 |\\n| [1.0.7](/releases/release-1.0.7.md) | 2018-01-22 |\\n| [1.1.0-alpha](/releases/release-1.1-alpha.md) | 2018-01-19 |\\n| [1.0.6](/releases/release-1.0.6.md) | 2018-01-08 |\\n| [1.0.5](/releases/release-1.0.5.md) | 2017-12-26 |\\n| [1.0.4](/releases/release-1.0.4.md) | 2017-12-11 |\\n| [1.0.3](/releases/release-1.0.3.md) | 2017-11-28 |\\n| [1.0.2](/releases/release-1.0.2.md) | 2017-11-13 |\\n| [1.0.1](/releases/release-1.0.1.md) | 2017-11-01 |\\n| [1.0.0](/releases/release-1.0-ga.md) | 2017-10-16 |\\n| [Pre-GA](/releases/release-pre-ga.md) | 2017-08-30 |\\n| [rc4](/releases/release-rc.4.md) | 2017-08-04 |\\n| [rc3](/releases/release-rc.3.md) | 2017-06-16 |\\n| [rc2](/releases/release-rc.2.md) | 2017-03-01 |\\n| [rc1](/releases/release-rc.1.md) | 2016-12-23 |', doc_link='https://docs.pingcap.com/tidb/v8.1/release-timeline'),\n",
       " 15777: DocumentData(id=15777, chunks={}, content=\"---\\ntitle: TiDB 3.0.7 Release Notes\\nsummary: TiDB 3.0.7 was released on December 4, 2019. It includes fixes for issues related to lock TTL, timezone parsing, result accuracy, data precision, and statistics accuracy. TiKV also received updates to improve deadlock detection and fix a memory leak issue.\\n---\\n\\n# TiDB 3.0.7 Release Notes\\n\\nRelease date: December 4, 2019\\n\\nTiDB version: 3.0.7\\n\\nTiDB Ansible version: 3.0.7\\n\\n## TiDB\\n\\n- Fix the issue that the lock TTL's value is too large because the TiDB server's local time is behind PD's timestamp [#13868](https://github.com/pingcap/tidb/pull/13868)\\n- Fix the issue that the timezone is incorrect after parsing the date from strings using `gotime.Local` [#13793](https://github.com/pingcap/tidb/pull/13793)\\n- Fix the issue that the result might be incorrect because the `binSearch` function does not return an error in the implementation of `builtinIntervalRealSig` [#13767](https://github.com/pingcap/tidb/pull/13767)\\n- Fix the issue that data is incorrect because the precision is lost when an integer is converted to an unsigned floating point or decimal type [#13755](https://github.com/pingcap/tidb/pull/13755)\\n- Fix the issue that the result is incorrect because the `not null` flag is not properly reset when the `USING` clause is used in Natural Outer Join and Outer Join [#13739](https://github.com/pingcap/tidb/pull/13739)\\n- Fix the issue that the statistics are not accurate because a data race occurs when statistics are updated [#13687](https://github.com/pingcap/tidb/pull/13687)\\n\\n## TiKV\\n\\n- Make the deadlock detector only observe valid Regions to make sure the deadlock manager is in a valid Region [#6110](https://github.com/tikv/tikv/pull/6110)\\n- Fix a potential memory leak issue [#6128](https://github.com/tikv/tikv/pull/6128)\\n\", doc_link='https://docs.pingcap.com/tidb/v8.1/release-3.0.7'),\n",
       " 15775: DocumentData(id=15775, chunks={}, content=\"---\\ntitle: TiDB 2.1 GA Release Notes\\nsummary: TiDB 2.1 GA was released on November 30, 2018, with significant improvements in stability, performance, compatibility, and usability. The release includes optimizations in SQL optimizer, SQL executor, statistics, expressions, server, DDL, compatibility, Placement Driver (PD), TiKV, and tools. It also introduces TiDB Lightning for fast full data import and supports new TiDB Binlog. However, TiDB 2.1 does not support downgrading to v2.0.x or earlier due to the adoption of the new storage engine. Additionally, parallel DDL is enabled in TiDB 2.1, so clusters with TiDB version earlier than 2.0.1 cannot upgrade to 2.1 using rolling update. If upgrading from TiDB 2.0.6 or earlier to TiDB 2.1, ongoing DDL operations may slow down the upgrading process.\\n---\\n\\n# TiDB 2.1 GA Release Notes\\n\\nOn November 30, 2018, TiDB 2.1 GA is released. See the following updates in this release. Compared with TiDB 2.0, this release has great improvements in stability, performance, compatibility, and usability.\\n\\n## TiDB\\n\\n+ SQL Optimizer\\n\\n    - Optimize the selection range of `Index Join` to improve the execution performance\\n\\n    - Optimize the selection of outer table for `Index Join` and use the table with smaller estimated value of Row Count the as the outer table\\n\\n    - Optimize Join Hint `TIDB_SMJ` so that Merge Join can be used even without proper index available\\n\\n    - Optimize Join Hint `TIDB_INLJ` to specify the Inner table to Join\\n\\n    - Optimize correlated subquery, push down Filter, and extend the index selection range, to improve the efficiency of some queries by orders of magnitude\\n\\n    - Support using Index Hint and Join Hint in the `UPDATE` and `DELETE` statement\\n\\n    - Support pushing down more functions: `ABS`/`CEIL`/`FLOOR`/`IS TRUE`/`IS FALSE`\\n\\n    - Optimize the constant folding algorithm for the `IF` and `IFNULL` built-in functions\\n\\n    - Optimize the output of the `EXPLAIN` statement and use hierarchy structure to show the relationship between operators\\n\\n+ SQL executor\\n\\n    - Refactor all the aggregation functions and improve execution efficiency of the `Stream` and `Hash` aggregation operators\\n\\n    - Implement the parallel `Hash Aggregate` operators and improve the computing performance by 350% in some scenarios\\n\\n    - Implement the parallel `Project` operators and improve the performance by 74% in some scenarios\\n\\n    - Read the data of the Inner table and Outer table of `Hash Join` concurrently to improve the execution performance\\n\\n    - Optimize the execution speed of the `REPLACE INTO` statement and increase the performance nearly by 10 times\\n\\n    - Optimize the memory usage of the time data type and decrease the memory usage of the time data type by fifty percent\\n\\n    - Optimize the point select performance and improve the point select efficiency result of Sysbench by 60%\\n\\n    - Improve the performance of TiDB on inserting or updating wide tables by 20 times\\n\\n    - Support configuring the memory upper limit of a single statement in the configuration file\\n\\n    - Optimize the execution of Hash Join, if the Join type is Inner Join or Semi Join and the inner table is empty, return the result without reading data from the outer table\\n\\n    - Support using the [`EXPLAIN ANALYZE` statement](/sql-statements/sql-statement-explain-analyze.md) to check the runtime statistics including the execution time and the number of returned rows of each operator\\n\\n+ Statistics\\n\\n    - Support enabling auto ANALYZE statistics only during certain period of the day\\n\\n    - Support updating the table statistics automatically according to the feedback of the queries\\n\\n    - Support configuring the number of buckets in the histogram using the `ANALYZE TABLE WITH BUCKETS` statement\\n\\n    - Optimize the Row Count estimation algorithm using histogram for mixed queries of equality query and range queries\\n\\n+ Expressions\\n\\n    + Support following built-in function:\\n\\n        - `json_contains`\\n\\n        - `json_contains_path`\\n\\n        - `encode/decode`\\n\\n+ Server\\n\\n    - Support queuing the locally conflicted transactions within tidb-server instance to optimize the performance of conflicted transactions\\n\\n    - Support Server Side Cursor\\n\\n    + Add the [HTTP API](https://github.com/pingcap/tidb/blob/release-2.1/docs/tidb_http_api.md)\\n\\n        - Scatter the distribution of table Regions in the TiKV cluster\\n\\n        - Control whether to open the `general log`\\n\\n        - Support modifying the log level online\\n\\n        - Check the TiDB cluster information\\n\\n    - [Add the `auto_analyze_ratio` system variables to control the ratio of Analyze](/faq/sql-faq.md#whats-the-trigger-strategy-for-auto-analyze-in-tidb)\\n\\n    - [Add the `tidb_retry_limit` system variable to control the automatic retry times of transactions](/system-variables.md#tidb_retry_limit)\\n\\n    - [Add the `tidb_disable_txn_auto_retry` system variable to control whether the transaction retries automatically](/system-variables.md#tidb_disable_txn_auto_retry)\\n\\n    - [Support using`admin show slow` statement to obtain the slow queries](/identify-slow-queries.md#admin-show-slow-command)\\n\\n    - [Add the `tidb_slow_log_threshold` environment variable to set the threshold of slow log automatically](/system-variables.md#tidb_slow_log_threshold)\\n\\n    - [Add the `tidb_query_log_max_len` environment variable to set the length of the SQL statement to be truncated in the log dynamically](/system-variables.md#tidb_query_log_max_len)\\n\\n+ DDL\\n\\n    - Support the parallel execution of the Add index statement and other statements to avoid the time consuming Add index operation blocking other operations\\n\\n    - Optimize the execution speed of `ADD INDEX` and improve it greatly in some scenarios\\n\\n    - Support the `select tidb_is_ddl_owner()` statement to facilitate deciding whether TiDB is `DDL Owner`\\n\\n    - Support the `ALTER TABLE FORCE` syntax\\n\\n    - Support the `ALTER TABLE RENAME KEY TO` syntax\\n\\n    - Add the table name and database name in the output information of `admin show ddl jobs`\\n\\n    - [Support using the `ddl/owner/resign` HTTP interface to release the DDL owner and start electing a new DDL owner](https://github.com/pingcap/tidb/blob/release-2.1/docs/tidb_http_api.md)\\n\\n+ Compatibility\\n\\n    - Support more MySQL syntaxes\\n\\n    - Make the `BIT` aggregate function support the `ALL` parameter\\n\\n    - Support the `SHOW PRIVILEGES` statement\\n\\n    - Support the `CHARACTER SET` syntax in the `LOAD DATA` statement\\n\\n    - Support the `IDENTIFIED WITH` syntax in the `CREATE USER` statement\\n\\n    - Support the `LOAD DATA IGNORE LINES` statement\\n\\n    - The `Show ProcessList` statement returns more accurate information\\n\\n## Placement Driver (PD)\\n\\n+ Optimize availability\\n\\n    - Introduce the version control mechanism and support rolling update of the cluster compatibly\\n\\n    - [Enable `Raft PreVote`](https://github.com/pingcap/pd/blob/5c7b18cf3af91098f07cf46df0b59fbf8c7c5462/conf/config.toml#L22) among PD nodes to avoid leader reelection when network recovers after network isolation\\n\\n    - Enable `raft learner` by default to lower the risk of unavailable data caused by machine failure during scheduling\\n\\n    - TSO allocation is no longer affected by the system clock going backwards\\n\\n    - Support the `Region merge` feature to reduce the overhead brought by metadata\\n\\n+ Optimize the scheduler\\n\\n    - Optimize the processing of Down Store to speed up making up replicas\\n\\n    - Optimize the hotspot scheduler to improve its adaptability when traffic statistics information jitters\\n\\n    - Optimize the start of Coordinator to reduce the unnecessary scheduling caused by restarting PD\\n\\n    - Optimize the issue that Balance Scheduler schedules small Regions frequently\\n\\n    - Optimize Region merge to consider the number of rows within the Region\\n\\n    - [Add more commands to control the scheduling policy](/pd-control.md#config-show--set-option-value--placement-rules)\\n\\n    - Improve [PD simulator](https://github.com/pingcap/pd/tree/release-2.1/tools/pd-simulator) to simulate the scheduling scenarios\\n\\n+ API and operation tools\\n\\n    - Add the [`GetPrevRegion` interface](https://github.com/pingcap/kvproto/blob/8e3f33ac49297d7c93b61a955531191084a2f685/proto/pdpb.proto#L40) to support the `TiDB reverse scan` feature\\n\\n    - Add the [`BatchSplitRegion` interface](https://github.com/pingcap/kvproto/blob/8e3f33ac49297d7c93b61a955531191084a2f685/proto/pdpb.proto#L54) to speed up TiKV Region splitting\\n\\n    - Add the [`GCSafePoint` interface](https://github.com/pingcap/kvproto/blob/8e3f33ac49297d7c93b61a955531191084a2f685/proto/pdpb.proto#L64-L66) to support distributed GC in TiDB\\n\\n    - Add the [`GetAllStores` interface](https://github.com/pingcap/kvproto/blob/8e3f33ac49297d7c93b61a955531191084a2f685/proto/pdpb.proto#L32), to support distributed GC in TiDB\\n\\n    + pd-ctl supports:\\n        - [using statistics for Region split](/pd-control.md#operator-check--show--add--remove)\\n\\n        - [calling `jq` to format the JSON output](/pd-control.md#jq-formatted-json-output-usage)\\n\\n        - [checking the Region information of the specified store](/pd-control.md#region-store-store_id)\\n\\n        - [checking topN Region list sorted by versions](/pd-control.md#region-topconfver-limit)\\n\\n        - [checking topN Region list sorted by size](/pd-control.md#region-topsize-limit)\\n\\n        - [more precise TSO encoding](/pd-control.md#tso)\\n\\n    - [pd-recover](/pd-recover.md) doesn't need to provide the `max-replica` parameter\\n\\n+ Metrics\\n\\n    - Add related metrics for `Filter`\\n\\n    - Add metrics about etcd Raft state machine\\n\\n+ Performance\\n\\n    - Optimize the performance of Region heartbeat to reduce the memory overhead brought by heartbeats\\n\\n    - Optimize the Region tree performance\\n\\n    - Optimize the performance of computing hotspot statistics\\n\\n## TiKV\\n\\n+ Coprocessor\\n\\n    - Add more built-in functions\\n\\n    - [Add Coprocessor `ReadPool` to improve the concurrency in processing the requests](https://github.com/tikv/rfcs/blob/master/text/0010-read-pool.md)\\n\\n    - Fix the time function parsing issue and the time zone related issues\\n\\n    - Optimize the memory usage for pushdown aggregation computing\\n\\n+ Transaction\\n\\n    - Optimize the read logic and memory usage of MVCC to improve the performance of the scan operation and the performance of full table scan is 1 time better than that in TiDB 2.0\\n\\n    - Fold the continuous Rollback records to ensure the read performance\\n\\n    - [Add the `UnsafeDestroyRange` API to support to collecting space for the dropping table/index](https://github.com/tikv/rfcs/blob/master/text/0002-unsafe-destroy-range.md)\\n\\n    - Separate the GC module to reduce the impact on write\\n\\n    - Add the `upper bound` support in the `kv_scan` command\\n\\n+ Raftstore\\n\\n    - Improve the snapshot writing process to avoid RocksDB stall\\n\\n    - [Add the `LocalReader` thread to process read requests and reduce the delay for read requests](https://github.com/tikv/rfcs/pull/17)\\n\\n    - [Support `BatchSplit` to avoid large Region brought by large amounts of write](https://github.com/tikv/rfcs/pull/6)\\n\\n    - Support `Region Split` according to statistics to reduce the I/O overhead\\n\\n    - Support `Region Split` according to the number of keys to improve the concurrency of index scan\\n\\n    - Improve the Raft message process to avoid unnecessary delay brought by `Region Split`\\n\\n    - Enable the `PreVote` feature by default to reduce the impact of network isolation on services\\n\\n+ Storage Engine\\n\\n    - Fix the `CompactFiles`bug in RocksDB and reduce the impact on importing data using Lightning\\n\\n    - Upgrade RocksDB to v5.15 to fix the possible issue of snapshot file corruption\\n\\n    - Improve `IngestExternalFile` to avoid the issue that flush could block write\\n\\n+ tikv-ctl\\n\\n    - [Add the `ldb` command to diagnose RocksDB related issues](https://tikv.org/docs/3.0/reference/tools/tikv-ctl/#ldb-command)\\n\\n    - The `compact` command supports specifying whether to compact data in the bottommost level\\n\\n## Tools\\n\\n- Fast full import of large amounts of data: [TiDB Lightning](/tidb-lightning/tidb-lightning-overview.md)\\n\\n- Support new [TiDB Binlog](/tidb-binlog/tidb-binlog-overview.md)\\n\\n## Upgrade caveat\\n\\n- TiDB 2.1 does not support downgrading to v2.0.x or earlier due to the adoption of the new storage engine\\n\\n+ Parallel DDL is enabled in TiDB 2.1, so the clusters with TiDB version earlier than 2.0.1 cannot upgrade to 2.1 using rolling update. You can choose either of the following two options:\\n\\n    - Stop the cluster and upgrade to 2.1 directly\\n    - Roll update to 2.0.1 or later 2.0.x versions, and then roll update to the 2.1 version\\n\\n- If you upgrade from TiDB 2.0.6 or earlier to TiDB 2.1, check if there is any ongoing DDL operation, especially the time consuming `Add Index` operation, because the DDL operations slow down the upgrading process. If there is ongoing DDL operation, wait for the DDL operation finishes and then roll update.\\n\", doc_link='https://docs.pingcap.com/tidb/v8.1/release-2.1-ga'),\n",
       " 15810: DocumentData(id=15810, chunks={}, content='---\\ntitle: TiDB 7.1.0 Release Notes\\nsummary: Learn about the new features, compatibility changes, improvements, and bug fixes in TiDB 7.1.0.\\n---\\n\\n# TiDB 7.1.0 Release Notes\\n\\nRelease date: May 31, 2023\\n\\nTiDB version: 7.1.0\\n\\nQuick access: [Quick start](https://docs.pingcap.com/tidb/v7.1/quick-start-with-tidb) | [Production deployment](https://docs.pingcap.com/tidb/v7.1/production-deployment-using-tiup)\\n\\nTiDB 7.1.0 is a Long-Term Support Release (LTS).\\n\\nCompared with the previous LTS 6.5.0, 7.1.0 not only includes new features, improvements, and bug fixes released in [6.6.0-DMR](/releases/release-6.6.0.md), [7.0.0-DMR](/releases/release-7.0.0.md), but also introduces the following key features and improvements:\\n\\n<table>\\n<thead>\\n  <tr>\\n    <th>Category</th>\\n    <th>Feature</th>\\n    <th>Description</th>\\n  </tr>\\n</thead>\\n<tbody>\\n  <tr>\\n    <td rowspan=\"4\">Scalability and Performance</td>\\n    <td>TiFlash supports the <a href=\"https://docs.pingcap.com/tidb/v7.1/tiflash-disaggregated-and-s3\" target=\"_blank\">disaggregated storage and compute architecture and S3 shared storage</a> (experimental, introduced in v7.0.0)</td>\\n    <td>TiFlash introduces a cloud-native architecture as an option:\\n      <ul>\\n        <li>Disaggregates TiFlash\\'s compute and storage, which is a milestone for elastic HTAP resource utilization.</li>\\n        <li>Introduces S3-based storage engine, which can provide shared storage at a lower cost.</li>\\n      </ul>\\n    </td>\\n  </tr>\\n  <tr>\\n    <td>TiKV supports <a href=\"https://docs.pingcap.com/tidb/v7.1/system-variables#tidb_store_batch_size\" target=\"_blank\">batch aggregating data requests</a> (introduced in v6.6.0) </td>\\n    <td>This enhancement significantly reduces total RPCs in TiKV batch-get operations. In situations where data is highly dispersed and the gRPC thread pool has insufficient resources, batching coprocessor requests can improve performance by more than 50%.</td>\\n  </tr>\\n  <tr>\\n    <td><a href=\"https://docs.pingcap.com/tidb/v7.1/troubleshoot-hot-spot-issues#scatter-read-hotspots\" target=\"_blank\">Load-based replica read</a></td>\\n    <td>In a read hotspot scenario, TiDB can redirect read requests for a hotspot TiKV node to its replicas. This feature efficiently scatters read hotspots and optimizes the use of cluster resources. To control the threshold for triggering load-based replica read, you can adjust the system variable <a href=\"https://docs.pingcap.com/tidb/v7.1/system-variables#tidb_load_based_replica_read_threshold-new-in-v700\" target=\"_blank\"><code>tidb_load_based_replica_read_threshold</code></a>.</td>\\n  </tr>\\n  <tr>\\n      <td>TiKV supports<a href=\"https://docs.pingcap.com/tidb/v7.1/partitioned-raft-kv\" target=\"_blank\"> partitioned Raft KV storage engine </a> (experimental)</td>\\n    <td>TiKV introduces a new generation of storage engine, the partitioned Raft KV. By allowing each data Region to have a dedicated RocksDB instance, it can expand the cluster\\'s storage capacity from TB-level to PB-level and provide more stable write latency and stronger scalability.</td>\\n    </tr>\\n  <tr>\\n    <td rowspan=\"2\">Reliability and availability</td>\\n    <td><a href=\"https://docs.pingcap.com/tidb/v7.1/tidb-resource-control\" target=\"_blank\">Resource control by resource groups</a> (GA)</td>\\n   <td>Support resource management based on resource groups, which allocates and isolates resources for different workloads in the same cluster. This feature significantly enhances the stability of multi-application clusters and lays the foundation for multi-tenancy. In v7.1.0, this feature introduces the ability to estimate system capacity based on actual workload or hardware deployment.</td>\\n  </tr>\\n  <tr>\\n    <td>TiFlash supports <a href=\"https://docs.pingcap.com/tidb/v7.1/tiflash-spill-disk\" target=\"_blank\">spill to disk</a> (introduced in v7.0.0)</td>\\n    <td>TiFlash supports intermediate result spill to disk to mitigate OOMs in data-intensive operations such as aggregations, sorts, and hash joins.</td>\\n  </tr>\\n  <tr>\\n    <td rowspan=\"3\">SQL</td>\\n    <td><a href=\"https://docs.pingcap.com/tidb/v7.1/sql-statement-create-index#multi-valued-indexes\" target=\"_blank\">Multi-valued indexes</a> (GA)</td>\\n    <td>Support MySQL-compatible multi-valued indexes and enhance the JSON type to improve compatibility with MySQL 8.0. This feature improves the efficiency of membership checks on multi-valued columns.</td>\\n  </tr>\\n  <tr>\\n    <td><a href=\"https://docs.pingcap.com/tidb/v7.1/time-to-live\" target=\"_blank\">Row-level TTL</a> (GA in v7.0.0)</td>\\n    <td>Support managing database size and improve performance by automatically expiring data of a certain age.</td>\\n  </tr>\\n  <tr>\\n    <td><a href=\"https://docs.pingcap.com/tidb/v7.1/generated-columns\" target=\"_blank\">Generated columns</a> (GA)</td>\\n    <td>Values in a generated column are calculated by a SQL expression in the column definition in real time. This feature pushes some application logic to the database level, thus improving query efficiency.</td>\\n  </tr>\\n  <tr>\\n    <td rowspan=\"2\">Security</td>\\n    <td><a href=\"https://docs.pingcap.com/tidb/v7.1/security-compatibility-with-mysql\" target=\"_blank\">LDAP authentication</a></td>\\n    <td>TiDB supports LDAP authentication, which is compatible with <a href=\"https://dev.mysql.com/doc/refman/8.0/en/ldap-pluggable-authentication.html\" target=\"_blank\">MySQL 8.0</a>.</td>\\n  </tr>\\n  <tr>\\n    <td><a href=\"https://static.pingcap.com/files/2023/09/18204824/TiDB-Database-Auditing-User-Guide1.pdf\" target=\"_blank\">Audit log enhancement</a> (<a href=\"https://www.pingcap.com/tidb-enterprise\" target=\"_blank\">Enterprise Edition</a> only)</td>\\n    <td>TiDB Enterprise Edition enhances the database auditing feature. It significantly improves the system auditing capacity by providing more fine-grained event filtering controls, more user-friendly filter settings, a new file output format in JSON, and lifecycle management of audit logs.</td>\\n  </tr>\\n</tbody>\\n</table>\\n\\n## Feature details\\n\\n### Performance\\n\\n* Enhance the Partitioned Raft KV storage engine (experimental) [#11515](https://github.com/tikv/tikv/issues/11515) [#12842](https://github.com/tikv/tikv/issues/12842) @[busyjay](https://github.com/busyjay) @[tonyxuqqi](https://github.com/tonyxuqqi) @[tabokie](https://github.com/tabokie) @[bufferflies](https://github.com/bufferflies) @[5kbpers](https://github.com/5kbpers) @[SpadeA-Tang](https://github.com/SpadeA-Tang) @[nolouch](https://github.com/nolouch)\\n\\n    TiDB v6.6.0 introduces the Partitioned Raft KV storage engine as an experimental feature, which uses multiple RocksDB instances to store TiKV Region data, and the data of each Region is independently stored in a separate RocksDB instance. The new storage engine can better control the number and level of files in the RocksDB instance, achieve physical isolation of data operations between Regions, and support stably managing more data. Compared with the original TiKV storage engine, using the Partitioned Raft KV storage engine can achieve about twice the write throughput and reduce the elastic scaling time by about 4/5 under the same hardware conditions and mixed read and write scenarios.\\n\\n    In TiDB v7.1.0, the Partitioned Raft KV storage engine supports tools such as TiDB Lightning, BR, and TiCDC.\\n\\n    Currently, this feature is experimental and not recommended for use in production environments. You can only use this engine in a newly created cluster and you cannot directly upgrade from the original TiKV storage engine.\\n\\n    For more information, see [documentation](/partitioned-raft-kv.md).\\n\\n* TiFlash supports late materialization (GA) [#5829](https://github.com/pingcap/tiflash/issues/5829) @[Lloyd-Pottiger](https://github.com/Lloyd-Pottiger)\\n\\n    In v7.0.0, late materialization was introduced in TiFlash as an experimental feature for optimizing query performance. This feature is disabled by default (the [`tidb_opt_enable_late_materialization`](/system-variables.md#tidb_opt_enable_late_materialization-new-in-v700) system variable defaults to `OFF`). When processing a `SELECT` statement with filter conditions (`WHERE` clause), TiFlash reads all the data from the columns required by the query, and then filters and aggregates the data based on the query conditions. When Late materialization is enabled, TiDB supports pushing down part of the filter conditions to the TableScan operator. That is, TiFlash first scans the column data related to the filter conditions that are pushed down to the TableScan operator, filters the rows that meet the condition, and then scans the other column data of these rows for further calculation, thereby reducing IO scans and computations of data processing.\\n\\n    Starting from v7.1.0, the TiFlash late materialization feature is generally available and enabled by default (the [`tidb_opt_enable_late_materialization`](/system-variables.md#tidb_opt_enable_late_materialization-new-in-v700) system variable defaults to `ON`). The TiDB optimizer decides which filters to be pushed down to the TableScan operator based on the statistics and the filter conditions of the query.\\n\\n    For more information, see [documentation](/tiflash/tiflash-late-materialization.md).\\n\\n* TiFlash supports automatically choosing an MPP Join algorithm according to the overhead of network transmission [#7084](https://github.com/pingcap/tiflash/issues/7084) @[solotzg](https://github.com/solotzg)\\n\\n    The TiFlash MPP mode supports multiple Join algorithms. Before v7.1.0, TiDB determines whether the MPP mode uses the Broadcast Hash Join algorithm based on the [`tidb_broadcast_join_threshold_count`](/system-variables.md#tidb_broadcast_join_threshold_count-new-in-v50) and [`tidb_broadcast_join_threshold_size`](/system-variables.md#tidb_broadcast_join_threshold_size-new-in-v50) variables and the actual data volume.\\n\\n    In v7.1.0, TiDB introduces the [`tidb_prefer_broadcast_join_by_exchange_data_size`](/system-variables.md#tidb_prefer_broadcast_join_by_exchange_data_size-new-in-v710) variable, which controls whether to choose the MPP Join algorithm based on the minimum overhead of network transmission. This variable is disabled by default, indicating that the default algorithm selection method remains the same as that before v7.1.0. You can set the variable to `ON` to enable it. When it is enabled, you no longer need to manually adjust the [`tidb_broadcast_join_threshold_count`](/system-variables.md#tidb_broadcast_join_threshold_count-new-in-v50) and [`tidb_broadcast_join_threshold_size`](/system-variables.md#tidb_broadcast_join_threshold_size-new-in-v50) variables (both variables does not take effect at this time), TiDB automatically estimates the threshold of network transmission by different Join algorithms, and then chooses the algorithm with the smallest overhead overall, thus reducing network traffic and improving MPP query performance.\\n\\n    For more information, see [documentation](/tiflash/use-tiflash-mpp-mode.md#algorithm-support-for-the-mpp-mode).\\n\\n* Support load-based replica read to mitigate read hotspots [#14151](https://github.com/tikv/tikv/issues/14151) @[sticnarf](https://github.com/sticnarf) @[you06](https://github.com/you06)\\n\\n    In a read hotspot scenario, the hotspot TiKV node cannot process read requests in time, resulting in the read requests queuing. However, not all TiKV resources are exhausted at this time. To reduce latency, TiDB v7.1.0 introduces the load-based replica read feature, which allows TiDB to read data from other TiKV nodes without queuing on the hotspot TiKV node. You can control the queue length of read requests using the [`tidb_load_based_replica_read_threshold`](/system-variables.md#tidb_load_based_replica_read_threshold-new-in-v700) system variable. When the estimated queue time of the leader node exceeds this threshold, TiDB prioritizes reading data from follower nodes. This feature can improve read throughput by 70% to 200% in a read hotspot scenario compared to not scattering read hotspots.\\n\\n    For more information, see [documentation](/troubleshoot-hot-spot-issues.md#scatter-read-hotspots).\\n\\n* Enhance the capability of caching execution plans for non-prepared statements (experimental) [#36598](https://github.com/pingcap/tidb/issues/36598) @[qw4990](https://github.com/qw4990)\\n\\n    TiDB v7.0.0 introduces non-prepared plan cache as an experimental feature to improve the load capacity of concurrent OLTP. In v7.1.0, TiDB enhances this feature and supports caching more SQL statements.\\n\\n    To improve memory utilization, TiDB v7.1.0 merges the cache pools of non-prepared and prepared plan caches. You can control the cache size using the system variable [`tidb_session_plan_cache_size`](/system-variables.md#tidb_session_plan_cache_size-new-in-v710). The [`tidb_prepared_plan_cache_size`](/system-variables.md#tidb_prepared_plan_cache_size-new-in-v610) and [`tidb_non_prepared_plan_cache_size`](/system-variables.md#tidb_non_prepared_plan_cache_size) system variables are deprecated.\\n\\n    To maintain forward compatibility, when you upgrade from an earlier version to v7.1.0 or later versions, the cache size `tidb_session_plan_cache_size` remains the same value as `tidb_prepared_plan_cache_size`, and [`tidb_enable_non_prepared_plan_cache`](/system-variables.md#tidb_enable_non_prepared_plan_cache) remains the setting before the upgrade. After sufficient performance testing, you can enable non-prepared plan cache using `tidb_enable_non_prepared_plan_cache`. For a newly created cluster, non-prepared plan cache is enabled by default.\\n\\n    Non-prepared plan cache does not support DML statements by default. To remove this restriction, you can set the [`tidb_enable_non_prepared_plan_cache_for_dml`](/system-variables.md#tidb_enable_non_prepared_plan_cache_for_dml-new-in-v710) system variable to `ON`.\\n\\n    For more information, see [documentation](/sql-non-prepared-plan-cache.md).\\n\\n* Support the TiDB Distributed eXecution Framework (DXF) (experimental) [#41495](https://github.com/pingcap/tidb/issues/41495) @[benjamin2037](https://github.com/benjamin2037)\\n\\n    Before TiDB v7.1.0, only one TiDB node can serve as the DDL owner and execute DDL tasks at the same time. Starting from TiDB v7.1.0, in the new DXF, multiple TiDB nodes can execute the same DDL task in parallel, thus better utilizing the resources of the TiDB cluster and significantly improving the performance of DDL. In addition, you can linearly improve the performance of DDL by adding more TiDB nodes. Note that this feature is currently experimental and only supports `ADD INDEX` operations.\\n\\n    To use the DXF, set the value of [`tidb_enable_dist_task`](/system-variables.md#tidb_enable_dist_task-new-in-v710) to `ON`:\\n\\n    ```sql\\n    SET GLOBAL tidb_enable_dist_task = ON;\\n    ```\\n\\n    For more information, see [documentation](/tidb-distributed-execution-framework.md).\\n\\n### Reliability\\n\\n* Resource Control becomes generally available (GA) [#38825](https://github.com/pingcap/tidb/issues/38825) @[nolouch](https://github.com/nolouch) @[BornChanger](https://github.com/BornChanger) @[glorv](https://github.com/glorv) @[tiancaiamao](https://github.com/tiancaiamao) @[Connor1996](https://github.com/Connor1996) @[JmPotato](https://github.com/JmPotato) @[hnes](https://github.com/hnes) @[CabinfeverB](https://github.com/CabinfeverB) @[HuSharp](https://github.com/HuSharp)\\n\\n    TiDB enhances the resource control feature based on resource groups, which becomes GA in v7.1.0. This feature significantly improves the resource utilization efficiency and performance of TiDB clusters. The introduction of the resource control feature is a milestone for TiDB. You can divide a distributed database cluster into multiple logical units, map different database users to corresponding resource groups, and set the quota for each resource group as needed. When the cluster resources are limited, all resources used by sessions in the same resource group are limited to the quota. In this way, even if a resource group is over-consumed, the sessions in other resource groups are not affected.\\n\\n    With this feature, you can combine multiple small and medium-sized applications from different systems into a single TiDB cluster. When the workload of an application grows larger, it does not affect the normal operation of other applications. When the system workload is low, busy applications can still be allocated the required system resources even if they exceed the set quotas, which can achieve the maximum utilization of resources. In addition, the rational use of the resource control feature can reduce the number of clusters, ease the difficulty of operation and maintenance, and save management costs.\\n\\n    In TiDB v7.1.0, this feature introduces the ability to estimate system capacity based on actual workload or hardware deployment. The estimation ability provides you with a more accurate reference for capacity planning and assists you in better managing TiDB resource allocation to meet the stability needs of enterprise-level scenarios.\\n\\n    To improve user experience, TiDB Dashboard provides the [Resource Manager page](/dashboard/dashboard-resource-manager.md). You can view the resource group configuration on this page and estimate cluster capacity in a visual way to facilitate reasonable resource allocation.\\n\\n    For more information, see [documentation](/tidb-resource-control.md).\\n\\n* Support the checkpoint mechanism for Fast Online DDL to improve fault tolerance and automatic recovery capability [#42164](https://github.com/pingcap/tidb/issues/42164) @[tangenta](https://github.com/tangenta)\\n\\n    TiDB v7.1.0 introduces a checkpoint mechanism for [Fast Online DDL](/ddl-introduction.md), which significantly improves the fault tolerance and automatic recovery capability of Fast Online DDL. Even if the TiDB owner node is restarted or changed due to failures, TiDB can still recover progress from checkpoints that are automatically updated on a regular basis, making the DDL execution more stable and efficient.\\n\\n    For more information, see [documentation](/system-variables.md#tidb_ddl_enable_fast_reorg-new-in-v630).\\n\\n* Backup & Restore supports checkpoint restore [#42339](https://github.com/pingcap/tidb/issues/42339) @[Leavrth](https://github.com/Leavrth)\\n\\n    Snapshot restore or log restore might be interrupted due to recoverable errors, such as disk exhaustion and node crash. Before TiDB v7.1.0, the recovery progress before the interruption would be invalidated even after the error is addressed, and you need to start the restore from scratch. For large clusters, this incurs considerable extra cost.\\n\\n    Starting from TiDB v7.1.0, Backup & Restore (BR) introduces the checkpoint restore feature, which enables you to continue an interrupted restore. This feature can retain most recovery progress of the interrupted restore.\\n\\n    For more information, see [documentation](/br/br-checkpoint-restore.md).\\n\\n* Optimize the strategy of loading statistics [#42160](https://github.com/pingcap/tidb/issues/42160) @[xuyifangreeneyes](https://github.com/xuyifangreeneyes)\\n\\n   TiDB v7.1.0 introduces lightweight statistics initialization as an experimental feature. Lightweight statistics initialization can significantly reduce the number of statistics that must be loaded during startup, thus improving the speed of loading statistics. This feature increases the stability of TiDB in complex runtime environments and reduces the impact on the overall service when TiDB nodes restart. You can set the parameter [`lite-init-stats`](/tidb-configuration-file.md#lite-init-stats-new-in-v710) to `true` to enable this feature.\\n\\n    During TiDB startup, SQL statements executed before the initial statistics are fully loaded might have suboptimal execution plans, thus causing performance issues. To avoid such issues, TiDB v7.1.0 introduces the configuration parameter [`force-init-stats`](/tidb-configuration-file.md#force-init-stats-new-in-v657-and-v710). With this option, you can control whether TiDB provides services only after statistics initialization has been finished during startup. This parameter is disabled by default.\\n\\n    For more information, see [documentation](/statistics.md#load-statistics).\\n\\n* TiCDC supports the data integrity validation feature for single-row data [#8718](https://github.com/pingcap/tiflow/issues/8718) [#42747](https://github.com/pingcap/tidb/issues/42747) @[3AceShowHand](https://github.com/3AceShowHand) @[zyguan](https://github.com/zyguan)\\n\\n    Starting from v7.1.0, TiCDC introduces the data integrity validation feature, which uses a checksum algorithm to validate the integrity of single-row data. This feature helps verify whether any error occurs in the process of writing data from TiDB, replicating it through TiCDC, and then writing it to a Kafka cluster. The data integrity validation feature only supports changefeeds that use Kafka as the downstream and currently supports the Avro protocol.\\n\\n    For more information, see [documentation](/ticdc/ticdc-integrity-check.md).\\n\\n* TiCDC optimizes DDL replication operations [#8686](https://github.com/pingcap/tiflow/issues/8686) @[hi-rustin](https://github.com/Rustin170506)\\n\\n    Before v7.1.0, when you perform a DDL operation that affects all rows on a large table (such as adding or deleting a column), the replication latency of TiCDC would significantly increase. Starting from v7.1.0, TiCDC optimizes this replication operation and mitigates the impact of DDL operations on downstream latency.\\n\\n    For more information, see [documentation](/ticdc/ticdc-faq.md#does-ticdc-replicate-data-changes-caused-by-lossy-ddl-operations-to-the-downstream).\\n\\n* Improve the stability of TiDB Lightning when importing TiB-level data [#43510](https://github.com/pingcap/tidb/issues/43510) [#43657](https://github.com/pingcap/tidb/issues/43657) @[D3Hunter](https://github.com/D3Hunter) @[lance6716](https://github.com/lance6716)\\n\\n    Starting from v7.1.0, TiDB Lightning has added four configuration items to improve stability when importing TiB-level data.\\n\\n    - `tikv-importer.region-split-batch-size` controls the number of Regions when splitting Regions in a batch. The default value is `4096`.\\n    - `tikv-importer.region-split-concurrency` controls the concurrency when splitting Regions. The default value is the number of CPU cores.\\n    - `tikv-importer.region-check-backoff-limit` controls the number of retries to wait for the Region to come online after the split and scatter operations. The default value is `1800` and the maximum retry interval is two seconds. The number of retries is not increased if any Region becomes online between retries.\\n    - `tikv-importer.pause-pd-scheduler-scope` controls the scope in which TiDB Lightning pauses PD scheduling. Value options are `\"table\"` and `\"global\"`. The default value is `\"table\"`. For TiDB versions earlier than v6.1.0, you can only configure the `\"global\"` option, which pauses global scheduling during data import. Starting from v6.1.0, the `\"table\"` option is supported, which means that scheduling is only paused for the Region that stores the target table data. It is recommended to set this configuration item to `\"global\"` in scenarios with large data volumes to improve stability.\\n\\n  For more information, see [documentation](/tidb-lightning/tidb-lightning-configuration.md).\\n\\n### SQL\\n\\n* Support saving TiFlash query results using the `INSERT INTO SELECT` statement (GA) [#37515](https://github.com/pingcap/tidb/issues/37515) @[gengliqi](https://github.com/gengliqi)\\n\\n    Starting from v6.5.0, TiDB supports pushing down the `SELECT` clause (analytical query) of the `INSERT INTO SELECT` statement to TiFlash. In this way, you can easily save the TiFlash query result to a TiDB table specified by `INSERT INTO` for further analysis, which takes effect as result caching (that is, result materialization).\\n\\n    In v7.1.0, this feature is generally available. During the execution of the `SELECT` clause in the `INSERT INTO SELECT` statement, the optimizer can intelligently decide whether to push a query down to TiFlash based on the [SQL mode](/sql-mode.md) and the cost estimates of the TiFlash replica. Therefore, the `tidb_enable_tiflash_read_for_write_stmt` system variable introduced during the experimental phase is now deprecated. Note that the computation rules of `INSERT INTO SELECT` statements for TiFlash do not meet the `STRICT SQL Mode` requirement, so TiDB allows the `SELECT` clause in the `INSERT INTO SELECT` statement to be pushed down to TiFlash only when the [SQL mode](/sql-mode.md) of the current session is not strict, which means that the `sql_mode` value does not contain `STRICT_TRANS_TABLES` and `STRICT_ALL_TABLES`.\\n\\n    For more information, see [documentation](/tiflash/tiflash-results-materialization.md).\\n\\n* MySQL-compatible multi-valued indexes become generally available (GA) [#39592](https://github.com/pingcap/tidb/issues/39592) @[xiongjiwei](https://github.com/xiongjiwei) @[qw4990](https://github.com/qw4990) @[YangKeao](https://github.com/YangKeao)\\n\\n    Filtering the values of an array in a JSON column is a common operation, but normal indexes cannot help speed up such an operation. Creating a multi-valued index on an array can greatly improve filtering performance. If an array in the JSON column has a multi-valued index, you can use the multi-valued index to filter retrieval conditions in `MEMBER OF()`, `JSON_CONTAINS()`, and `JSON_OVERLAPS()` functions, thereby reducing I/O consumption and improving operation speed.\\n\\n    In v7.1.0, the multi-valued indexes feature becomes generally available (GA). It supports more complete data types and is compatible with TiDB tools. You can use multi-valued indexes to speed up the search operations on JSON arrays in production environments.\\n\\n    For more information, see [documentation](/sql-statements/sql-statement-create-index.md#multi-valued-indexes).\\n\\n* Improve the partition management for Hash and Key partitioned tables [#42728](https://github.com/pingcap/tidb/issues/42728) @[mjonss](https://github.com/mjonss)\\n\\n    Before v7.1.0, Hash and Key partitioned tables in TiDB only support the `TRUNCATE PARTITION` partition management statement. Starting from v7.1.0, Hash and Key partitioned tables also support `ADD PARTITION` and `COALESCE PARTITION` partition management statements. Therefore, you can flexibly adjust the number of partitions in Hash and Key partitioned tables as needed. For example, you can increase the number of partitions with the `ADD PARTITION` statement, or decrease the number of partitions with the `COALESCE PARTITION` statement.\\n\\n    For more information, see [documentation](/partitioned-table.md#manage-hash-and-key-partitions).\\n\\n* The syntax of Range INTERVAL partitioning becomes generally available (GA) [#35683](https://github.com/pingcap/tidb/issues/35683) @[mjonss](https://github.com/mjonss)\\n\\n    The syntax of Range INTERVAL partitioning (introduced in v6.3.0) becomes GA. With this syntax, you can define Range partitioning by a desired interval without enumerating all partitions, which drastically reduces the length of Range partitioning DDL statements. The syntax is equivalent to that of the original Range partitioning.\\n\\n    For more information, see [documentation](/partitioned-table.md#range-interval-partitioning).\\n\\n* Generated columns become generally available (GA) @[bb7133](https://github.com/bb7133)\\n\\n    Generated columns are a valuable feature for a database. When creating a table, you can define that the value of a column is calculated based on the values of other columns in the table, rather than being explicitly inserted or updated by users. This generated column can be either a virtual column or a stored column. TiDB has supported MySQL-compatible generated columns since earlier versions, and this feature becomes GA in v7.1.0.\\n\\n    Using generated columns can improve MySQL compatibility for TiDB, simplifying the process of migrating from MySQL. It also reduces data maintenance complexity and improves data consistency and query efficiency.\\n\\n    For more information, see [documentation](/generated-columns.md).\\n\\n### DB operations\\n\\n* Support smooth cluster upgrade without manually canceling DDL operations (experimental) [#39751](https://github.com/pingcap/tidb/issues/39751) @[zimulala](https://github.com/zimulala)\\n\\n    Before TiDB v7.1.0, to upgrade a cluster, you must manually cancel its running or queued DDL tasks before the upgrade and then add them back after the upgrade.\\n\\n    To provide a smoother upgrade experience, TiDB v7.1.0 supports automatically pausing and resuming DDL tasks. Starting from v7.1.0, you can upgrade your clusters without manually canceling DDL tasks in advance. TiDB will automatically pause any running or queued user DDL tasks before the upgrade and resume these tasks after the rolling upgrade, making it easier for you to upgrade your TiDB clusters.\\n\\n    For more information, see [documentation](/smooth-upgrade-tidb.md).\\n\\n### Observability\\n\\n* Enhance optimizer diagnostic information [#43122](https://github.com/pingcap/tidb/issues/43122) @[time-and-fate](https://github.com/time-and-fate)\\n\\n    Obtaining sufficient information is the key to SQL performance diagnostics. In v7.1.0, TiDB continues to add optimizer runtime information to various diagnostic tools, providing better insights into how execution plans are selected and assisting in troubleshooting SQL performance issues. The new information includes:\\n\\n    * `debug_trace.json` in the output of [`PLAN REPLAYER`](/sql-plan-replayer.md).\\n    * Partial statistics details for `operator info` in the output of [`EXPLAIN`](/explain-walkthrough.md).\\n    * Partial statistics details in the `Stats` field of [slow queries](/identify-slow-queries.md).\\n\\n  For more information, see [Use `PLAN REPLAYER` to save and restore the on-site information of a cluster](/sql-plan-replayer.md), [`EXPLAIN` walkthrough](/explain-walkthrough.md), and [Identify slow queries](/identify-slow-queries.md).\\n\\n### Security\\n\\n* Replace the interface used for querying TiFlash system table information [#6941](https://github.com/pingcap/tiflash/issues/6941) @[flowbehappy](https://github.com/flowbehappy)\\n\\n    Starting from v7.1.0, when providing the query service of [`INFORMATION_SCHEMA.TIFLASH_TABLES`](/information-schema/information-schema-tiflash-tables.md) and [`INFORMATION_SCHEMA.TIFLASH_SEGMENTS`](/information-schema/information-schema-tiflash-segments.md) system tables for TiDB, TiFlash uses the gRPC port instead of the HTTP port, which avoids the security risks of the HTTP service.\\n\\n* Support LDAP authentication [#43580](https://github.com/pingcap/tidb/issues/43580) @[YangKeao](https://github.com/YangKeao)\\n\\n    Starting from v7.1.0, TiDB supports LDAP authentication and provides two authentication plugins: `authentication_ldap_sasl` and `authentication_ldap_simple`.\\n\\n    For more information, see [documentation](/security-compatibility-with-mysql.md).\\n\\n* Enhance the database auditing feature (Enterprise Edition)\\n\\n    In v7.1.0, TiDB Enterprise Edition enhances the database auditing feature, which significantly expands its capacity and improves the user experience to meet the needs of enterprises for database security compliance:\\n\\n    - Introduce the concepts of \"Filter\" and \"Rule\" for more granular audit event definitions and more fine-grained audit settings.\\n    - Support defining rules in JSON format, providing a more user-friendly configuration method.\\n    - Add automatic log rotation and space management functions, and support configuring log rotation in two dimensions: retention time and log size.\\n    - Support outputting audit logs in both TEXT and JSON formats, facilitating easier integration with third-party tools.\\n    - Support audit log redaction. You can replace all literals to enhance security.\\n\\n  Database auditing is an important feature in TiDB Enterprise Edition. This feature provides a powerful monitoring and auditing tool for enterprises to ensure data security and compliance. It can help enterprise managers in tracking the source and impact of database operations to prevent illegal data theft or tampering. Furthermore, database auditing can also help enterprises meet various regulatory and compliance requirements, ensuring legal and ethical compliance. This feature has important application value for enterprise information security.\\n\\n    For more information, see [user guide](https://static.pingcap.com/files/2023/09/18204824/TiDB-Database-Auditing-User-Guide1.pdf). This feature is included in TiDB Enterprise Edition. To use this feature, navigate to the [TiDB Enterprise](https://www.pingcap.com/tidb-enterprise) page to get TiDB Enterprise Edition.\\n\\n## Compatibility changes\\n\\n> **Note:**\\n>\\n> This section provides compatibility changes you need to know when you upgrade from v7.0.0 to the current version (v7.1.0). If you are upgrading from v6.6.0 or earlier versions to the current version, you might also need to check the compatibility changes introduced in intermediate versions.\\n\\n### Behavior changes\\n\\n* To improve security, TiFlash deprecates the HTTP service port (default `8123`) and uses the gRPC port as a replacement\\n\\n    If you have upgraded TiFlash to v7.1.0, then during the TiDB upgrade to v7.1.0, TiDB cannot read the TiFlash system tables ([`INFORMATION_SCHEMA.TIFLASH_TABLES`](/information-schema/information-schema-tiflash-tables.md) and [`INFORMATION_SCHEMA.TIFLASH_SEGMENTS`](/information-schema/information-schema-tiflash-segments.md)).\\n\\n* TiDB Lightning in TiDB versions from v6.2.0 to v7.0.0 decides whether to pause global scheduling based on the TiDB cluster version. When TiDB cluster version >= v6.1.0, scheduling is only paused for the Region that stores the target table data and is resumed after the target table import is complete. While for other versions, TiDB Lightning pauses global scheduling. Starting from TiDB v7.1.0, you can control whether to pause global scheduling by configuring [`pause-pd-scheduler-scope`](/tidb-lightning/tidb-lightning-configuration.md). By default, TiDB Lightning pauses scheduling for the Region that stores the target table data. If the target cluster version is earlier than v6.1.0, an error occurs. In this case, you can change the value of the parameter to `\"global\"` and try again.\\n\\n* When you use [`FLASHBACK CLUSTER TO TIMESTAMP`](/sql-statements/sql-statement-flashback-cluster.md) in TiDB v7.1.0, some Regions might remain in the FLASHBACK process even after the completion of the FLASHBACK operation. It is recommended to avoid using this feature in v7.1.0. For more information, see issue [#44292](https://github.com/pingcap/tidb/issues/44292). If you have encountered this issue, you can use the [TiDB snapshot backup and restore](/br/br-snapshot-guide.md) feature to restore data.\\n\\n### System variables\\n\\n| Variable name | Change type | Description |\\n|--------|------------------------------|------|\\n| [`tidb_enable_tiflash_read_for_write_stmt`](/system-variables.md#tidb_enable_tiflash_read_for_write_stmt-new-in-v630) | Deprecated | Changes the default value from `OFF` to `ON`. When [`tidb_allow_mpp = ON`](/system-variables.md#tidb_allow_mpp-new-in-v50), the optimizer intelligently decides whether to push a query down to TiFlash based on the [SQL mode](/sql-mode.md) and the cost estimates of the TiFlash replica. |\\n| [`tidb_non_prepared_plan_cache_size`](/system-variables.md#tidb_non_prepared_plan_cache_size) | Deprecated | Starting from v7.1.0, this system variable is deprecated. You can use [`tidb_session_plan_cache_size`](/system-variables.md#tidb_session_plan_cache_size-new-in-v710) to control the maximum number of plans that can be cached. |\\n| [`tidb_prepared_plan_cache_size`](/system-variables.md#tidb_prepared_plan_cache_size-new-in-v610) | Deprecated | Starting from v7.1.0, this system variable is deprecated. You can use [`tidb_session_plan_cache_size`](/system-variables.md#tidb_session_plan_cache_size-new-in-v710) to control the maximum number of plans that can be cached. |\\n| `tidb_ddl_distribute_reorg` | Deleted | This variable is renamed to [`tidb_enable_dist_task`](/system-variables.md#tidb_enable_dist_task-new-in-v710). |\\n| [`default_authentication_plugin`](/system-variables.md#default_authentication_plugin) | Modified | Introduces two new value options: `authentication_ldap_sasl` and `authentication_ldap_simple`. |\\n| [`tidb_load_based_replica_read_threshold`](/system-variables.md#tidb_load_based_replica_read_threshold-new-in-v700) | Modified | Takes effect starting from v7.1.0 and controls the threshold for triggering load-based replica read. Changes the default value from `\"0s\"` to `\"1s\"` after further tests. |\\n| [`tidb_opt_enable_late_materialization`](/system-variables.md#tidb_opt_enable_late_materialization-new-in-v700) | Modified | Changes the default value from `OFF` to `ON`, meaning that the TiFlash late materialization feature is enabled by default. |\\n| [`authentication_ldap_sasl_auth_method_name`](/system-variables.md#authentication_ldap_sasl_auth_method_name-new-in-v710) | Newly added | Specifies the authentication method name in LDAP SASL authentication. |\\n| [`authentication_ldap_sasl_bind_base_dn`](/system-variables.md#authentication_ldap_sasl_bind_base_dn-new-in-v710) | Newly added | Limits the search scope within the search tree in LDAP SASL authentication. If a user is created without `AS ...` clause, TiDB automatically searches the `dn` in LDAP server according to the user name. |\\n| [`authentication_ldap_sasl_bind_root_dn`](/system-variables.md#authentication_ldap_sasl_bind_root_dn-new-in-v710) | Newly added | Specifies the `dn` used to login to the LDAP server to search users in LDAP SASL authentication. |\\n| [`authentication_ldap_sasl_bind_root_pwd`](/system-variables.md#authentication_ldap_sasl_bind_root_pwd-new-in-v710) | Newly added | Specifies the password used to login to the LDAP server to search users in LDAP SASL authentication. |\\n| [`authentication_ldap_sasl_ca_path`](/system-variables.md#authentication_ldap_sasl_ca_path-new-in-v710) | Newly added | Specifies the absolute path of the certificate authority file for StartTLS connections in LDAP SASL authentication. |\\n| [`authentication_ldap_sasl_init_pool_size`](/system-variables.md#authentication_ldap_sasl_init_pool_size-new-in-v710) | Newly added | Specifies the initial connections in the connection pool to the LDAP server in LDAP SASL authentication. |\\n| [`authentication_ldap_sasl_max_pool_size`](/system-variables.md#authentication_ldap_sasl_max_pool_size-new-in-v710) | Newly added | Specifies the maximum connections in the connection pool to the LDAP server in LDAP SASL authentication. |\\n| [`authentication_ldap_sasl_server_host`](/system-variables.md#authentication_ldap_sasl_server_host-new-in-v710) | Newly added | Specifies the LDAP server host in LDAP SASL authentication. |\\n| [`authentication_ldap_sasl_server_port`](/system-variables.md#authentication_ldap_sasl_server_port-new-in-v710) | Newly added | Specifies the LDAP server TCP/IP port number in LDAP SASL authentication. |\\n| [`authentication_ldap_sasl_tls`](/system-variables.md#authentication_ldap_sasl_tls-new-in-v710) | Newly added | Specifies whether connections by the plugin to the LDAP server are protected with StartTLS in LDAP SASL authentication. |\\n| [`authentication_ldap_simple_auth_method_name`](/system-variables.md#authentication_ldap_simple_auth_method_name-new-in-v710) | Newly added | Specifies the authentication method name in LDAP simple authentication. It only supports `SIMPLE`. |\\n| [`authentication_ldap_simple_bind_base_dn`](/system-variables.md#authentication_ldap_simple_bind_base_dn-new-in-v710) | Newly added | Limits the search scope within the search tree in LDAP simple authentication. If a user is created without `AS ...` clause, TiDB will automatically search the `dn` in LDAP server according to the user name. |\\n| [`authentication_ldap_simple_bind_root_dn`](/system-variables.md#authentication_ldap_simple_bind_root_dn-new-in-v710) | Newly added | Specifies the `dn` used to login to the LDAP server to search users in LDAP simple authentication. |\\n| [`authentication_ldap_simple_bind_root_pwd`](/system-variables.md#authentication_ldap_simple_bind_root_pwd-new-in-v710) | Newly added | Specifies the password used to login to the LDAP server to search users in LDAP simple authentication. |\\n| [`authentication_ldap_simple_ca_path`](/system-variables.md#authentication_ldap_simple_ca_path-new-in-v710) | Newly added | Specifies the absolute path of the certificate authority file for StartTLS connections in LDAP simple authentication. |\\n| [`authentication_ldap_simple_init_pool_size`](/system-variables.md#authentication_ldap_simple_init_pool_size-new-in-v710) | Newly added | Specifies the initial connections in the connection pool to the LDAP server in LDAP simple authentication. |\\n| [`authentication_ldap_simple_max_pool_size`](/system-variables.md#authentication_ldap_simple_max_pool_size-new-in-v710) | Newly added | Specifies the maximum connections in the connection pool to the LDAP server in LDAP simple authentication. |\\n| [`authentication_ldap_simple_server_host`](/system-variables.md#authentication_ldap_simple_server_host-new-in-v710) | Newly added | Specifies the LDAP server host in LDAP simple authentication. |\\n| [`authentication_ldap_simple_server_port`](/system-variables.md#authentication_ldap_simple_server_port-new-in-v710) | Newly added | Specifies the LDAP server TCP/IP port number in LDAP simple authentication. |\\n| [`authentication_ldap_simple_tls`](/system-variables.md#authentication_ldap_simple_tls-new-in-v710) | Newly added | Specifies whether connections by the plugin to the LDAP server are protected with StartTLS in LDAP simple authentication. |\\n| [`tidb_enable_dist_task`](/system-variables.md#tidb_enable_dist_task-new-in-v710) | Newly added | Controls whether to enable the Distributed eXecution Framework (DXF). After enabling the DXF, DDL, import, and other supported DXF tasks will be jointly completed by multiple TiDB nodes in the cluster. This variable was renamed from `tidb_ddl_distribute_reorg`. |\\n| [`tidb_enable_non_prepared_plan_cache_for_dml`](/system-variables.md#tidb_enable_non_prepared_plan_cache_for_dml-new-in-v710) | Newly added | Controls whether to enable the [Non-prepared plan cache](/sql-non-prepared-plan-cache.md) feature for DML statements. |\\n| [`tidb_enable_row_level_checksum`](/system-variables.md#tidb_enable_row_level_checksum-new-in-v710) | Newly added | Controls whether to enable the TiCDC data integrity validation for single-row data feature.|\\n| [`tidb_opt_fix_control`](/system-variables.md#tidb_opt_fix_control-new-in-v653-and-v710) | Newly added | This variable provides more fine-grained control over the optimizer and helps to prevent performance regression after upgrading caused by behavior changes in the optimizer. |\\n| [`tidb_plan_cache_invalidation_on_fresh_stats`](/system-variables.md#tidb_plan_cache_invalidation_on_fresh_stats-new-in-v710) | Newly added | Controls whether to invalidate the plan cache automatically when statistics on related tables are updated. |\\n| [`tidb_plan_cache_max_plan_size`](/system-variables.md#tidb_plan_cache_max_plan_size-new-in-v710) | Newly added | Controls the maximum size of a plan that can be cached in prepared or non-prepared plan cache. |\\n| [`tidb_prefer_broadcast_join_by_exchange_data_size`](/system-variables.md#tidb_prefer_broadcast_join_by_exchange_data_size-new-in-v710) | Newly added | Controls whether to use the algorithm with the minimum overhead of network transmission. If this variable is enabled, TiDB estimates the size of the data to be exchanged in the network using `Broadcast Hash Join` and `Shuffled Hash Join` respectively, and then chooses the one with the smaller size. [`tidb_broadcast_join_threshold_count`](/system-variables.md#tidb_broadcast_join_threshold_count-new-in-v50) and [`tidb_broadcast_join_threshold_size`](/system-variables.md#tidb_broadcast_join_threshold_size-new-in-v50) will not take effect after this variable is enabled. |\\n| [`tidb_session_plan_cache_size`](/system-variables.md#tidb_session_plan_cache_size-new-in-v710) | Newly added | Controls the maximum number of plans that can be cached. Prepared plan cache and non-prepared plan cache share the same cache. |\\n\\n### Configuration file parameters\\n\\n| Configuration file | Configuration parameter | Change type | Description |\\n| -------- | -------- | -------- | -------- |\\n| TiDB | [`performance.force-init-stats`](/tidb-configuration-file.md#force-init-stats-new-in-v657-and-v710) | Newly added | Controls whether to wait for statistics initialization to finish before providing services during TiDB startup. |\\n| TiDB | [`performance.lite-init-stats`](/tidb-configuration-file.md#lite-init-stats-new-in-v710) | Newly added | Controls whether to use lightweight statistics initialization during TiDB startup. |\\n| TiDB | [`log.timeout`](/tidb-configuration-file.md#timeout-new-in-v710) | Newly added | Sets the timeout for log-writing operations in TiDB. In case of a disk failure that prevents logs from being written, this configuration item can trigger the TiDB process to panic instead of hang. The default value is `0`, which means no timeout is set. |\\n| TiKV | [`region-compact-min-redundant-rows`](/tikv-configuration-file.md#region-compact-min-redundant-rows-new-in-v710) | Newly added | Sets the number of redundant MVCC rows required to trigger RocksDB compaction. The default value is `50000`. |\\n| TiKV | [`region-compact-redundant-rows-percent`](/tikv-configuration-file.md#region-compact-redundant-rows-percent-new-in-v710) | Newly added | Sets the percentage of redundant MVCC rows required to trigger RocksDB compaction. The default value is `20`. |\\n| TiKV | [`split.byte-threshold`](/tikv-configuration-file.md#byte-threshold-new-in-v50) | Modified | Changes the default value from `30MiB` to `100MiB` when [`region-split-size`](/tikv-configuration-file.md#region-split-size) is greater than or equal to 4 GB. |\\n| TiKV | [`split.qps-threshold`](/tikv-configuration-file.md#qps-threshold) | Modified | Changes the default value from `3000` to `7000` when [`region-split-size`](/tikv-configuration-file.md#region-split-size) is greater than or equal to 4 GB. |\\n| TiKV | [`split.region-cpu-overload-threshold-ratio`](/tikv-configuration-file.md#region-cpu-overload-threshold-ratio-new-in-v620) | Modified | Changes the default value from `0.25` to `0.75` when [`region-split-size`](/tikv-configuration-file.md#region-split-size) is greater than or equal to 4 GB. |\\n| TiKV | [`region-compact-check-step`](/tikv-configuration-file.md#region-compact-check-step) | Modified | Changes the default value from `100` to `5` when Partitioned Raft KV is enabled (`storage.engine=\"partitioned-raft-kv\"`). |\\n| PD | [`store-limit-version`](/pd-configuration-file.md#store-limit-version-new-in-v710) | Newly added | Controls the mode of store limit. Value options are `\"v1\"` and `\"v2\"`. |\\n| PD | [`schedule.enable-diagnostic`](/pd-configuration-file.md#enable-diagnostic-new-in-v630) | Modified | Changes the default value from `false` to `true`, meaning that the diagnostic feature of scheduler is enabled by default. |\\n| TiFlash | `http_port` | Deleted | Deprecates the HTTP service port (default `8123`). |\\n| TiDB Lightning | [`tikv-importer.pause-pd-scheduler-scope`](/tidb-lightning/tidb-lightning-configuration.md) | Newly added | Controls the scope in which TiDB Lightning pauses PD scheduling. The default value is `\"table\"` and value options are `\"global\"` and `\"table\"`. |\\n| TiDB Lightning | [`tikv-importer.region-check-backoff-limit`](/tidb-lightning/tidb-lightning-configuration.md) | Newly added | Controls the number of retries to wait for the Region to come online after the split and scatter operations. The default value is `1800`. The maximum retry interval is two seconds. The number of retries is not increased if any Region becomes online between retries.|\\n| TiDB Lightning | [`tikv-importer.region-split-batch-size`](/tidb-lightning/tidb-lightning-configuration.md) | Newly added | Controls the number of Regions when splitting Regions in a batch. The default value is `4096`. |\\n| TiDB Lightning | [`tikv-importer.region-split-concurrency`](/tidb-lightning/tidb-lightning-configuration.md) | Newly added | Controls the concurrency when splitting Regions. The default value is the number of CPU cores. |\\n| TiCDC | [`insecure-skip-verify`](/ticdc/ticdc-sink-to-kafka.md) | Newly added | Controls whether the authentication algorithm is set when TLS is enabled in the scenario of replicating data to Kafka. |\\n| TiCDC | [`integrity.corruption-handle-level`](/ticdc/ticdc-changefeed-config.md#cli-and-configuration-parameters-of-ticdc-changefeeds) | Newly added | Specifies the log level of the Changefeed when the checksum validation for single-row data fails. The default value is `\"warn\"`. Value options are `\"warn\"` and `\"error\"`. |\\n| TiCDC | [`integrity.integrity-check-level`](/ticdc/ticdc-changefeed-config.md#cli-and-configuration-parameters-of-ticdc-changefeeds) | Newly added | Controls whether to enable the checksum validation for single-row data. The default value is `\"none\"`, which means to disable the feature. |\\n| TiCDC | [`sink.only-output-updated-columns`](/ticdc/ticdc-changefeed-config.md#cli-and-configuration-parameters-of-ticdc-changefeeds) | Newly added | Controls whether to only output the updated columns. The default value is `false`. |\\n| TiCDC | [`sink.enable-partition-separator`](/ticdc/ticdc-changefeed-config.md#cli-and-configuration-parameters-of-ticdc-changefeeds) | Modified | Changes the default value from `false` to `true` after further tests, meaning that partitions in a table are stored in separate directories by default. It is recommended that you keep the value as `true` to avoid the potential issue of data loss during replication of partitioned tables to storage services. |\\n\\n## Improvements\\n\\n+ TiDB\\n\\n    - Display the number of distinct values for the corresponding column in the Cardinality column of the `SHOW INDEX` result [#42227](https://github.com/pingcap/tidb/issues/42227) @[winoros](https://github.com/winoros)\\n    - Use `SQL_NO_CACHE` to prevent TTL Scan queries from impacting the TiKV block cache [#43206](https://github.com/pingcap/tidb/issues/43206) @[lcwangchao](https://github.com/lcwangchao)\\n    - Improve an error message related to `MAX_EXECUTION_TIME` to make it compatible with MySQL [#43031](https://github.com/pingcap/tidb/issues/43031) @[dveeden](https://github.com/dveeden)\\n    - Support using the MergeSort operator on partitioned tables in IndexLookUp [#26166](https://github.com/pingcap/tidb/issues/26166) @[Defined2014](https://github.com/Defined2014)\\n    - Enhance `caching_sha2_password` to make it compatible with MySQL [#43576](https://github.com/pingcap/tidb/issues/43576) @[asjdf](https://github.com/asjdf)\\n\\n+ TiKV\\n\\n    - Reduce the impact of split operations on write QPS when using partitioned Raft KV [#14447](https://github.com/tikv/tikv/issues/14447) @[SpadeA-Tang](https://github.com/SpadeA-Tang)\\n    - Optimize the space occupied by snapshots when using partitioned Raft KV [#14581](https://github.com/tikv/tikv/issues/14581) @[bufferflies](https://github.com/bufferflies)\\n    - Provide more detailed time information for each stage of processing requests in TiKV [#12362](https://github.com/tikv/tikv/issues/12362) @[cfzjywxk](https://github.com/cfzjywxk)\\n    - Use PD as metastore in log backup [#13867](https://github.com/tikv/tikv/issues/13867) @[YuJuncen](https://github.com/YuJuncen)\\n\\n+ PD\\n\\n    - Add a controller that automatically adjusts the size of the store limit based on the execution details of the snapshot. To enable this controller, set `store-limit-version` to `v2`. Once enabled, you do not need to manually adjust the `store limit` configuration to control the speed of scaling in or scaling out [#6147](https://github.com/tikv/pd/issues/6147) @[bufferflies](https://github.com/bufferflies)\\n    - Add historical load information to avoid frequent scheduling of Regions with unstable loads by the hotspot scheduler when the storage engine is raft-kv2 [#6297](https://github.com/tikv/pd/issues/6297) @[bufferflies](https://github.com/bufferflies)\\n    - Add a leader health check mechanism. When the PD server where the etcd leader is located cannot be elected as the leader, PD actively switches the etcd leader to ensure that the PD leader is available [#6403](https://github.com/tikv/pd/issues/6403) @[nolouch](https://github.com/nolouch)\\n\\n+ TiFlash\\n\\n    - Improve TiFlash performance and stability in the disaggregated storage and compute architecture [#6882](https://github.com/pingcap/tiflash/issues/6882) @[JaySon-Huang](https://github.com/JaySon-Huang) @[breezewish](https://github.com/breezewish) @[JinheLin](https://github.com/JinheLin)\\n    - Support optimizing query performance in Semi Join or Anti Semi Join by selecting the smaller table as the build side [#7280](https://github.com/pingcap/tiflash/issues/7280) @[yibin87](https://github.com/yibin87)\\n    - Improve performance of data import from BR and TiDB Lightning to TiFlash with default configurations [#7272](https://github.com/pingcap/tiflash/issues/7272) @[breezewish](https://github.com/breezewish)\\n\\n+ Tools\\n\\n    + Backup & Restore (BR)\\n\\n        - Support modifying the TiKV configuration item `log-backup.max-flush-interval` during log backup [#14433](https://github.com/tikv/tikv/issues/14433) @[joccau](https://github.com/joccau)\\n\\n    + TiCDC\\n\\n        - Optimize the directory structure when DDL events occur in the scenario of replicating data to object storage [#8890](https://github.com/pingcap/tiflow/issues/8890) @[CharlesCheung96](https://github.com/CharlesCheung96)\\n        - Optimize the method of setting GC TLS for the upstream when the TiCDC replication task fails [#8403](https://github.com/pingcap/tiflow/issues/8403) @[charleszheng44](https://github.com/charleszheng44)\\n        - Support replicating data to the Kafka-on-Pulsar downstream [#8892](https://github.com/pingcap/tiflow/issues/8892) @[hi-rustin](https://github.com/Rustin170506)\\n        - Support using the open-protocol protocol to only replicate the changed columns after an update occurs when replicating data to Kafka [#8706](https://github.com/pingcap/tiflow/issues/8706) @[sdojjy](https://github.com/sdojjy)\\n        - Optimize the error handling of TiCDC in the downstream failures or other scenarios [#8657](https://github.com/pingcap/tiflow/issues/8657) @[hicqu](https://github.com/hicqu)\\n        - Add a configuration item `insecure-skip-verify` to control whether to set the authentication algorithm in the scenario of enabling TLS [#8867](https://github.com/pingcap/tiflow/issues/8867) @[hi-rustin](https://github.com/Rustin170506)\\n\\n    + TiDB Lightning\\n\\n        - Change the severity level of the precheck item related to uneven Region distribution from `Critical` to `Warn` to avoid blocking users from importing data [#42836](https://github.com/pingcap/tidb/issues/42836) @[okJiang](https://github.com/okJiang)\\n        - Add a retry mechanism when encountering an `unknown RPC` error during data import [#43291](https://github.com/pingcap/tidb/issues/43291) @[D3Hunter](https://github.com/D3Hunter)\\n        - Enhance the retry mechanism for Region jobs [#43682](https://github.com/pingcap/tidb/issues/43682) @[lance6716](https://github.com/lance6716)\\n\\n## Bug fixes\\n\\n+ TiDB\\n\\n    - Fix the issue that there is no prompt about manually executing `ANALYZE TABLE` after reorganizing partitions [#42183](https://github.com/pingcap/tidb/issues/42183) @[CbcWestwolf](https://github.com/CbcWestwolf)\\n    - Fix the issue of missing table names in the `ADMIN SHOW DDL JOBS` result when a `DROP TABLE` operation is being executed [#42268](https://github.com/pingcap/tidb/issues/42268) @[tiancaiamao](https://github.com/tiancaiamao)\\n    - Fix the issue that `Ignore Event Per Minute` and `Stats Cache LRU Cost` charts might not be displayed normally in the Grafana monitoring panel [#42562](https://github.com/pingcap/tidb/issues/42562) @[pingandb](https://github.com/pingandb)\\n    - Fix the issue that the `ORDINAL_POSITION` column returns incorrect results when querying the `INFORMATION_SCHEMA.COLUMNS` table [#43379](https://github.com/pingcap/tidb/issues/43379) @[bb7133](https://github.com/bb7133)\\n    - Fix the case sensitivity issue in some columns of the permission table [#41048](https://github.com/pingcap/tidb/issues/41048) @[bb7133](https://github.com/bb7133)\\n    - Fix the issue that after a new column is added in the cache table, the value is `NULL` instead of the default value of the column [#42928](https://github.com/pingcap/tidb/issues/42928) @[lqs](https://github.com/lqs)\\n    - Fix the issue that CTE results are incorrect when pushing down predicates [#43645](https://github.com/pingcap/tidb/issues/43645) @[winoros](https://github.com/winoros)\\n    - Fix the issue of DDL retry caused by write conflict when executing `TRUNCATE TABLE` for partitioned tables with many partitions and TiFlash replicas [#42940](https://github.com/pingcap/tidb/issues/42940) @[mjonss](https://github.com/mjonss)\\n    - Fix the issue that there is no warning when using `SUBPARTITION` in creating partitioned tables [#41198](https://github.com/pingcap/tidb/issues/41198) [#41200](https://github.com/pingcap/tidb/issues/41200) @[mjonss](https://github.com/mjonss)\\n    - Fix the incompatibility issue with MySQL when dealing with value overflow issues in generated columns [#40066](https://github.com/pingcap/tidb/issues/40066) @[jiyfhust](https://github.com/jiyfhust)\\n    - Fix the issue that `REORGANIZE PARTITION` cannot be concurrently executed with other DDL operations [#42442](https://github.com/pingcap/tidb/issues/42442) @[bb7133](https://github.com/bb7133)\\n    - Fix the issue that canceling the partition reorganization task in DDL might cause subsequent DDL operations to fail [#42448](https://github.com/pingcap/tidb/issues/42448) @[lcwangchao](https://github.com/lcwangchao)\\n    - Fix the issue that assertions on delete operations are incorrect under certain conditions [#42426](https://github.com/pingcap/tidb/issues/42426) @[tiancaiamao](https://github.com/tiancaiamao)\\n    - Fix the issue that TiDB server cannot start due to an error in reading the cgroup information with the error message \"can\\'t read file memory.stat from cgroup v1: open /sys/memory.stat no such file or directory\" [#42659](https://github.com/pingcap/tidb/issues/42659) @[hawkingrei](https://github.com/hawkingrei)\\n    - Fix the `Duplicate Key` issue that occurs when updating the partition key of a row on a partitioned table with a global index [#42312](https://github.com/pingcap/tidb/issues/42312) @[L-maple](https://github.com/L-maple)\\n    - Fix the issue that the `Scan Worker Time By Phase` chart in the TTL monitoring panel does not display data [#42515](https://github.com/pingcap/tidb/issues/42515) @[lcwangchao](https://github.com/lcwangchao)\\n    - Fix the issue that some queries on partitioned tables with a global index return incorrect results [#41991](https://github.com/pingcap/tidb/issues/41991) [#42065](https://github.com/pingcap/tidb/issues/42065) @[L-maple](https://github.com/L-maple)\\n    - Fix the issue of displaying some error logs during the process of reorganizing a partitioned table [#42180](https://github.com/pingcap/tidb/issues/42180) @[mjonss](https://github.com/mjonss)\\n    - Fix the issue that the data length in the `QUERY` column of the `INFORMATION_SCHEMA.DDL_JOBS` table might exceed the column definition [#42440](https://github.com/pingcap/tidb/issues/42440) @[tiancaiamao](https://github.com/tiancaiamao)\\n    - Fix the issue that the `INFORMATION_SCHEMA.CLUSTER_HARDWARE` table might display incorrect values in containers [#42851](https://github.com/pingcap/tidb/issues/42851) @[hawkingrei](https://github.com/hawkingrei)\\n    - Fix the issue that an incorrect result is returned when you query a partitioned table using `ORDER BY` + `LIMIT` [#43158](https://github.com/pingcap/tidb/issues/43158) @[Defined2014](https://github.com/Defined2014)\\n    - Fix the issue of multiple DDL tasks running simultaneously using the ingest method [#42903](https://github.com/pingcap/tidb/issues/42903) @[tangenta](https://github.com/tangenta)\\n    - Fix the wrong value returned when querying a partitioned table using `Limit` [#24636](https://github.com/pingcap/tidb/issues/24636)\\n    - Fix the issue of displaying the incorrect TiDB address in IPv6 environment [#43260](https://github.com/pingcap/tidb/issues/43260) @[nexustar](https://github.com/nexustar)\\n    - Fix the issue of displaying incorrect values for system variables `tidb_enable_tiflash_read_for_write_stmt` and `tidb_enable_exchange_partition` [#43281](https://github.com/pingcap/tidb/issues/43281) @[gengliqi](https://github.com/gengliqi)\\n    - Fix the issue that when `tidb_scatter_region` is enabled, Region does not automatically split after a partition is truncated [#43174](https://github.com/pingcap/tidb/issues/43174) [#43028](https://github.com/pingcap/tidb/issues/43028) @[jiyfhust](https://github.com/jiyfhust)\\n    - Add checks on the tables with generated columns and report errors for unsupported DDL operations on these columns [#38988](https://github.com/pingcap/tidb/issues/38988) [#24321](https://github.com/pingcap/tidb/issues/24321) @[tiancaiamao](https://github.com/tiancaiamao)\\n    - Fix the issue that the error message is incorrect in certain type conversion errors [#41730](https://github.com/pingcap/tidb/issues/41730) @[hawkingrei](https://github.com/hawkingrei)\\n    - Fix the issue that after a TiDB node is normally shutdown, DDL tasks triggered on this node will be canceled [#43854](https://github.com/pingcap/tidb/issues/43854) @[zimulala](https://github.com/zimulala)\\n    - Fix the issue that when the PD member address changes, allocating ID for the `AUTO_INCREMENT` column will be blocked for a long time [#42643](https://github.com/pingcap/tidb/issues/42643) @[tiancaiamao](https://github.com/tiancaiamao)\\n    - Fix the issue of reporting the `GC lifetime is shorter than transaction duration` error during DDL execution [#40074](https://github.com/pingcap/tidb/issues/40074) @[tangenta](https://github.com/tangenta)\\n    - Fix the issue that metadata locks unexpectedly block the DDL execution [#43755](https://github.com/pingcap/tidb/issues/43755) @[wjhuang2016](https://github.com/wjhuang2016)\\n    - Fix the issue that the cluster cannot query some system views in IPv6 environment [#43286](https://github.com/pingcap/tidb/issues/43286) @[Defined2014](https://github.com/Defined2014)\\n    - Fix the issue of not finding the partition during inner join in dynamic pruning mode [#43686](https://github.com/pingcap/tidb/issues/43686) @[mjonss](https://github.com/mjonss)\\n    - Fix the issue that TiDB reports syntax errors when analyzing tables [#43392](https://github.com/pingcap/tidb/issues/43392) @[guo-shaoge](https://github.com/guo-shaoge)\\n    - Fix the issue that TiCDC might lose some row changes during table renaming [#43338](https://github.com/pingcap/tidb/issues/43338) @[tangenta](https://github.com/tangenta)\\n    - Fix the issue that TiDB server crashes when the client uses cursor reads [#38116](https://github.com/pingcap/tidb/issues/38116) @[YangKeao](https://github.com/YangKeao)\\n    - Fix the issue that `ADMIN SHOW DDL JOBS LIMIT` returns incorrect results [#42298](https://github.com/pingcap/tidb/issues/42298) @[CbcWestwolf](https://github.com/CbcWestwolf)\\n    - Fix the TiDB panic issue that occurs when querying union views and temporary tables with `UNION` [#42563](https://github.com/pingcap/tidb/issues/42563) @[lcwangchao](https://github.com/lcwangchao)\\n    - Fix the issue that renaming tables does not take effect when committing multiple statements in a transaction [#39664](https://github.com/pingcap/tidb/issues/39664) @[tiancaiamao](https://github.com/tiancaiamao)\\n    - Fix the incompatibility issue between the behavior of prepared plan cache and non-prepared plan cache during time conversion [#42439](https://github.com/pingcap/tidb/issues/42439) @[qw4990](https://github.com/qw4990)\\n    - Fix the wrong results caused by plan cache for Decimal type [#43311](https://github.com/pingcap/tidb/issues/43311) @[qw4990](https://github.com/qw4990)\\n    - Fix the TiDB panic issue in null-aware anti join (NAAJ) due to the wrong field type check [#42459](https://github.com/pingcap/tidb/issues/42459) @[AilinKid](https://github.com/AilinKid)\\n    - Fix the issue that DML execution failures in pessimistic transactions at the RC isolation level might cause inconsistency between data and indexes [#43294](https://github.com/pingcap/tidb/issues/43294) @[ekexium](https://github.com/ekexium)\\n    - Fix the issue that in some extreme cases, when the first statement of a pessimistic transaction is retried, resolving locks on this transaction might affect transaction correctness [#42937](https://github.com/pingcap/tidb/issues/42937) @[MyonKeminta](https://github.com/MyonKeminta)\\n    - Fix the issue that in some rare cases, residual pessimistic locks of pessimistic transactions might affect data correctness when GC resolves locks [#43243](https://github.com/pingcap/tidb/issues/43243) @[MyonKeminta](https://github.com/MyonKeminta)\\n    - Fix the issue that the `LOCK` to `PUT` optimization leads to duplicate data being returned in specific queries [#28011](https://github.com/pingcap/tidb/issues/28011) @[zyguan](https://github.com/zyguan)\\n    - Fix the issue that when data is changed, the locking behavior of the unique index is not consistent with that when the data is unchanged [#36438](https://github.com/pingcap/tidb/issues/36438) @[zyguan](https://github.com/zyguan)\\n\\n+ TiKV\\n\\n    - Fix the issue that when you enable `tidb_pessimistic_txn_fair_locking`, in some extreme cases, expired requests caused by failed RPC retries might affect data correctness during the resolve lock operation [#14551](https://github.com/tikv/tikv/issues/14551) @[MyonKeminta](https://github.com/MyonKeminta)\\n    - Fix the issue that when you enable `tidb_pessimistic_txn_fair_locking`, in some extreme cases, expired requests caused by failed RPC retries might cause transaction conflicts to be ignored, thus affecting transaction consistency [#14311](https://github.com/tikv/tikv/issues/14311) @[MyonKeminta](https://github.com/MyonKeminta)\\n    - Fix the issue that encryption key ID conflict might cause the deletion of the old keys [#14585](https://github.com/tikv/tikv/issues/14585) @[tabokie](https://github.com/tabokie)\\n    - Fix the performance degradation issue caused by accumulated lock records when a cluster is upgraded from a previous version to v6.5 or later versions [#14780](https://github.com/tikv/tikv/issues/14780) @[MyonKeminta](https://github.com/MyonKeminta)\\n    - Fix the issue that the `raft entry is too large` error occurs during the PITR recovery process [#14313](https://github.com/tikv/tikv/issues/14313) @[YuJuncen](https://github.com/YuJuncen)\\n    - Fix the issue that TiKV panics during the PITR recovery process due to `log_batch` exceeding 2 GB [#13848](https://github.com/tikv/tikv/issues/13848) @[YuJuncen](https://github.com/YuJuncen)\\n\\n+ PD\\n\\n    - Fix the issue that the number of `low space store` in the PD monitoring panel is abnormal after TiKV panics [#6252](https://github.com/tikv/pd/issues/6252) @[HuSharp](https://github.com/HuSharp)\\n    - Fix the issue that Region Health monitoring data is deleted after PD leader switch [#6366](https://github.com/tikv/pd/issues/6366) @[iosmanthus](https://github.com/iosmanthus)\\n    - Fix the issue that the rule checker cannot repair unhealthy Regions with the `schedule=deny` label [#6426](https://github.com/tikv/pd/issues/6426) @[nolouch](https://github.com/nolouch)\\n    - Fix the issue that some existing labels are lost after TiKV or TiFlash restarts [#6467](https://github.com/tikv/pd/issues/6467) @[JmPotato](https://github.com/JmPotato)\\n    - Fix the issue that the replication status cannot be switched when there are learner nodes in the replication mode [#14704](https://github.com/tikv/tikv/issues/14704) @[nolouch](https://github.com/nolouch)\\n\\n+ TiFlash\\n\\n    - Fix the issue that querying data in the `TIMESTAMP` or `TIME` type returns errors after enabling late materialization [#7455](https://github.com/pingcap/tiflash/issues/7455) @[Lloyd-Pottiger](https://github.com/Lloyd-Pottiger)\\n    - Fix the issue that large update transactions might cause TiFlash to repeatedly report errors and restart [#7316](https://github.com/pingcap/tiflash/issues/7316) @[JaySon-Huang](https://github.com/JaySon-Huang)\\n\\n+ Tools\\n\\n    + Backup & Restore (BR)\\n\\n        - Fix the issue of backup slowdown when a TiKV node crashes in a cluster [#42973](https://github.com/pingcap/tidb/issues/42973) @[YuJuncen](https://github.com/YuJuncen)\\n        - Fix the issue of inaccurate error messages caused by a backup failure in some cases [#43236](https://github.com/pingcap/tidb/issues/43236) @[YuJuncen](https://github.com/YuJuncen)\\n\\n    + TiCDC\\n\\n        - Fix the issue of TiCDC time zone setting [#8798](https://github.com/pingcap/tiflow/issues/8798) @[hi-rustin](https://github.com/Rustin170506)\\n        - Fix the issue that TiCDC cannot automatically recover when PD address or leader fails [#8812](https://github.com/pingcap/tiflow/issues/8812) [#8877](https://github.com/pingcap/tiflow/issues/8877) @[asddongmen](https://github.com/asddongmen)\\n        - Fix the issue that checkpoint lag increases when one of the upstream TiKV nodes crashes [#8858](https://github.com/pingcap/tiflow/issues/8858) @[hicqu](https://github.com/hicqu)\\n        - Fix the issue that when replicating data to object storage, the `EXCHANGE PARTITION` operation in the upstream cannot be properly replicated to the downstream [#8914](https://github.com/pingcap/tiflow/issues/8914) @[CharlesCheung96](https://github.com/CharlesCheung96)\\n        - Fix the OOM issue caused by excessive memory usage of the sorter component in some special scenarios [#8974](https://github.com/pingcap/tiflow/issues/8974) @[hicqu](https://github.com/hicqu)\\n        - Fix the TiCDC node panic that occurs when the downstream Kafka sinks are rolling restarted [#9023](https://github.com/pingcap/tiflow/issues/9023) @[asddongmen](https://github.com/asddongmen)\\n\\n    + TiDB Data Migration (DM)\\n\\n        - Fix the issue that latin1 data might be corrupted during replication [#7028](https://github.com/pingcap/tiflow/issues/7028) @[lance6716](https://github.com/lance6716)\\n\\n    + TiDB Dumpling\\n\\n        - Fix the issue that the `UNSIGNED INTEGER` type primary key cannot be used for splitting chunks [#42620](https://github.com/pingcap/tidb/issues/42620) @[lichunzhu](https://github.com/lichunzhu)\\n        - Fix the issue that TiDB Dumpling might panic when `--output-file-template` is incorrectly set [#42391](https://github.com/pingcap/tidb/issues/42391) @[lichunzhu](https://github.com/lichunzhu)\\n\\n    + TiDB Binlog\\n\\n        - Fix the issue that an error might occur when encountering a failed DDL statement [#1228](https://github.com/pingcap/tidb-binlog/issues/1228) @[okJiang](https://github.com/okJiang)\\n\\n    + TiDB Lightning\\n\\n        - Fix the performance degradation issue during data import [#42456](https://github.com/pingcap/tidb/issues/42456) @[lance6716](https://github.com/lance6716)\\n        - Fix the issue of `write to tikv with no leader returned` when importing a large amount of data [#43055](https://github.com/pingcap/tidb/issues/43055) @[lance6716](https://github.com/lance6716)\\n        - Fix the issue of excessive `keys within region is empty, skip doIngest` logs during data import [#43197](https://github.com/pingcap/tidb/issues/43197) @[D3Hunter](https://github.com/D3Hunter)\\n        - Fix the issue that panic might occur during partial write [#43363](https://github.com/pingcap/tidb/issues/43363) @[lance6716](https://github.com/lance6716)\\n        - Fix the issue that OOM might occur when importing a wide table [#43728](https://github.com/pingcap/tidb/issues/43728) @[D3Hunter](https://github.com/D3Hunter)\\n        - Fix the issue of missing data in the TiDB Lightning Grafana dashboard [#43357](https://github.com/pingcap/tidb/issues/43357) @[lichunzhu](https://github.com/lichunzhu)\\n        - Fix the import failure due to incorrect setting of `keyspace-name` [#43684](https://github.com/pingcap/tidb/issues/43684) @[zeminzhou](https://github.com/zeminzhou)\\n        - Fix the issue that data import might be skipped during range partial write in some cases [#43768](https://github.com/pingcap/tidb/issues/43768) @[lance6716](https://github.com/lance6716)\\n\\n## Performance test\\n\\nTo learn about the performance of TiDB v7.1.0, you can refer to the [TPC-C performance test report](https://docs.pingcap.com/tidbcloud/v7.1.0-performance-benchmarking-with-tpcc) and [Sysbench performance test report](https://docs.pingcap.com/tidbcloud/v7.1.0-performance-benchmarking-with-sysbench) of the TiDB Cloud Dedicated cluster.\\n\\n## Contributors\\n\\nWe would like to thank the following contributors from the TiDB community:\\n\\n- [blacktear23](https://github.com/blacktear23)\\n- [ethercflow](https://github.com/ethercflow)\\n- [hihihuhu](https://github.com/hihihuhu)\\n- [jiyfhust](https://github.com/jiyfhust)\\n- [L-maple](https://github.com/L-maple)\\n- [lqs](https://github.com/lqs)\\n- [pingandb](https://github.com/pingandb)\\n- [yorkhellen](https://github.com/yorkhellen)\\n- [yujiarista](https://github.com/yujiarista) (First-time contributor)\\n', doc_link='https://docs.pingcap.com/tidb/v8.1/release-7.1.0'),\n",
       " 16126: DocumentData(id=16126, chunks={}, content='---\\ntitle: TiDB TPC-C Performance Test Report -- v5.4.0 vs. v5.3.0\\nsummary: TiDB v5.4.0 TPC-C performance is 3.16% better than v5.3.0. The improvement is consistent across different thread counts 2.80% (50 threads), 4.27% (100 threads), 3.45% (200 threads), and 2.11% (400 threads).\\n---\\n\\n# TiDB TPC-C Performance Test Report -- v5.4.0 vs. v5.3.0\\n\\n## Test overview\\n\\nThis test aims at comparing the TPC-C performance of TiDB v5.4.0 and v5.3.0 in the Online Transactional Processing (OLTP) scenario. The results show that compared with v5.3.0, the TPC-C performance of v5.4.0 is improved by 3.16%.\\n\\n## Test environment (AWS EC2）\\n\\n### Hardware configuration\\n\\n| Service type | EC2 type | Instance count |\\n|:----------|:----------|:----------|\\n| PD        | m5.xlarge |     3     |\\n| TiKV      | i3.4xlarge|     3     |\\n| TiDB      | c5.4xlarge|     3     |\\n| TPC-C  | c5.9xlarge|     1     |\\n\\n### Software version\\n\\n| Service type | Software version |\\n|:----------|:-----------|\\n| PD        | v5.3.0 and v5.4.0   |\\n| TiDB      | v5.3.0 and v5.4.0   |\\n| TiKV      | v5.3.0 and v5.4.0   |\\n| TiUP  | 1.5.1     |\\n\\n### Parameter configuration\\n\\nTiDB v5.4.0 and TiDB v5.3.0 use the same configuration.\\n\\n#### TiDB parameter configuration\\n\\n\\n```yaml\\nlog.level: \"error\"\\nperformance.max-procs: 20\\nprepared-plan-cache.enabled: true\\ntikv-client.max-batch-wait-time: 2000000\\n```\\n\\n#### TiKV parameter configuration\\n\\n\\n```yaml\\npessimistic-txn.pipelined: true\\nraftdb.allow-concurrent-memtable-write: true\\nraftdb.max-background-jobs: 4\\nraftstore.apply-max-batch-size: 2048\\nraftstore.apply-pool-size: 3\\nraftstore.store-max-batch-size: 2048\\nraftstore.store-pool-size: 3\\nreadpool.storage.normal-concurrency: 10\\nreadpool.unified.max-thread-count: 20\\nreadpool.unified.min-thread-count: 5\\nrocksdb.max-background-jobs: 8\\nserver.grpc-concurrency: 6\\nstorage.scheduler-worker-pool-size: 20\\n```\\n\\n#### TiDB global variable configuration\\n\\n\\n```sql\\nset global tidb_hashagg_final_concurrency=1;\\nset global tidb_hashagg_partial_concurrency=1;\\nset global tidb_enable_async_commit = 1;\\nset global tidb_enable_1pc = 1;\\nset global tidb_guarantee_linearizability = 0;\\nset global tidb_enable_clustered_index = 1;\\n```\\n\\n#### HAProxy configuration - haproxy.cfg\\n\\nFor more details about how to use HAProxy on TiDB, see [Best Practices for Using HAProxy in TiDB](/best-practices/haproxy-best-practices.md).\\n\\n\\n```yaml\\nglobal                                     # Global configuration.\\n   chroot      /var/lib/haproxy            # Changes the current directory and sets superuser privileges for the startup process to improve security.\\n   pidfile     /var/run/haproxy.pid        # Writes the PIDs of HAProxy processes into this file.\\n   maxconn     4000                        # The maximum number of concurrent connections for a single HAProxy process.\\n   user        haproxy                     # The same with the UID parameter.\\n   group       haproxy                     # The same with the GID parameter. A dedicated user group is recommended.\\n   nbproc      64                          # The number of processes created when going daemon. When starting multiple processes to forward requests, ensure that the value is large enough so that HAProxy does not block processes.\\n   daemon                                  # Makes the process fork into background. It is equivalent to the command line \"-D\" argument. It can be disabled by the command line \"-db\" argument.\\ndefaults                                   # Default configuration.\\n   log global                              # Inherits the settings of the global configuration.\\n   retries 2                               # The maximum number of retries to connect to an upstream server. If the number of connection attempts exceeds the value, the backend server is considered unavailable.\\n   timeout connect  2s                     # The maximum time to wait for a connection attempt to a backend server to succeed. It should be set to a shorter time if the server is located on the same LAN as HAProxy.\\n   timeout client 30000s                   # The maximum inactivity time on the client side.\\n   timeout server 30000s                   # The maximum inactivity time on the server side.\\nlisten tidb-cluster                        # Database load balancing.\\n   bind 0.0.0.0:3390                       # The Floating IP address and listening port.\\n   mode tcp                                # HAProxy uses layer 4, the transport layer.\\n   balance roundrobin                      # The server with the fewest connections receives the connection. \"leastconn\" is recommended where long sessions are expected, such as LDAP, SQL and TSE, rather than protocols using short sessions, such as HTTP. The algorithm is dynamic, which means that server weights might be adjusted on the fly for slow starts for instance.\\n   server tidb-1 10.9.18.229:4000 check inter 2000 rise 2 fall 3       # Detects port 4000 at a frequency of once every 2000 milliseconds. If it is detected as successful twice, the server is considered available; if it is detected as failed three times, the server is considered unavailable.\\n   server tidb-2 10.9.39.208:4000 check inter 2000 rise 2 fall 3\\n   server tidb-3 10.9.64.166:4000 check inter 2000 rise 2 fall 3\\n```\\n\\n### Prepare test data\\n\\n1. Deploy TiDB v5.4.0 and v5.3.0 using TiUP.\\n2. Create a database named `tpcc`: `create database tpcc;`.\\n3. Use BenchmarkSQL to import the TPC-C 5000 Warehouse data: `tiup bench tpcc prepare --warehouses 5000 --db tpcc -H 127.0.0.1 -P 4000`.\\n4. Run the `tiup bench tpcc run -U root --db tpcc --host 127.0.0.1 --port 4000 --time 1800s --warehouses 5000 --threads {{thread}}` command to perform stress tests on TiDB via HAProxy. For each concurrency, the test takes 30 minutes.\\n5. Extract the tpmC data of New Order from the result.\\n\\n## Test result\\n\\nCompared with v5.3.0, the TPC-C performance of v5.4.0 is **improved by 3.16%**.\\n\\n| Threads | v5.3.0 tpmC | v5.4.0 tpmC | tpmC improvement (%) |\\n|:----------|:----------|:----------|:----------|\\n|50|43002.4|44204.4|2.80|\\n|100|50162.7|52305|4.27|\\n|200|55768.2|57690.7|3.45|\\n|400|56836.8|58034.6|2.11|\\n\\n![TPC-C](https://download.pingcap.com/images/docs/tpcc_v530_vs_v540.png)\\n', doc_link='https://docs.pingcap.com/tidb/v8.1/v5.4-performance-benchmarking-with-tpcc'),\n",
       " 15841: DocumentData(id=15841, chunks={}, content='---\\ntitle: TiDB 5.0.4 Release Notes\\nsummary: Compatibility changes include fixes for slow `SHOW VARIABLES` execution, default value change for `tidb_stmt_summary_max_stmt_count`, and bug fixes that may cause upgrade incompatibilities. Feature enhancements include support for setting `tidb_enforce_mpp=1` and dynamic TiCDC configurations. Improvements cover auto-analyze trigger, MPP query retry support, and stable result mode. Bug fixes address various issues in TiDB, TiKV, PD, TiFlash, and tools like Dumpling and TiCDC.\\n---\\n\\n# TiDB 5.0.4 Release Notes\\n\\nRelease Date: September 27, 2021\\n\\nTiDB version: 5.0.4\\n\\n## Compatibility changes\\n\\n+ TiDB\\n\\n    - Fix the issue that executing `SHOW VARIABLES` in a new session is slow. This fix reverts some changes made in [#19341](https://github.com/pingcap/tidb/pull/19341) and might cause compatibility issues. [#24326](https://github.com/pingcap/tidb/issues/24326)\\n    - Change the default value of the `tidb_stmt_summary_max_stmt_count` variable from `200` to `3000` [#25873](https://github.com/pingcap/tidb/pull/25873)\\n    + The following bug fixes change execution results, which might cause upgrade incompatibilities:\\n        - Fix the issue that TiDB returns wrong result when the children of `UNION` contain the `NULL` value [#26559](https://github.com/pingcap/tidb/issues/26559)\\n        - Fix the issue that `greatest(datetime) union null` returns empty string [#26532](https://github.com/pingcap/tidb/issues/26532)\\n        - Fix the issue that the behavior of the `last_day` function is incompatible in SQL mode [#26000](https://github.com/pingcap/tidb/pull/26000)\\n        - Fix the issue that the `having` clause might not work correctly [#26496](https://github.com/pingcap/tidb/issues/26496)\\n        - Fix the wrong execution results that occur when the collations around the `between` expression are different [#27146](https://github.com/pingcap/tidb/issues/27146)\\n        - Fix the wrong execution results that occur when the column in the `group_concat` function has a non-bin collation [#27429](https://github.com/pingcap/tidb/issues/27429)\\n        - Fix an issue that using a `count(distinct)` expression on multiple columns returns wrong result when the new collation is enabled [#27091](https://github.com/pingcap/tidb/issues/27091)\\n        - Fix the result wrong that occurs when the argument of the `extract` function is a negative duration [#27236](https://github.com/pingcap/tidb/issues/27236)\\n        - Fix the issue that inserting an invalid date does not report an error when the `SQL_MODE` is \\'STRICT_TRANS_TABLES\\' [#26762](https://github.com/pingcap/tidb/issues/26762)\\n        - Fix the issue that using an invalid default date does not report an error when the `SQL_MODE` is \\'NO_ZERO_IN_DATE\\' [#26766](https://github.com/pingcap/tidb/issues/26766)\\n        - Fix a bug on the query range of prefix index [#26029](https://github.com/pingcap/tidb/issues/26029)\\n        - Fix the issue that the `LOAD DATA` statement might abnormally import non-utf8 data [#25979](https://github.com/pingcap/tidb/issues/25979)\\n        - Fix the issue that `insert ignore on duplicate update` might insert wrong data when the secondary index has the same column with the primary key [#25809](https://github.com/pingcap/tidb/issues/25809)\\n        - Fix the issue that `insert ignore duplicate update` might insert wrong data when a partitioned table has a clustered index [#25846](https://github.com/pingcap/tidb/issues/25846)\\n        - Fix the issue that the query result might be wrong when the key is the `ENUM` type in point get or batch point get [#24562](https://github.com/pingcap/tidb/issues/24562)\\n        - Fix the wrong result that occurs when dividing a `BIT`-type value [#23479](https://github.com/pingcap/tidb/issues/23479)\\n        - Fix the issue that the results of `prepared` statements and direct queries might be inconsistent [#22949](https://github.com/pingcap/tidb/issues/22949)\\n        - Fix the issue that the query result might be wrong when a `YEAR` type is compared with a string or an integer type [#23262](https://github.com/pingcap/tidb/issues/23262)\\n\\n## Feature enhancements\\n\\n+ TiDB\\n\\n    - Support setting `tidb_enforce_mpp=1` to ignore the optimizer estimation and forcibly use the MPP mode [#26382](https://github.com/pingcap/tidb/pull/26382)\\n\\n+ TiKV\\n\\n    - Support changing TiCDC configurations dynamically [#10645](https://github.com/tikv/tikv/issues/10645)\\n\\n+ PD\\n\\n    - Add OIDC-based SSO support for TiDB Dashboard [#3884](https://github.com/tikv/pd/pull/3884)\\n\\n+ TiFlash\\n\\n    - Support the `HAVING()` function in DAG requests\\n    - Support the `DATE()` function\\n    - Add Grafana panels for write throughput per instance\\n\\n## Improvements\\n\\n+ TiDB\\n\\n    - Trigger auto-analyze based on the histogram row count [#24237](https://github.com/pingcap/tidb/issues/24237)\\n    - Stop sending requests to a TiFlash node for a period if the node has failed and restarted before [#26757](https://github.com/pingcap/tidb/pull/26757)\\n    - Increase the `split region` upper limit to make `split table` and `presplit` more stable [#26657](https://github.com/pingcap/tidb/pull/26657)\\n    - Support retry for MPP queries [#26483](https://github.com/pingcap/tidb/pull/26483)\\n    - Check the availability of TiFlash before launching MPP queries [#1807](https://github.com/pingcap/tics/issues/1807)\\n    - Support the stable result mode to make the query result more stable [#26084](https://github.com/pingcap/tidb/pull/26084)\\n    - Support the MySQL system variable `init_connect` and its associated features [#18894](https://github.com/pingcap/tidb/issues/18894)\\n    - Thoroughly push down the `COUNT(DISTINCT)` aggregation function in the MPP mode [#25861](https://github.com/pingcap/tidb/pull/25861)\\n    - Print log warnings when the aggregation function cannot be pushed down in `EXPLAIN` statements [#25736](https://github.com/pingcap/tidb/pull/25736)\\n    - Add error labels for `TiFlashQueryTotalCounter` in Grafana dashboard [#25327](https://github.com/pingcap/tidb/pull/25327)\\n    - Support getting the MVCC data of a clustered index table through a secondary index by HTTP API [#24209](https://github.com/pingcap/tidb/issues/24209)\\n    - Optimize the memory allocation of `prepared` statement in parser [#24371](https://github.com/pingcap/tidb/pull/24371)\\n\\n+ TiKV\\n\\n    - Handle read ready and write ready separately to reduce read latency [#10475](https://github.com/tikv/tikv/issues/10475)\\n    - Reduce the size of Resolved TS messages to save network bandwidth [#2448](https://github.com/pingcap/tiflow/issues/2448)\\n    - Drop log instead of blocking threads when the slogger thread is overloaded and the queue is filled up [#10841](https://github.com/tikv/tikv/issues/10841)\\n    - Make the slow log of TiKV coprocessor only consider the time spent on processing requests [#10841](https://github.com/tikv/tikv/issues/10841)\\n    - Make prewrite as idempotent as possible to reduce the chance of undetermined errors [#10587](https://github.com/tikv/tikv/pull/10587)\\n    - Avoid the false \"GC can not work\" alert under low write flow [#10662](https://github.com/tikv/tikv/pull/10662)\\n    - Make the database to be restored always match the original cluster size during backup. [#10643](https://github.com/tikv/tikv/pull/10643)\\n    - Ensure that the panic output is flushed to the log [#9955](https://github.com/tikv/tikv/pull/9955)\\n\\n+ PD\\n\\n    - Improve the performance of synchronizing Region information between PDs [#3993](https://github.com/tikv/pd/pull/3993)\\n\\n+ Tools\\n\\n    + Dumpling\\n\\n        - Support backing up MySQL-compatible databases that do not support the `START TRANSACTION ... WITH CONSISTENT SNAPSHOT` or the `SHOW CREATE TABLE` syntax [#309](https://github.com/pingcap/dumpling/issues/309)\\n\\n    + TiCDC\\n\\n        - Optimize memory management when Unified Sorter is using memory to sort [#2553](https://github.com/pingcap/tiflow/issues/2553)\\n        - Prohibit operating TiCDC clusters across major or minor versions [#2598](https://github.com/pingcap/tiflow/pull/2598)\\n        - Reduce the goroutine usage when a table\\'s Regions are all transferred away from a TiKV node [#2284](https://github.com/pingcap/tiflow/issues/2284)\\n        - Remove `file sorter` [#2326](https://github.com/pingcap/tiflow/pull/2326)\\n        - Always pull the old values from TiKV and the output is adjusted according to `enable-old-value` [#2301](https://github.com/pingcap/tiflow/issues/2301)\\n        - Improve the error message returned when a PD endpoint misses the certificate [#1973](https://github.com/pingcap/tiflow/issues/1973)\\n        - Optimize workerpool for fewer goroutines when concurrency is high [#2211](https://github.com/pingcap/tiflow/issues/2211)\\n        - Add a global gRPC connection pool and share gRPC connections among KV clients [#2533](https://github.com/pingcap/tiflow/pull/2533)\\n\\n## Bug Fixes\\n\\n+ TiDB\\n\\n    - Fix the issue that TiDB might panic when querying a partitioned table and the partition key has the `IS NULL` condition [#23802](https://github.com/pingcap/tidb/issues/23802)\\n    - Fix the issue that the overflow check of the `FLOAT64` type is different with that of MySQL [#23897](https://github.com/pingcap/tidb/issues/23897)\\n    - Fix the wrong character set and collation for the `case when` expression [#26662](https://github.com/pingcap/tidb/issues/26662)\\n    - Fix the issue that committing pessimistic transactions might cause write conflicts [#25964](https://github.com/pingcap/tidb/issues/25964)\\n    - Fix a bug that the index keys in a pessimistic transaction might be repeatedly committed [#26359](https://github.com/pingcap/tidb/issues/26359) [#10600](https://github.com/tikv/tikv/pull/10600)\\n    - Fix the issue that TiDB might panic when resolving the async commit locks [#25778](https://github.com/pingcap/tidb/issues/25778)\\n    - Fix a bug that a column might not be found when using `INDEX MERGE` [#25045](https://github.com/pingcap/tidb/issues/25045)\\n    - Fix a bug that `ALTER USER REQUIRE SSL` clears users\\' `authentication_string` [#25225](https://github.com/pingcap/tidb/issues/25225)\\n    - Fix a bug that the value of the `tidb_gc_scan_lock_mode` global variable on a new cluster shows \"PHYSICAL\" instead of the actual default mode \"LEGACY\" [#25100](https://github.com/pingcap/tidb/issues/25100)\\n    - Fix the bug that the `TIKV_REGION_PEERS` system table does not show the correct `DOWN` status [#24879](https://github.com/pingcap/tidb/issues/24879)\\n    - Fix the issue of memory leaks that occurs when HTTP API is used [#24649](https://github.com/pingcap/tidb/pull/24649)\\n    - Fix the issue that views do not support `DEFINER` [#24414](https://github.com/pingcap/tidb/issues/24414)\\n    - Fix the issue that `tidb-server --help` exits with the code `2` [#24046](https://github.com/pingcap/tidb/issues/24046)\\n    - Fix the issue that setting the global variable `dml_batch_size` does not take effect [#24709](https://github.com/pingcap/tidb/issues/24709)\\n    - Fix the issue that using `read_from_storage` and partitioned table at the same time causes an error [#20372](https://github.com/pingcap/tidb/issues/20372)\\n    - Fix the issue that TiDB panics when executing the projection operator [#24264](https://github.com/pingcap/tidb/issues/24264)\\n    - Fix the issue that statistics might cause queries to panic [#24061](https://github.com/pingcap/tidb/pull/24061)\\n    - Fix the issue that using the `approx_percentile` function on a `BIT` column might panic [#23662](https://github.com/pingcap/tidb/issues/23662)\\n    - Fix the issue that the metrics on the **Coprocessor Cache** panel in Grafana are wrong [#26338](https://github.com/pingcap/tidb/issues/26338)\\n    - Fix the issue that concurrently truncating the same partition causes DDL statements to stuck [#26229](https://github.com/pingcap/tidb/issues/26229)\\n    - Fix the issue of wrong query results that occurs when the session variable is used as the `GROUP BY` item [#27106](https://github.com/pingcap/tidb/issues/27106)\\n    - Fix the wrong implicit conversion between `VARCHAR` and timestamp when joining tables [#25902](https://github.com/pingcap/tidb/issues/25902)\\n    - Fix the wrong results in associated subquery statements [#27233](https://github.com/pingcap/tidb/issues/27233)\\n\\n+ TiKV\\n\\n    - Fix the potential disk full issue caused by corrupted snapshot files [#10813](https://github.com/tikv/tikv/issues/10813)\\n    - Fix the TiKV panic issue that occurs when upgrading from a pre-5.0 version with Titan enabled [#10843](https://github.com/tikv/tikv/pull/10843)\\n    - Fix the issue that TiKV of a newer version cannot be rolled back to v5.0.x [#10843](https://github.com/tikv/tikv/pull/10843)\\n    - Fix the TiKV panic issue that occurs when upgrading from a pre-5.0 version to a 5.0 version or later. If a cluster was upgraded from TiKV v3.x with Titan enabled before the upgrade, this cluster might encounter the issue. [#10774](https://github.com/tikv/tikv/issues/10774)\\n    - Fix the parsing failure caused by the left pessimistic locks [#26404](https://github.com/pingcap/tidb/issues/26404)\\n    - Fix the panic that occurs when calculating duration on certain platforms [#10571](https://github.com/tikv/tikv/pull/10571)\\n    - Fix the issue that the keys of `batch_get_command` in Load Base Split are unencoded [#10542](https://github.com/tikv/tikv/issues/10542)\\n\\n+ PD\\n\\n    - Fix the issue that PD does not fix the down peers in time [#4077](https://github.com/tikv/pd/issues/4077)\\n    - Fix the issue that the replica count of the default placement rules stays constant after `replication.max-replicas` is updated [#3886](https://github.com/tikv/pd/issues/3886)\\n    - Fix a bug that PD might panic when scaling out TiKV [#3868](https://github.com/tikv/pd/issues/3868)\\n    - Fix the scheduling conflict issue that occurs when multiple schedulers are running at same time [#3807](https://github.com/tikv/pd/issues/3807)\\n    - Fix the issue that the scheduler might appear again even if it has been deleted [#2572](https://github.com/tikv/pd/issues/2572)\\n\\n+ TiFlash\\n\\n    - Fix the potential panic issue that occurs when running table scan tasks\\n    - Fix the potential memory leak issue that occurs when executing MPP tasks\\n    - Fix a bug that TiFlash raises the `duplicated region` error when handling DAQ requests\\n    - Fix the issue of unexpected results when executing the aggregation functions `COUNT` or `COUNT DISTINCT`\\n    - Fix the potential panic issue that occurs when executing MPP tasks\\n    - Fix a potential bug that TiFlash cannot restore data when deployed on multiple disks\\n    - Fix the potential panic issue that occurs when deconstructing `SharedQueryBlockInputStream`\\n    - Fix the potential panic issue that occurs when deconstructing `MPPTask`\\n    - Fix the issue of unexpected results when TiFlash fails to establish MPP connections\\n    - Fix the potential panic issue that occurs when resolving locks\\n    - Fix the issue that the store size in metrics is inaccurate under heavy writing\\n    - Fix a bug of incorrect results that occurs when queries contain filters like `CONSTANT`, `<`, `<=`, `>`, `>=`, or `COLUMN`\\n    - Fix the potential issue that TiFlash cannot garbage-collect the delta data after running for a long time\\n    - Fix a potential bug that metrics display wrong values\\n    - Fix the potential issue of data inconsistency that occurs when TiFlash is deployed on multiple disks\\n\\n+ Tools\\n\\n    + Dumpling\\n\\n        - Fix the issue that the execution of `show table status` is stuck in MySQL 8.0.3 or a later version [#322](https://github.com/pingcap/dumpling/issues/322)\\n\\n    + TiCDC\\n\\n        - Fix the issue of process panic that occurs when encoding the data types such as `mysql.TypeString, mysql.TypeVarString, mysql.TypeVarchar` into JSON [#2758](https://github.com/pingcap/tiflow/issues/2758)\\n        - Fix a data inconsistency issue that occurs because multiple processors might write data to the same table when this table is being re-scheduled [#2417](https://github.com/pingcap/tiflow/pull/2417)\\n        - Decrease the gRPC window size to avoid the OOM that occurs when TiCDC captures too many Regions [#2724](https://github.com/pingcap/tiflow/pull/2724)\\n        - Fix the error that the gRPC connection is frequently broken when the memory pressure is high [#2202](https://github.com/pingcap/tiflow/issues/2202)\\n        - Fix a bug that causes TiCDC to panic on the unsigned `TINYINT` type [#2648](https://github.com/pingcap/tiflow/issues/2648)\\n        - Fix the issue that TiCDC Open Protocol outputs an empty value when inserting a transaction and deleting data of the same row in the upstream [#2612](https://github.com/pingcap/tiflow/issues/2612)\\n        - Fix a bug that DDL handling fails when a changefeed starts at the finish TS of a schema change [#2603](https://github.com/pingcap/tiflow/issues/2603)\\n        - Fix the issue that irresponsive downstreams interrupt the replication task in old owner until the task times out [#2295](https://github.com/pingcap/tiflow/issues/2295)\\n        - Fix a bug in metadata management [#2558](https://github.com/pingcap/tiflow/pull/2558)\\n        - Fix the issue of data inconsistency that occurs after the TiCDC owner switch [#2230](https://github.com/pingcap/tiflow/issues/2230)\\n        - Fix the issue that outdated capture might appear in the output of the `capture list` command [#2388](https://github.com/pingcap/tiflow/issues/2388)\\n        - Fix the `ErrSchemaStorageTableMiss` error that occurs when the DDL Job duplication is encountered in the integrated test [#2422](https://github.com/pingcap/tiflow/issues/2422)\\n        - Fix the bug that a changefeed cannot be removed if the `ErrGCTTLExceeded` error occurs [#2391](https://github.com/pingcap/tiflow/issues/2391)\\n        - Fix a bug that replicating large tables to cdclog fails [#1259](https://github.com/pingcap/tiflow/issues/1259) [#2424](https://github.com/pingcap/tiflow/issues/2424)\\n        - Fix the CLI backward compatibility issue [#2373](https://github.com/pingcap/tiflow/issues/2373)\\n        - Fix the issue of insecure concurrent access to the map in `SinkManager` [#2299](https://github.com/pingcap/tiflow/pull/2299)\\n        - Fix the issue of potential DDL loss when the owner crashes when executing DDL statements [#1260](https://github.com/pingcap/tiflow/issues/1260)\\n        - Fix the issue that the lock is resolved immediately after a Region is initialized [#2188](https://github.com/pingcap/tiflow/issues/2188)\\n        - Fix the issue of extra partition dispatching that occurs when adding a new partitioned table [#2263](https://github.com/pingcap/tiflow/pull/2263)\\n        - Fix the issue that TiCDC keeps warning on removed changefeeds [#2156](https://github.com/pingcap/tiflow/issues/2156)\\n', doc_link='https://docs.pingcap.com/tidb/v8.1/release-5.0.4'),\n",
       " 15809: DocumentData(id=15809, chunks={}, content='---\\ntitle: TiDB 5.3 Release Notes\\nsummary: TiDB 5.3.0 introduces temporary tables, table attributes, and user privileges on TiDB Dashboard for improved performance and security. It also enhances TiDB Data Migration, supports parallel import using multiple TiDB Lightning instances, and continuous profiling for better observability. Compatibility changes and configuration file parameters have been modified. The release also includes new SQL features, security enhancements, stability improvements, and diagnostic efficiency. Additionally, bug fixes and improvements have been made to TiDB, TiKV, PD, TiFlash, and TiCDC. The cyclic replication feature between TiDB clusters has been removed. Telemetry now includes information about the usage of the TEMPORARY TABLE feature.\\n---\\n\\n# TiDB 5.3 Release Notes\\n\\nRelease date: November 30, 2021\\n\\nTiDB version: 5.3.0\\n\\nIn v5.3, the key new features or improvements are as follows:\\n\\n+ Introduce temporary tables to simplify your application logic and improve performance\\n+ Support setting attributes for tables and partitions\\n+ Support creating users with the least privileges on TiDB Dashboard to enhance system security\\n+ Optimize the timestamp processing flow in TiDB to improve the overall performance\\n+ Enhance the performance of TiDB Data Migration (DM) so that data is migrated from MySQL to TiDB with lower latency\\n+ Support parallel import using multiple TiDB Lightning instances to improve the efficiency of full data migration\\n+ Support saving and restoring the on-site information of a cluster with a single SQL statement, which helps improve the efficiency of troubleshooting issues relating to execution plans\\n+ Support the continuous profiling experimental feature to improve the observability of database performance\\n+ Continue optimizing the storage and computing engines to improve the system performance and stability\\n+ Reduce the write latency of TiKV by separating I/O operations from Raftstore thread pool (disabled by default)\\n\\n## Compatibility changes\\n\\n> **Note:**\\n>\\n> When upgrading from an earlier TiDB version to v5.3.0, if you want to know the compatibility change notes of all intermediate versions, you can check the [Release Notes](/releases/release-notes.md) of the corresponding version.\\n\\n### System variables\\n\\n|  Variable name    |  Change type    |  Description    |\\n| :---------- | :----------- | :----------- |\\n| [`tidb_enable_noop_functions`](/system-variables.md#tidb_enable_noop_functions-new-in-v40) | Modified |  Temporary tables are now supported by TiDB so `CREATE TEMPORARY TABLE` and `DROP TEMPORARY TABLE` no longer require enabling `tidb_enable_noop_functions`. |\\n| [`tidb_enable_pseudo_for_outdated_stats`](/system-variables.md#tidb_enable_pseudo_for_outdated_stats-new-in-v530) | Newly added | Controls the behavior of the optimizer when the statistics on a table expire. The default value is `ON`. When the number of modified rows in the table is greater than 80% of the total rows (This ratio can be adjusted by the configuration [`pseudo-estimate-ratio`](/tidb-configuration-file.md#pseudo-estimate-ratio)), the optimizer considers that the statistics other than the total number of rows are no longer reliable and use pseudo statistics instead. When you set the value as `OFF`, even if the statistics expire, the optimizer still uses them. |\\n|[`tidb_enable_tso_follower_proxy`](/system-variables.md#tidb_enable_tso_follower_proxy-new-in-v530) | Newly added  | Determines whether to enable or disable the TSO Follower Proxy feature. The default value is `OFF`, which means the TSO Follower Proxy feature is disabled. At this time, TiDB only gets TSO from PD leader. When this feature is enabled, TiDB evenly sends the requests to all PD nodes when acquiring TSO. The PD follower then forwards the TSO requests to reduce the CPU pressure of PD leader. |\\n|[`tidb_tso_client_batch_max_wait_time`](/system-variables.md#tidb_tso_client_batch_max_wait_time-new-in-v530) | Newly added |  Sets the maximum waiting time for a batch saving operation when TiDB requests TSO from PD. The default value is `0`, which means no additional waiting. |\\n| [`tidb_tmp_table_max_size`](/system-variables.md#tidb_tmp_table_max_size-new-in-v530) | Newly added  | Limits the maximum size of a single [temporary table](/temporary-tables.md). If the temporary table exceeds this size, an error will occur. |\\n\\n### Configuration file parameters\\n\\n|  Configuration file    |  Configuration item  | Change type |  Description  |\\n| :---------- | :----------- | :----------- | :----------- |\\n| TiDB | [`prepared-plan-cache.capacity`](/tidb-configuration-file.md#capacity)  | Modified | Controls the number of cached statements. The default value is changed from `100` to `1000`.|\\n| TiKV | [`storage.reserve-space`](/tikv-configuration-file.md#reserve-space) | Modified | Controls space reserved for disk protection when TiKV is started. Starting from v5.3.0, 80% of the reserved space is used as the extra disk space required for operations and maintenance when the disk space is insufficient, and the other 20% is used to store temporary files. |\\n| TiKV | `memory-usage-limit` | Modified  | This configuration item is new in TiDB v5.3.0 and its value is calculated based on storage.block-cache.capacity. |\\n| TiKV | [`raftstore.store-io-pool-size`](/tikv-configuration-file.md#store-io-pool-size-new-in-v530) | Newly added | The allowable number of threads that process Raft I/O tasks, which is the size of the StoreWriter thread pool. When you modify the size of this thread pool, refer to [Performance tuning for TiKV thread pools](/tune-tikv-thread-performance.md#performance-tuning-for-tikv-thread-pools). |\\n| TiKV | [`raftstore.raft-write-size-limit`](/tikv-configuration-file.md#raft-write-size-limit-new-in-v530) | Newly added | Determines the threshold at which Raft data is written into the disk. If the data size is larger than the value of this configuration item, the data is written to the disk. When the value of `raftstore.store-io-pool-size` is `0`, this configuration item does not take effect. |\\n| TiKV | `raftstore.raft-msg-flush-interval` | Newly added | Determines the interval at which Raft messages are sent in batches. The Raft messages in batches are sent at every interval specified by this configuration item. When the value of `raftstore.store-io-pool-size` is `0`, this configuration item does not take effect. |\\n| TiKV | `raftstore.raft-reject-transfer-leader-duration`  | Deleted | Determines the smallest duration that a Leader is transferred to a newly added node. |\\n| PD | [`log.file.max-days`](/pd-configuration-file.md#max-days) | Modified | Controls the maximum number of days that logs are retained for. The default value is changed from `1` to `0`. |\\n| PD | [`log.file.max-backups`](/pd-configuration-file.md#max-backups) | Modified | Controls the maximum number of logs that are retained for. The default value is changed from `7` to `0`.  |\\n| PD | [`patrol-region-interval`](/pd-configuration-file.md#patrol-region-interval) | Modified | Controls the running frequency at which replicaChecker checks the health state of a Region. The smaller this value is, the faster replicaChecker runs. Normally, you do not need to adjust this parameter. The default value is changed from `100ms` to `10ms`. |\\n| PD | [`max-snapshot-count`](/pd-configuration-file.md#max-snapshot-count) | Modified | Controls the maximum number of snapshots that a single store receives or sends at the same time. PD schedulers depend on this configuration to prevent the resources used for normal traffic from being preempted. The default value is changed from `3` to `64`. |\\n| PD | [`max-pending-peer-count`](/pd-configuration-file.md#max-pending-peer-count) | Modified | Controls the maximum number of pending peers in a single store. PD schedulers depend on this configuration to prevent too many Regions with outdated logs from being generated on some nodes. The default value is changed from `16` to `64`. |\\n| TiD Lightning | `meta-schema-name` | Newly added | The schema name where the meta information for each TiDB Lightning instance is stored in the target cluster. The default value is \"lightning_metadata\". |\\n\\n### Others\\n\\n- Temporary tables:\\n\\n    - If you have created local temporary tables in a TiDB cluster earlier than v5.3.0, these tables are actually ordinary tables, and handled as ordinary tables after the cluster is upgraded to v5.3.0 or a later version. If you have created global temporary tables in a TiDB cluster of v5.3.0 or a later version, when the cluster is downgraded to a version earlier than v5.3.0, these tables are handled as ordinary tables and cause a data error.\\n    - Since v5.3.0, TiCDC and BR support [global temporary tables](/temporary-tables.md#global-temporary-tables). If you use TiCDC and BR of a version earlier than v5.3.0 to replicate global temporary tables to the downstream, a table definition error occurs.\\n    - The following clusters are expected to be v5.3.0 or later; otherwise, data error is reported when you create a global temporary table:\\n\\n        - the cluster to be imported using TiDB migration tools\\n        - the cluster restored using TiDB migration tools\\n        - the downstream cluster in a replication task using TiDB migration tools\\n    - For the compatibility information of temporary tables, refer to [Compatibility with MySQL temporary tables](/temporary-tables.md#compatibility-with-mysql-temporary-tables) and [Compatibility restrictions with other TiDB features](/temporary-tables.md#compatibility-restrictions-with-other-tidb-features).\\n\\n- For releases earlier than v5.3.0, TiDB reports an error when a system variable is set to an illegal value. For v5.3.0 and later releases, TiDB returns success with a warning such as \"|Warning | 1292 | Truncated incorrect xxx: \\'xx\\'\" when a system variable is set to an illegal value.\\n- Fix the issue that the `SHOW VIEW` permission is not required to execute `SHOW CREATE VIEW`. Now you are expected to have the `SHOW VIEW` permission to execute the `SHOW CREATE VIEW` statement.\\n- The system variable `sql_auto_is_null` is added to the noop functions. When `tidb_enable_noop_functions = 0/OFF`, modifying this variable value causes an error.\\n- The `GRANT ALL ON performance_schema.*` syntax is no longer permitted. If you execute this statement in TiDB, an error occurs.\\n- Fix the issue that auto-analyze is unexpectedly triggered outside the specified time period when new indexes are added before v5.3.0. In v5.3.0, after you set the time period through the `tidb_auto_analyze_start_time` and `tidb_auto_analyze_end_time` variables, auto-analyze is triggered only during this time period.\\n- The default storage directory for plugins is changed from `\"\"` to `/data/deploy/plugin`.\\n- The DM code is migrated to [the folder \"dm\" in TiCDC code repository](https://github.com/pingcap/tiflow/tree/release-5.3/dm). Now DM follows TiDB in version numbers. Next to v2.0.x, the new DM version is v5.3.0, and you can upgrade from v2.0.x to v5.3.0 without any risk.\\n- The default deployed version of Prometheus is upgraded from v2.8.1 to [v2.27.1](https://github.com/prometheus/prometheus/releases/tag/v2.27.1), which is released in May 2021. This version provides more features and fixes a security issue. Compared with Prometheus v2.8.1, alert time representation in v2.27.1 is changed from Unix timestamp to UTC. For details, refer to [Prometheus commit](https://github.com/prometheus/prometheus/commit/7646cbca328278585be15fa615e22f2a50b47d06) for more details.\\n\\n## New features\\n\\n### SQL\\n\\n- **Use SQL interface to set placement rules for data (experimental feature)**\\n\\n    Support the `[CREATE | ALTER] PLACEMENT POLICY` syntax that provides a SQL interface to set placement rules for data. Using this feature, you can specify tables and partitions to be scheduled to specific regions, data centers, racks, hosts, or replica count rules. This meets your application demands for lower cost and higher flexibility. The typical user scenarios are as follows:\\n\\n    - Merge multiple databases of different applications to reduce the cost on database maintenance, and achieve application resource isolation through the rule configuration\\n    - Increase replica count for important data to improve the application availability and data reliability\\n    - Store new data into SSDs and store old data into HHDs to lower the cost on data archiving and storage\\n    - Schedule the leaders of hotspot data to high-performance TiKV instances\\n    - Separate cold data to lower-cost storage mediums to improve cost efficiency\\n\\n    [User document](/placement-rules-in-sql.md), [#18030](https://github.com/pingcap/tidb/issues/18030)\\n\\n- **Temporary tables**\\n\\n    Support the `CREATE [GLOBAL] TEMPORARY TABLE` statement to create temporary tables. Using this feature, you can easily manage the temporary data generated in the calculation process of an application. Temporary data is stored in memory and you can use the `tidb_tmp_table_max_size` variable to limit the size of a temporary table. TiDB supports the following types of temporary tables:\\n\\n    - Global temporary tables\\n        - Visible to all sessions in the cluster, and table schemas are persistent.\\n        - Provides transaction-level data isolation. The temporary data is effective only in the transaction. After the transaction finishes, the data is automatically dropped.\\n    - Local temporary tables\\n        - Visible only to the current session, and tables schemas are not persistent.\\n        - Supports duplicated table names. You do not need to design complicated naming rules for your application.\\n        - Provides session-level data isolation, which enables you to design a simpler application logic. After the transaction finishes, the temporary tables are dropped.\\n\\n        [User document](/temporary-tables.md), [#24169](https://github.com/pingcap/tidb/issues/24169)\\n\\n- **Support the `FOR UPDATE OF TABLES` syntax**\\n\\n    For a SQL statement that joins multiple tables, TiDB supports acquiring pessimistic locks on the rows correlated to the tables that are included in `OF TABLES`.\\n\\n    [User document](/sql-statements/sql-statement-select.md), [#28689](https://github.com/pingcap/tidb/issues/28689)\\n\\n- **Table attributes**\\n\\n    Support the `ALTER TABLE [PARTITION] ATTRIBUTES` statement that allows you to set attributes for a table or partition. Currently, TiDB only supports setting the `merge_option` attribute. By adding this attribute, you can explicitly control the Region merge behavior.\\n\\n    User scenarios: When you perform the `SPLIT TABLE` operation, if no data is inserted after a certain period of time (controlled by the PD parameter [`split-merge-interval`](/pd-configuration-file.md#split-merge-interval)), the empty Regions are automatically merged by default. In this case, you can set the table attribute to `merge_option=deny` to avoid the automatic merging of Regions.\\n\\n    [User document](/table-attributes.md), [#3839](https://github.com/tikv/pd/issues/3839)\\n\\n### Security\\n\\n- **Support creating users with the least privileges on TiDB Dashboard**\\n\\n    The account system of TiDB Dashboard is consistent with that of TiDB SQL. Users accessing TiDB Dashboard are authenticated and authorized based on TiDB SQL users\\' privileges. Therefore, TiDB Dashboard requires limited privileges, or merely the read-only privilege. You can configure users to access TiDB Dashboard based on the principle of least privilege, thus avoiding access of high-privileged users.\\n\\n    It is recommended that you create a least-privileged SQL user to access and sign in with TiDB Dashboard. This avoids access of high-privileged users and improves security.\\n\\n    [User document](/dashboard/dashboard-user.md)\\n\\n### Performance\\n\\n- **Optimize the timestamp processing flow of PD**\\n\\n    TiDB optimizes its timestamp processing flow and reduces the timestamp processing load of PD by enabling PD Follower Proxy and modifying the batch waiting time required when the PD client requests TSO in batches. This helps improve the overall scalability of the system.\\n\\n    - Support enabling or disabling PD Follower Proxy through the system variable [`tidb_enable_tso_follower_proxy`](/system-variables.md#tidb_enable_tso_follower_proxy-new-in-v530). Suppose that the TSO requests load of PD is too high. In this case, enabling PD follower proxy can batch forward the TSO requests collected during the request cycle on followers to the leader nodes. This solution can effectively reduce the number of direct interactions between clients and leaders, reduce the pressure of the load on leaders, and improve the overall performance of TiDB.\\n\\n    > **Note:**\\n    >\\n    > When the number of clients is small and the PD leader CPU load is not full, it is NOT recommended to enable PD Follower Proxy.\\n\\n    - Support using the system variable [`tidb_tso_client_batch_max_wait_time`](/system-variables.md#tidb_tso_client_batch_max_wait_time-new-in-v530) to set the maximum waiting time needed for the PD client to batch request TSO. The unit of this time is milliseconds. In case that PD has a high TSO requests load, you can reduce the load and improve the throughput by increasing the waiting time to get a larger batch size.\\n\\n    > **Note:**\\n    >\\n    > When the TSO request load is not high, it is NOT recommended to modify this variable value.\\n\\n    [User document](/system-variables.md#tidb_tso_client_batch_max_wait_time-new-in-v530), [#3149](https://github.com/tikv/pd/issues/3149)\\n\\n### Stability\\n\\n- **Support Online Unsafe Recovery after some stores are permanently damaged (experimental feature)**\\n\\n    Support the `pd-ctl unsafe remove-failed-stores` command that performs online data unsafe recovery. Suppose that the majority of data replicas encounter issues like permanent damage (such as disk damage), and these issues cause the data ranges in an application to be unreadable or unwritable. In this case, you can use the Online Unsafe Recovery feature implemented in PD to recover the data, so that the data is readable or writable again.\\n\\n    It is recommended to perform the feature-related operations with the support of the TiDB team.\\n\\n    [User document](/online-unsafe-recovery.md), [#10483](https://github.com/tikv/tikv/issues/10483)\\n\\n### Data migration\\n\\n- **DM replication performance enhanced**\\n\\n    Supports the following features to ensure lower-latency data replication from MySQL to TiDB:\\n\\n    - Compact multiple updates on a single row into one statement\\n    - Merge batch updates of multiple rows into one statement\\n\\n- **Add DM OpenAPI to better maintain DM clusters (experimental feature)**\\n\\n    DM provides the OpenAPI feature for querying and operating the DM cluster. It is similar to the feature of [dmctl tools](/dm/dmctl-introduction.md).\\n\\n    Currently, DM OpenAPI is an experimental feature and disabled by default. It is not recommended to use it in a production environment.\\n\\n    [User document](/dm/dm-open-api.md)\\n\\n- **TiDB Lightning Parallel Import**\\n\\n    TiDB Lightning provides parallel import capability to extend the original feature. It allows you to deploy multiple Lightning instances at the same time to import single tables or multiple tables to downstream TiDB in parallel. Without changing the way customers use it, it greatly improves the data migration ability, allowing you to migrate data in a more real-time way to further process, integrate and analyze them. It improves the efficiency of enterprise data management.\\n\\n    In our test, using 10 TiDB Lightning instances, a total of 20 TiB MySQL data can be imported to TiDB within 8 hours. The performance of multiple table import is also improved. A single TiDB Lightning instance can support importing at 250 GiB/h, and the overall migration is 8 times faster than the original performance.\\n\\n    [User document](/tidb-lightning/tidb-lightning-distributed-import.md)\\n\\n- **TiDB Lightning Prechecks**\\n\\n    TiDB Lightning provides the ability to check the configuration before running a migration task. It is enabled by default. This feature automatically performs some routine checks for disk space and execution configuration. The main purpose is to ensure that the whole subsequent import process goes smoothly.\\n\\n    [User document](/tidb-lightning/tidb-lightning-prechecks.md)\\n\\n- **TiDB Lightning supports importing files of GBK character set**\\n\\n    You can specify the character set of the source data file. TiDB Lightning will convert the source file from the specified character set to UTF-8 encoding during the import process.\\n\\n    [User document](/tidb-lightning/tidb-lightning-configuration.md)\\n\\n- **Sync-diff-inspector improvement**\\n\\n    - Improve the comparison speed from 375 MB/s to 700 MB/s\\n    - Reduce the memory consumption of TiDB nodes by nearly half during comparison\\n    - Optimize the user interface and display the progress bar during comparison\\n\\n    [User document](/sync-diff-inspector/sync-diff-inspector-overview.md)\\n\\n### Diagnostic efficiency\\n\\n- **Save and restore the on-site information of a cluster**\\n\\n    When you locate and troubleshoot the issues of a TiDB cluster, you often need to provide information on the system and the query plan. To help you get the information and troubleshoot cluster issues in a more convenient and efficient way, the `PLAN REPLAYER` command is introduced in TiDB v5.3.0. This command enables you to easily save and restore the on-site information of a cluster, improves the efficiency of troubleshooting, and helps you more easily archive the issues for management.\\n\\n    The features of `PLAN REPLAYER` are as follows:\\n\\n    - Exports the information of a TiDB cluster at an on-site troubleshooting to a ZIP-formatted file for storage.\\n    - Imports into a cluster the ZIP-formatted file exported from another TiDB cluster. This file contains the information of the latter TiDB cluster at an on-site troubleshooting.\\n\\n    [User document](/sql-plan-replayer.md), [#26325](https://github.com/pingcap/tidb/issues/26325)\\n\\n### TiDB data share subscription\\n\\n- **TiCDC Eventually Consistent Replication**\\n\\n    TiCDC provides the eventually consistent replication capability in disaster scenarios. When a disaster occurs in the primary TiDB cluster and the service cannot be resumed in a short period of time, TiCDC needs to provide the ability to ensure the consistency of data in the secondary cluster. Meanwhile, TiCDC needs to allow the business to quickly switch the traffic to the secondary cluster to avoid the database being unavailable for a long time and affecting the business.\\n\\n    This feature supports TiCDC to replicate incremental data from a TiDB cluster to the secondary relational database TiDB/Aurora/MySQL/MariaDB. In case the primary cluster crashes, TiCDC can recover the secondary cluster to a certain snapshot in the primary cluster within 5 minutes, given the condition that before disaster the replication status of TiCDC is normal and replication lag is small. It allows data loss of less than 30 minutes, that is, RTO <= 5min, and RPO <= 30min.\\n\\n    [User document](/ticdc/ticdc-sink-to-mysql.md#eventually-consistent-replication-in-disaster-scenarios)\\n\\n- **TiCDC supports the HTTP protocol OpenAPI for managing TiCDC tasks**\\n\\n    Since TiDB v5.3.0, TiCDC OpenAPI becomes a General Availability (GA) feature. You can query and operate TiCDC clusters using OpenAPI in the production environment.\\n\\n### Deployment and maintenance\\n\\n- **Continuous Profiling (experimental feature)**\\n\\n    TiDB Dashboard supports the Continuous Profiling feature, which stores instance performance analysis results automatically in real time when TiDB clusters are running. You can check the performance analysis result in a flame graph, which is more observable and shortens troubleshooting time.\\n\\n    This feature is disabled by default and needs to be enabled on the **Continuous Profile** page of TiDB Dashboard.\\n\\n    This feature is only available for clusters upgraded or installed using TiUP v1.7.0 or above.\\n\\n    [User document](/dashboard/continuous-profiling.md)\\n\\n## Telemetry\\n\\nTiDB adds the information to the telemetry report about whether or not the TEMPORARY TABLE feature is used. This does not include table names or table data.\\n\\nTo learn more about telemetry and how to disable this behavior, refer to [Telemetry](/telemetry.md).\\n\\n## Removed feature\\n\\nStarting from TiCDC v5.3.0, the cyclic replication feature between TiDB clusters (an experimental feature in v5.0.0) has been removed. If you have already used this feature to replicate data before upgrading TiCDC, the related data is not affected after the upgrade.\\n\\n## Improvements\\n\\n+ TiDB\\n\\n    - Show the affected SQL statements in the debug log when the coprocessor encounters a lock, which is helpful in diagnosing problems [#27718](https://github.com/pingcap/tidb/issues/27718)\\n    - Support showing the size of the backup and restore data when backing up and restoring data in the SQL logical layer [#27247](https://github.com/pingcap/tidb/issues/27247)\\n    - Improve the default collection logic of ANALYZE when `tidb_analyze_version` is `2`, which accelerates collection and reduces resource overhead\\n    - Introduce the `ANALYZE TABLE table_name COLUMNS col_1, col_2, ... , col_n` syntax. The syntax allows collecting statistics only on a portion of the columns in wide tables, which improves the speed of statistics collection\\n\\n+ TiKV\\n\\n    - Enhance disk space protection to improve storage stability\\n\\n        To solve the issue that TiKV might panic in case of a disk fully-written error, TiKV introduces a two-level threshold defense mechanism to protect the disk remaining space from being exhausted by excess traffic. Additionally, the mechanism provides the ability to reclaim space when the threshold is triggered. When the remaining space threshold is triggered, some write operations will fail and TiKV will return a disk full error as well as a list of disk full nodes. In this case, to recover the space and restore the service, you can execute `Drop/Truncate Table` or scale out the nodes.\\n\\n    - Simplify the algorithm of L0 flow control [#10879](https://github.com/tikv/tikv/issues/10879)\\n    - Improve the error log report in the raft client module [#10944](https://github.com/tikv/tikv/pull/10944)\\n    - Improve logging threads to avoid them becoming a performance bottleneck [#10841](https://github.com/tikv/tikv/issues/10841)\\n    - Add more statistics types of write queries [#10507](https://github.com/tikv/tikv/issues/10507)\\n    - Reduce the write latency by separating I/O operations from Raftstore thread pool (disabled by default). For more information about tuning, see [Tune TiKV Thread Pool Performance](/tune-tikv-thread-performance.md) [#10540](https://github.com/tikv/tikv/issues/10540)\\n\\n+ PD\\n\\n    - Add more types of write queries to QPS dimensions in the hotspot scheduler [#3869](https://github.com/tikv/pd/issues/3869)\\n    - Support dynamically adjusting the retry limit of the Balance Region scheduler to improve the performance of the scheduler [#3744](https://github.com/tikv/pd/issues/3744)\\n    - Update TiDB Dashboard to v2021.10.08.1 [#4070](https://github.com/tikv/pd/pull/4070)\\n    - Support that the evict leader scheduler can schedule Regions with unhealthy peers [#4093](https://github.com/tikv/pd/issues/4093)\\n    - Speed up the exit process of schedulers [#4146](https://github.com/tikv/pd/issues/4146)\\n\\n+ TiFlash\\n\\n    - Improve the execution efficiency of the TableScan operator greatly\\n    - Improve the execution efficiency of the Exchange operator\\n    - Reduce write amplification and memory usage during GC of the storage engine (experimental feature)\\n    - Improve the stability and availability of TiFlash when TiFlash restarts, which reduces possible query failures following the restart\\n    - Support pushing down multiple new String and Time functions to the MPP engine\\n\\n        - String functions: LIKE pattern, FORMAT(), LOWER(), LTRIM(), RTRIM(), SUBSTRING_INDEX(), TRIM(), UCASE(), UPPER()\\n        - Mathematical functions: ROUND (decimal, int)\\n        - Date and time functions: HOUR(), MICROSECOND(), MINUTE(), SECOND(), SYSDATE()\\n        - Type conversion function: CAST(time, real)\\n        - Aggregation functions: GROUP_CONCAT(), SUM(enum)\\n\\n    - Support 512-bit SIMD\\n    - Enhance the cleanup algorithm for outdated data to reduce disk usage and read files more efficiently\\n    - Fix the issue that dashboard does not display memory or CPU information in some non-Linux systems\\n    - Unify the naming style of TiFlash log files (keep the naming style consistent with that of TiKV) and support dynamic modification of logger.count and logger.size\\n    - Improve the data validation capability of column-based files (checksums, experimental feature)\\n\\n+ Tools\\n\\n    + TiCDC\\n\\n        - Reduce the default value of the Kafka sink configuration item `MaxMessageBytes` from 64 MB to 1 MB to fix the issue that large messages are rejected by the Kafka Broker [#3104](https://github.com/pingcap/tiflow/pull/3104)\\n        - Reduce memory usage in the replication pipeline [#2553](https://github.com/pingcap/tiflow/issues/2553) [#3037](https://github.com/pingcap/tiflow/pull/3037) [#2726](https://github.com/pingcap/tiflow/pull/2726)\\n        - Optimize monitoring items and alert rules to improve observability of synchronous links, memory GC, and stock data scanning processes [#2735](https://github.com/pingcap/tiflow/pull/2735) [#1606](https://github.com/pingcap/tiflow/issues/1606) [#3000](https://github.com/pingcap/tiflow/pull/3000) [#2985](https://github.com/pingcap/tiflow/issues/2985) [#2156](https://github.com/pingcap/tiflow/issues/2156)\\n        - When the sync task status is normal, no more historical error messages are displayed to avoid misleading users [#2242](https://github.com/pingcap/tiflow/issues/2242)\\n\\n## Bug Fixes\\n\\n+ TiDB\\n\\n    - Fix an error that occurs during execution caused by the wrong execution plan. The wrong execution plan is caused by the shallow copy of schema columns when pushing down the aggregation operators on partitioned tables [#27797](https://github.com/pingcap/tidb/issues/27797) [#26554](https://github.com/pingcap/tidb/issues/26554)\\n    - Fix the issue that `plan cache` cannot detect changes of unsigned flags [#28254](https://github.com/pingcap/tidb/issues/28254)\\n    - Fix the wrong partition pruning when the partition function is out of range [#28233](https://github.com/pingcap/tidb/issues/28233)\\n    - Fix the issue that planner might cache invalid plans for `join` in some cases [#28087](https://github.com/pingcap/tidb/issues/28087)\\n    - Fix wrong `IndexLookUpJoin` when hash column type is `enum` [#27893](https://github.com/pingcap/tidb/issues/27893)\\n    - Fix a batch client bug that recycling idle connection might block sending requests in some rare cases [#27688](https://github.com/pingcap/tidb/pull/27688)\\n    - Fix the TiDB Lightning panic issue when it fails to perform checksum on a target cluster [#27686](https://github.com/pingcap/tidb/pull/27686)\\n    - Fix wrong results of the `date_add` and `date_sub` functions in some cases [#27232](https://github.com/pingcap/tidb/issues/27232)\\n    - Fix wrong results of the `hour` function in vectorized expression [#28643](https://github.com/pingcap/tidb/issues/28643)\\n    - Fix the authenticating issue when connecting to MySQL 5.1 or an older client version  [#27855](https://github.com/pingcap/tidb/issues/27855)\\n    - Fix the issue that auto analyze might be triggered out of the specified time when a new index is added [#28698](https://github.com/pingcap/tidb/issues/28698)\\n    - Fix a bug that setting any session variable invalidates `tidb_snapshot` [#28683](https://github.com/pingcap/tidb/pull/28683)\\n    - Fix a bug that BR is not working for clusters with many missing-peer Regions [#27534](https://github.com/pingcap/tidb/issues/27534)\\n    - Fix the unexpected error like `tidb_cast to Int32 is not supported` when the unsupported `cast` is pushed down to TiFlash [#23907](https://github.com/pingcap/tidb/issues/23907)\\n    - Fix the issue that `DECIMAL overflow` is missing in the `%s value is out of range in \\'%s\\'`error message  [#27964](https://github.com/pingcap/tidb/issues/27964)\\n    - Fix a bug that the availability detection of MPP node does not work in some corner cases [#3118](https://github.com/pingcap/tics/issues/3118)\\n    - Fix the `DATA RACE` issue when assigning `MPP task ID` [#27952](https://github.com/pingcap/tidb/issues/27952)\\n    - Fix the `INDEX OUT OF RANGE` error for a MPP query after deleting an empty `dual table` [#28250](https://github.com/pingcap/tidb/issues/28250)\\n    - Fix the issue of false positive error log `invalid cop task execution summaries length` for MPP queries [#1791](https://github.com/pingcap/tics/issues/1791)\\n    - Fix the issue of error log `cannot found column in Schema column` for MPP queries [#28149](https://github.com/pingcap/tidb/pull/28149)\\n    - Fix the issue that TiDB might panic when TiFlash is shutting down [#28096](https://github.com/pingcap/tidb/issues/28096)\\n    - Remove the support for insecure 3DES (Triple Data Encryption Algorithm) based TLS cipher suites [#27859](https://github.com/pingcap/tidb/pull/27859)\\n    - Fix the issue that Lightning connects to offline TiKV nodes during pre-check and causes import failures [#27826](https://github.com/pingcap/tidb/pull/27826)\\n    - Fix the issue that pre-check cost too much time when importing many files to tables [#27605](https://github.com/pingcap/tidb/issues/27605)\\n    - Fix the issue that rewriting expressions makes `between` infer wrong collation [#27146](https://github.com/pingcap/tidb/issues/27146)\\n    - Fix the issue that `group_concat` function did not consider the collation [#27429](https://github.com/pingcap/tidb/issues/27429)\\n    - Fix the result wrong that occurs when the argument of the `extract` function is a negative duration [#27236](https://github.com/pingcap/tidb/issues/27236)\\n    - Fix the issue that creating partition fails if `NO_UNSIGNED_SUBTRACTION` is set [#26765](https://github.com/pingcap/tidb/issues/26765)\\n    - Avoid expressions with side effects in column pruning and aggregation pushdown [#27106](https://github.com/pingcap/tidb/issues/27106)\\n    - Remove useless gRPC logs [#24190](https://github.com/pingcap/tidb/issues/24190)\\n    - Limit the valid decimal length to fix precision-related issues [#3091](https://github.com/pingcap/tics/issues/3091)\\n    - Fix the issue of a wrong way to check for overflow in `plus` expression [#26977](https://github.com/pingcap/tidb/issues/26977)\\n    - Fix the issue of `data too long` error when dumping statistics from the table with `new collation` data [#27024](https://github.com/pingcap/tidb/issues/27024)\\n    - Fix the issue that the retried transactions\\' statements are not included in `TIDB_TRX` [#28670](https://github.com/pingcap/tidb/pull/28670)\\n    - Fix the wrong default value of the `plugin_dir` configuration [#28084](https://github.com/pingcap/tidb/issues/28084)\\n    - Fix the issue that the `CONVERT_TZ` function returns `NULL` when it is given a named timezone and a UTC offset [#8311](https://github.com/pingcap/tidb/issues/8311)\\n    - Fix the issue that `CREATE SCHEMA` does not use the character set specified by `character_set_server` and `collation_server` for new schemas if none are provided as part of the statement [#27214](https://github.com/pingcap/tidb/issues/27214)\\n\\n+ TiKV\\n\\n    - Fix the issue of unavailable TiKV caused by Raftstore deadlock when migrating Regions. The workaround is to disable the scheduling and restart the unavailable TiKV [#10909](https://github.com/tikv/tikv/issues/10909)\\n    - Fix the issue that CDC adds scan retries frequently due to the Congest error [#11082](https://github.com/tikv/tikv/issues/11082)\\n    - Fix the issue that the Raft connection is broken when the channel is full [#11047](https://github.com/tikv/tikv/issues/11047)\\n    - Fix the issue that batch messages are too large in Raft client implementation [#9714](https://github.com/tikv/tikv/issues/9714)\\n    - Fix the issue that some coroutines leak in `resolved_ts` [#10965](https://github.com/tikv/tikv/issues/10965)\\n    - Fix a panic issue that occurs to the coprocessor when the size of response exceeds 4 GiB [#9012](https://github.com/tikv/tikv/issues/9012)\\n    - Fix the issue that snapshot Garbage Collection (GC) misses GC snapshot files when snapshot files cannot be garbage collected [#10813](https://github.com/tikv/tikv/issues/10813)\\n    - Fix a panic issue caused by timeout when processing Coprocessor requests [#10852](https://github.com/tikv/tikv/issues/10852)\\n    - Fix a memory leak caused by monitoring data of statistics threads [#11195](https://github.com/tikv/tikv/issues/11195)\\n    - Fix a panic issue caused by getting the cgroup information from some platforms [#10980](https://github.com/tikv/tikv/pull/10980)\\n    - Fix the issue of poor scan performance because MVCC Deletion versions are not dropped by compaction filter GC [#11248](https://github.com/tikv/tikv/pull/11248)\\n\\n+ PD\\n\\n    - Fix the issue that PD incorrectly delete the peers with data and in pending status because the number of peers exceeds the number of configured peers [#4045](https://github.com/tikv/pd/issues/4045)\\n    - Fix the issue that PD does not fix down peers in time [#4077](https://github.com/tikv/pd/issues/4077)\\n    - Fix the issue that the scatter range scheduler cannot schedule empty Regions [#4118](https://github.com/tikv/pd/pull/4118)\\n    - Fix the issue that the key manager cost too much CPU [#4071](https://github.com/tikv/pd/issues/4071)\\n    - Fix the data race issue that might occur when setting configurations of hot Region scheduler [#4159](https://github.com/tikv/pd/issues/4159)\\n    - Fix slow leader election caused by stuck Region syncer [#3936](https://github.com/tikv/pd/issues/3936)\\n\\n+ TiFlash\\n\\n    - Fix the issue of inaccurate TiFlash Store Size statistics\\n    - Fix the issue that TiFlash fails to start up on some platforms due to the absence of library `nsl`\\n    - Block the infinite wait of `wait index` when writing pressure is heavy (a default timeout of 5 minutes is added), which prevents TiFlash from waiting too long for data replication to provide services\\n    - Fix the slow and no result issues of the log search when the log volume is large\\n    - Fix the issue that only the most recent logs can be searched when searching old historical logs\\n    - Fix the possible wrong result when a new collation is enabled\\n    - Fix the possible parsing errors when an SQL statement contains extremely long nested expressions\\n    - Fix the possible `Block schema mismatch` error of the Exchange operator\\n    - Fix the possible `Can\\'t compare` error when comparing Decimal types\\n    - Fix the `3rd arguments of function substringUTF8 must be constants` error of the `left/substring` function\\n\\n+ Tools\\n\\n    + TiCDC\\n\\n        - Fix the issue that TiCDC replication task might terminate when the upstream TiDB instance unexpectedly exits [#3061](https://github.com/pingcap/tiflow/issues/3061)\\n        - Fix the issue that TiCDC process might panic when TiKV sends duplicate requests to the same Region [#2386](https://github.com/pingcap/tiflow/issues/2386)\\n        - Fix unnecessary CPU consumption when verifying downstream TiDB/MySQL availability [#3073](https://github.com/pingcap/tiflow/issues/3073)\\n        - Fix the issue that the volume of Kafka messages generated by TiCDC is not constrained by `max-message-size` [#2962](https://github.com/pingcap/tiflow/issues/2962)\\n        - Fix the issue that TiCDC sync task might pause when an error occurs during writing a Kafka message [#2978](https://github.com/pingcap/tiflow/issues/2978)\\n        - Fix the issue that some partitioned tables without valid indexes might be ignored when `force-replicate` is enabled [#2834](https://github.com/pingcap/tiflow/issues/2834)\\n        - Fix the issue that scanning stock data might fail due to TiKV performing GC when scanning stock data takes too long [#2470](https://github.com/pingcap/tiflow/issues/2470)\\n        - Fix a possible panic issue when encoding some types of columns into Open Protocol format [#2758](https://github.com/pingcap/tiflow/issues/2758)\\n        - Fix a possible panic issue when encoding some types of columns into Avro format [#2648](https://github.com/pingcap/tiflow/issues/2648)\\n\\n    + TiDB Binlog\\n\\n        - Fix the issue that when most tables are filtered out, checkpoint cannot be updated under some special load [#1075](https://github.com/pingcap/tidb-binlog/pull/1075)\\n', doc_link='https://docs.pingcap.com/tidb/v8.1/release-5.3.0'),\n",
       " 16141: DocumentData(id=16141, chunks={}, content='---\\ntitle: TiDB TPC-C Performance Test Report -- v6.0.0 vs. v5.4.0\\nsummary: TiDB v6.0.0 TPC-C performance is 24.20% better than v5.4.0. The improvement is consistent across different thread counts, with the highest improvement at 26.97% for 100 threads.\\n---\\n\\n# TiDB TPC-C Performance Test Report -- v6.0.0 vs. v5.4.0\\n\\n## Test overview\\n\\nThis test aims at comparing the TPC-C performance of TiDB v6.0.0 and v5.4.0 in the Online Transactional Processing (OLTP) scenario. The results show that compared with v5.4.0, the TPC-C performance of v6.0.0 is improved by 24.20%.\\n\\n## Test environment (AWS EC2）\\n\\n### Hardware configuration\\n\\n| Service type | EC2 type | Instance count |\\n|:----------|:----------|:----------|\\n| PD        | m5.xlarge |     3     |\\n| TiKV      | i3.4xlarge|     3     |\\n| TiDB      | c5.4xlarge|     3     |\\n| TPC-C  | c5.9xlarge|     1     |\\n\\n### Software version\\n\\n| Service type | Software version  |\\n| :----------- | :---------------- |\\n| PD           | v5.4.0 and v6.0.0 |\\n| TiDB         | v5.4.0 and v6.0.0 |\\n| TiKV         | v5.4.0 and v6.0.0 |\\n| TiUP         | 1.9.3             |\\n| HAProxy      | 2.5.0             |\\n\\n### Parameter configuration\\n\\nTiDB v6.0.0 and TiDB v5.4.0 use the same configuration.\\n\\n#### TiDB parameter configuration\\n\\n\\n```yaml\\nlog.level: \"error\"\\nprepared-plan-cache.enabled: true\\ntikv-client.max-batch-wait-time: 2000000\\n```\\n\\n#### TiKV parameter configuration\\n\\n\\n```yaml\\npessimistic-txn.pipelined: true\\nraftdb.allow-concurrent-memtable-write: true\\nraftdb.max-background-jobs: 4\\nraftstore.apply-max-batch-size: 2048\\nraftstore.apply-pool-size: 3\\nraftstore.store-max-batch-size: 2048\\nraftstore.store-pool-size: 3\\nreadpool.storage.normal-concurrency: 10\\nrocksdb.max-background-jobs: 8\\nserver.grpc-concurrency: 6\\n```\\n\\n#### TiDB global variable configuration\\n\\n\\n```sql\\nset global tidb_hashagg_final_concurrency=1;\\nset global tidb_hashagg_partial_concurrency=1;\\nset global tidb_enable_async_commit = 1;\\nset global tidb_enable_1pc = 1;\\nset global tidb_guarantee_linearizability = 0;\\nset global tidb_enable_clustered_index = 1;\\n```\\n\\n#### HAProxy configuration - haproxy.cfg\\n\\nFor more details about how to use HAProxy on TiDB, see [Best Practices for Using HAProxy in TiDB](/best-practices/haproxy-best-practices.md).\\n\\n\\n```yaml\\nglobal                                     # Global configuration.\\n   pidfile     /var/run/haproxy.pid        # Writes the PIDs of HAProxy processes into this file.\\n   maxconn     4000                        # The maximum number of concurrent connections for a single HAProxy process.\\n   user        haproxy                     # The same with the UID parameter.\\n   group       haproxy                     # The same with the GID parameter. A dedicated user group is recommended.\\n   nbproc      64                          # The number of processes created when going daemon. When starting multiple processes to forward requests, ensure that the value is large enough so that HAProxy does not block processes.\\n   daemon                                  # Makes the process fork into background. It is equivalent to the command line \"-D\" argument. It can be disabled by the command line \"-db\" argument.\\ndefaults                                   # Default configuration.\\n   log global                              # Inherits the settings of the global configuration.\\n   retries 2                               # The maximum number of retries to connect to an upstream server. If the number of connection attempts exceeds the value, the backend server is considered unavailable.\\n   timeout connect  2s                     # The maximum time to wait for a connection attempt to a backend server to succeed. It should be set to a shorter time if the server is located on the same LAN as HAProxy.\\n   timeout client 30000s                   # The maximum inactivity time on the client side.\\n   timeout server 30000s                   # The maximum inactivity time on the server side.\\nlisten tidb-cluster                        # Database load balancing.\\n   bind 0.0.0.0:3390                       # The Floating IP address and listening port.\\n   mode tcp                                # HAProxy uses layer 4, the transport layer.\\n   balance leastconn                      # The server with the fewest connections receives the connection. \"leastconn\" is recommended where long sessions are expected, such as LDAP, SQL and TSE, rather than protocols using short sessions, such as HTTP. The algorithm is dynamic, which means that server weights might be adjusted on the fly for slow starts for instance.\\n   server tidb-1 10.9.18.229:4000 check inter 2000 rise 2 fall 3       # Detects port 4000 at a frequency of once every 2000 milliseconds. If it is detected as successful twice, the server is considered available; if it is detected as failed three times, the server is considered unavailable.\\n   server tidb-2 10.9.39.208:4000 check inter 2000 rise 2 fall 3\\n   server tidb-3 10.9.64.166:4000 check inter 2000 rise 2 fall 3\\n```\\n\\n### Prepare test data\\n\\n1. Deploy TiDB v6.0.0 and v5.4.0 using TiUP.\\n2. Create a database named `tpcc`: `create database tpcc;`.\\n3. Use BenchmarkSQL to import the TPC-C 5000 Warehouse data: `tiup bench tpcc prepare --warehouse 5000 --db tpcc -H 127.0.0.1 -p 4000`.\\n4. Run the `tiup bench tpcc run -U root --db tpcc --host 127.0.0.1 --port 4000 --time 1800s --warehouses 5000 --threads {{thread}}` command to perform stress tests on TiDB via HAProxy. For each concurrency, the test takes 30 minutes.\\n5. Extract the tpmC data of New Order from the result.\\n\\n## Test result\\n\\nCompared with v5.4.0, the TPC-C performance of v6.0.0 is **improved by 24.20%**.\\n\\n| Threads | v5.4.0 tpmC | v6.0.0 tpmC | tpmC improvement (%) |\\n|:----------|:----------|:----------|:----------|\\n|50|44822.8|54956.6|22.61|\\n|100|52150.3|66216.6|26.97|\\n|200|57344.9|72116.7|25.76|\\n|400|58675|71254.8|21.44|\\n\\n![TPC-C](https://download.pingcap.com/images/docs/tpcc_v540_vs_v600.png)\\n', doc_link='https://docs.pingcap.com/tidb/v8.1/v6.0-performance-benchmarking-with-tpcc'),\n",
       " 15792: DocumentData(id=15792, chunks={}, content=\"---\\ntitle: TiDB 5.0.1 Release Notes\\nsummary: TiDB 5.0.1 was released on April 24, 2021. The default value of `committer-concurrency` changed to 128. Various bug fixes and improvements were made to TiDB, TiKV, PD, TiFlash, and Tools. For example, TiDB fixed issues with query results and performance regression, while TiKV fixed issues with coprocessors and startup failures. Tools like TiDB Lightning and Backup & Restore also received bug fixes.\\n---\\n\\n# TiDB 5.0.1 Release Notes\\n\\nRelease date: April 24, 2021\\n\\nTiDB version: 5.0.1\\n\\n## Compatibility change\\n\\n- The default value of the `committer-concurrency` configuration item is changed from `16` to `128`.\\n\\n## Improvements\\n\\n+ TiDB\\n\\n    - Support the built-in function `VITESS_HASH()` [#23915](https://github.com/pingcap/tidb/pull/23915)\\n\\n+ TiKV\\n\\n    - Use `zstd` to compress the Region snapshot [#10005](https://github.com/tikv/tikv/pull/10005)\\n\\n+ PD\\n\\n    - Modify the Region score calculator to better satisfy isomerous stores [#3605](https://github.com/pingcap/pd/pull/3605)\\n    - Avoid unexpected statistics after adding the `scatter region` scheduler [#3602](https://github.com/pingcap/pd/pull/3602)\\n\\n+ Tools\\n\\n    + Backup & Restore (BR)\\n\\n        - Remove some misleading information from the summary log [#1009](https://github.com/pingcap/br/pull/1009)\\n\\n## Bug Fixes\\n\\n+ TiDB\\n\\n    - Fix the issue that the execution result of project elimination might be wrong when the projection result is empty [#24093](https://github.com/pingcap/tidb/pull/24093)\\n    - Fix the issue of wrong query results when a column contains `NULL` values in some cases [#24063](https://github.com/pingcap/tidb/pull/24063)\\n    - Forbid generating MPP plans when the scan contains virtual columns [#24058](https://github.com/pingcap/tidb/pull/24058)\\n    - Fix the wrong reuse of `PointGet` and `TableDual` in Plan Cache [#24043](https://github.com/pingcap/tidb/pull/24043)\\n    - Fix the error that occurs when the optimizer builds the `IndexMerge` plan for clustered indexes [#24042](https://github.com/pingcap/tidb/pull/24042)\\n    - Fix the type inference of the BIT-type errors [#24027](https://github.com/pingcap/tidb/pull/24027)\\n    - Fix the issue that some optimizer hints do not take effect when the `PointGet` operator exists [#23685](https://github.com/pingcap/tidb/pull/23685)\\n    - Fix the issue that DDL operations might fail when rolling back due to an error [#24080](https://github.com/pingcap/tidb/pull/24080)\\n    - Fix the issue that the index range of the binary literal constant is incorrectly built [#24041](https://github.com/pingcap/tidb/pull/24041)\\n    - Fix the potential wrong results of the `IN` clause in some cases [#24023](https://github.com/pingcap/tidb/pull/24023)\\n    - Fix the wrong results of some string functions  [#23879](https://github.com/pingcap/tidb/pull/23879)\\n    - Users now need both `INSERT` and `DELETE` privileges on a table to perform `REPLACE` operations [#23939](https://github.com/pingcap/tidb/pull/23939)\\n    - Fix the performance regression when executing the point query [#24070](https://github.com/pingcap/tidb/pull/24070)\\n    - Fix the wrong `TableDual` plans caused by incorrectly comparing binaries and bytes [#23918](https://github.com/pingcap/tidb/pull/23918)\\n\\n+ TiKV\\n\\n    - Fix the issue that the coprocessor fails to properly handle the signed or unsigned integer types in the `IN` expression [#10018](https://github.com/tikv/tikv/pull/10018)\\n    - Fix the issue of many empty Regions after batch ingesting SST files [#10015](https://github.com/tikv/tikv/pull/10015)\\n    - Fix the potential panic that occurs when the input of `cast_string_as_time` is invalid UTF-8 bytes [#9995](https://github.com/tikv/tikv/pull/9995)\\n    - Fix a bug that TiKV cannot start up after the file dictionary file is damaged [#9992](https://github.com/tikv/tikv/pull/9992)\\n\\n+ TiFlash\\n\\n    - Fix the issue that the storage engine fails to remove the data of some ranges\\n    - Fix the issue of incorrect results when casting the time type to the integer type\\n    - Fix a bug that the `receiver` cannot find corresponding tasks within 10 seconds\\n    - Fix the issue that there might be invalid iterators in `cancelMPPQuery`\\n    - Fix a bug that the behavior of the `bitwise` operator is different from that of TiDB\\n    - Fix the alert issue caused by overlapping ranges when using the `prefix key`\\n    - Fix the issue of incorrect results when casting the string type to the integer type\\n    - Fix the issue that consecutive and fast writes might make TiFlash out of memory\\n    - Fix the issue that duplicated column names will make TiFlash raise errors\\n    - Fix the issue that TiFlash fails to parse MPP plans\\n    - Fix the potential issue that the exception of null pointer might be raised during the table GC\\n    - Fix the TiFlash panic issue that occurs when writing data to dropped tables\\n    - Fix the issue that TiFlash might panic during BR restore\\n\\n+ Tools\\n\\n    + TiDB Lightning\\n\\n        - Fix the issue of the inaccurate table count in the progress log during the import [#1005](https://github.com/pingcap/br/pull/1005)\\n\\n    + Backup & Restore (BR)\\n\\n        - Fix a bug that the actual backup speed exceeds the `--ratelimit` limit [#1026](https://github.com/pingcap/br/pull/1026)\\n        - Fix the issue of backup interruption caused by the failure of a few TiKV nodes [#1019](https://github.com/pingcap/br/pull/1019)\\n        - Fix the issue of the inaccurate table count in the progress log during TiDB Lightning's import [#1005](https://github.com/pingcap/br/pull/1005)\\n\\n    + TiCDC\\n\\n        - Fix the concurrency issue in Unified Sorter and filter the unhelpful error messages [#1678](https://github.com/pingcap/tiflow/pull/1678)\\n        - Fix a bug that the creation of redundant directories might interrupt the replication with MinIO [#1672](https://github.com/pingcap/tiflow/pull/1672)\\n        - Set the default value of the `explicit_defaults_for_timestamp` session variable to `ON` to make the MySQL 5.7 downstream keep the same behavior with the upstream TiDB [#1659](https://github.com/pingcap/tiflow/pull/1659)\\n        - Fix the issue that the incorrect handling of `io.EOF` might cause replication interruption [#1648](https://github.com/pingcap/tiflow/pull/1648)\\n        - Correct the TiKV CDC endpoint CPU metric in the TiCDC dashboard [#1645](https://github.com/pingcap/tiflow/pull/1645)\\n        - Increase `defaultBufferChanSize` to avoid replication blocking in some cases [#1632](https://github.com/pingcap/tiflow/pull/1632)\\n\", doc_link='https://docs.pingcap.com/tidb/v8.1/release-5.0.1'),\n",
       " 15922: DocumentData(id=15922, chunks={}, content='---\\ntitle: TiDB 2.0.3 Release Notes\\nsummary: TiDB 2.0.3 was released on June 1, 2018, with improvements in system compatibility and stability. It includes various fixes and optimizations for TiDB, PD, and TiKV. Some highlights are support for modifying log level online, fixing issues with unique index and `ON DUPLICATE KEY UPDATE`, and addressing panic issues in specific conditions.\\n---\\n\\n# TiDB 2.0.3 Release Notes\\n\\nOn June 1, 2018, TiDB 2.0.3 is released. Compared with TiDB 2.0.2, this release has great improvement in system compatibility and stability.\\n\\n## TiDB\\n\\n- Support modifying the log level online\\n- Support the `COM_CHANGE_USER` command\\n- Support using the `TIME` type parameters under the binary protocol\\n- Optimize the cost estimation of query conditions with the `BETWEEN` expression\\n- Do not display the `FOREIGN KEY` information in the result of `SHOW CREATE TABLE`\\n- Optimize the cost estimation for queries with the `LIMIT` clause\\n- Fix the issue about the `YEAR` type as the unique index\\n- Fix the issue about `ON DUPLICATE KEY UPDATE` in conditions without the unique index\\n- Fix the compatibility issue of the `CEIL` function\\n- Fix the accuracy issue of the `DIV` calculation in the `DECIMAL` type\\n- Fix the false alarm of `ADMIN CHECK TABLE`\\n- Fix the panic issue of `MAX`/`MIN` under specific expression parameters\\n- Fix the issue that the result of `JOIN` is null in special conditions\\n- Fix the `IN` expression issue when building and querying Range\\n- Fix a Range calculation issue when using `Prepare` to query and `Plan Cache` is enabled\\n- Fix the issue that the Schema information is frequently loaded in abnormal conditions\\n\\n## PD\\n\\n- Fix the panic issue when collecting hot-cache metrics in specific conditions\\n- Fix the issue about scheduling of the obsolete Regions\\n\\n## TiKV\\n\\n- Fix the bug that the learner flag mistakenly reports to PD\\n- Report an error instead of getting a result if `divisor/dividend` is 0 in `do_div_mod`', doc_link='https://docs.pingcap.com/tidb/v8.1/release-2.0.3'),\n",
       " 15796: DocumentData(id=15796, chunks={}, content='---\\ntitle: TiDB Versioning\\nsummary: Learn the version numbering system of TiDB.\\n---\\n\\n# TiDB Versioning\\n\\n<Important>\\n\\nIt is recommended to always upgrade to the latest patch release of your release series.\\n\\n</Important>\\n\\nTiDB offers two release series:\\n\\n* Long-Term Support Releases\\n* Development Milestone Releases (introduced in TiDB v6.0.0)\\n\\nTo learn about the support policy for major releases of TiDB, see [TiDB Release Support Policy](https://www.pingcap.com/tidb-release-support-policy/).\\n\\n## Release versioning\\n\\nTiDB versioning has the form of `X.Y.Z`. `X.Y` represents a release series.\\n\\n- Since TiDB 1.0, `X` increments every year. Each `X` release introduces new features and improvements.\\n- `Y` increments from 0. Each `Y` release introduces new features and improvements.\\n- In the first release of a release series, `Z` is set to 0 by default. For patch releases, `Z` increments from 1.\\n\\nFor the versioning system of TiDB v5.0.0 and earlier versions, refer to [Historical versioning](#historical-versioning-deprecated).\\n\\n## Long-Term Support releases\\n\\nLong-Term Support (LTS) versions are released approximately every six months and introduce new features, improvements, bug fixes and security vulnerability fixes.\\n\\nLTS releases are versioned as `X.Y.Z`. `Z` defaults to 0.\\n\\nExample versions:\\n\\n- 6.1.0\\n- 5.4.0\\n\\nDuring the lifecycle of LTS, patch releases are made available on demand. Patch releases contain bug fixes and security vulnerability fixes, and do not introduce new features.\\n\\nPatch releases are versioned as `X.Y.Z`. `X.Y` is consistent with the corresponding LTS versioning. The patch number `Z` increments from 1.\\n\\nExample version:\\n\\n- 6.1.1\\n\\n<Note>\\n\\nv5.1.0, v5.2.0, v5.3.0, v5.4.0 were released only two months after their preceding releases, but all four releases are LTS and provide patch releases.\\n\\n</Note>\\n\\n## Development Milestone Releases\\n\\nDevelopment Milestone Releases (DMR) are released approximately every two months that do not contain LTS. DMR versions introduce new features, improvements and bug fixes. TiDB does not provide patch releases based on DMR, and any related bugs are fixed in the subsequent release series.\\n\\nDMRs are versioned as `X.Y.Z`. `Z` defaults to 0. A `-DMR` suffix is appended to the version number.\\n\\nExample version:\\n\\n- 6.0.0-DMR\\n\\n## Versioning of TiDB ecosystem tools\\n\\nSome TiDB tools are released together with the TiDB server and use the same version numbering system, such as TiDB Lightning. Some TiDB tools are released separately from the TiDB server and use their own version numbering system, such as TiUP and TiDB Operator.\\n\\n## Historical versioning (deprecated)\\n\\n### General Availability releases\\n\\nGeneral Availability (GA) releases are stable versions of the current release series of TiDB. GA versions are released after Release Candidate (RC) versions. GA can be used in production environments.\\n\\nExample versions:\\n\\n- 1.0\\n- 2.1 GA\\n- 5.0 GA\\n\\n### Release Candidate releases\\n\\nRelease Candidate (RC) releases introduce new features and improvements. RC versions are significantly more stable than Beta versions. RC can be used for early testing, but are not suitable for production.\\n\\nExample versions:\\n\\n- RC1\\n- 2.0-RC1\\n- 3.0.0-rc.1\\n\\n### Beta releases\\n\\nBeta releases introduces new features and improvements. Beta versions are greatly improved over Alpha versions and have eliminated critical bugs, but still contain some bugs. Beta releases are available for users to test the latest features.\\n\\nExample versions:\\n\\n- 1.1 Beta\\n- 2.1 Beta\\n- 4.0.0-beta.1\\n\\n### Alpha releases\\n\\nAlpha releases are internal releases for testing and introduce new features and improvements. Alpha releases are the initial versions of the current release series. Alpha releases might have some bugs and are available for users to test the latest features.\\n\\nExample version:\\n\\n- 1.1 Alpha\\n', doc_link='https://docs.pingcap.com/tidb/v8.1/versioning'),\n",
       " 15926: DocumentData(id=15926, chunks={}, content='---\\ntitle: TiDB 1.0 release notes\\nsummary: TiDB 1.0 is released with a focus on MySQL compatibility, SQL optimization, stability, and performance. It includes enhancements to the SQL query optimizer, internal data format optimization, and support for various operators. PD now supports read flow based balancing and setting store weight. TiKV has improved coprocessor support and performance, and added a Debug API. Special thanks to enterprises, open source software, and individual contributors for their support.\\n---\\n\\n# TiDB 1.0 Release Notes\\n\\nOn October 16, 2017, TiDB 1.0 is now released! This release is focused on MySQL compatibility, SQL optimization, stability, and performance.\\n\\n## TiDB\\n\\n- The SQL query optimizer:\\n    - Adjust the cost model\\n    - Analyze pushdown\\n    - Function signature pushdown\\n- Optimize the internal data format to reduce the interim data size\\n- Enhance the MySQL compatibility\\n- Support the `NO_SQL_CACHE` syntax and limit the cache usage in the storage engine\\n- Refactor the Hash Aggregator operator to reduce the memory usage\\n- Support the Stream Aggregator operator\\n\\n## PD\\n\\n- Support read flow based balancing\\n- Support setting the Store weight and weight based balancing\\n\\n## TiKV\\n\\n- Coprocessor now supports more pushdown functions\\n- Support pushing down the sampling operation\\n- Support manually triggering data compact to collect space quickly\\n- Improve the performance and stability\\n- Add a Debug API for debugging\\n- TiSpark Beta Release:\\n- Support configuration framework\\n- Support ThriftSever/JDBC and Spark SQL\\n\\n## Acknowledgement\\n\\n### Special thanks to the following enterprises and teams\\n\\n- Archon\\n- Mobike\\n- Samsung Electronics\\n- SpeedyCloud\\n- Tencent Cloud\\n- UCloud\\n\\n### Thanks to the open source software and services from the following organizations and individuals\\n\\n- Asta Xie\\n- CNCF\\n- CoreOS\\n- Databricks\\n- Docker\\n- Github\\n- Grafana\\n- gRPC\\n- Jepsen\\n- Kubernetes\\n- Namazu\\n- Prometheus\\n- RedHat\\n- RocksDB Team\\n- Rust Team\\n\\n### Thanks to the individual contributors\\n\\n- 8cbx\\n- Akihiro Suda\\n- aliyx\\n- alston111111\\n- andelf\\n- Andy Librian\\n- Arthur Yang\\n- astaxie\\n- Bai, Yang\\n- bailaohe\\n- Bin Liu\\n- Blame cosmos\\n- Breezewish\\n- Carlos Ferreira\\n- Ce Gao\\n- Changjian Zhang\\n- Cheng Lian\\n- Cholerae Hu\\n- Chu Chao\\n- coldwater\\n- Cole R Lawrence\\n- cuiqiu\\n- cuiyuan\\n- Cwen\\n- Dagang\\n- David Chen\\n- David Ding\\n- dawxy\\n- dcadevil\\n- Deshi Xiao\\n- Di Tang\\n- disksing\\n- dongxu\\n- dreamquster\\n- Drogon\\n- Du Chuan\\n- Dylan Wen\\n- eBoyy\\n- Eric Romano\\n- Ewan Chou\\n- Fiisio\\n- follitude\\n- Fred Wang\\n- fud\\n- fudali\\n- gaoyangxiaozhu\\n- Gogs\\n- goroutine\\n- Gregory Ian\\n- Guanqun Lu\\n- Guilherme Hübner Franco\\n- Haibin Xie\\n- Han Fei\\n- hawkingrei\\n- Hiroaki Nakamura\\n- hiwjd\\n- Hongyuan Wang\\n- Hu Ming\\n- Hu Ziming\\n- Huachao Huang\\n- HuaiyuXu\\n- Huxley Hu\\n- iamxy\\n- Ian\\n- insion\\n- iroi44\\n- Ivan.Yang\\n- Jack Yu\\n- jacky liu\\n- Jan Mercl\\n- Jason W\\n- Jay\\n- Jay Lee\\n- Jianfei Wang\\n- Jiaxing Liang\\n- Jie Zhou\\n- jinhelin\\n- Jonathan Boulle\\n- Karl Ostendorf\\n- knarfeh\\n- Kuiba\\n- leixuechun\\n- li\\n- Li Shihai\\n- Liao Qiang\\n- Light\\n- lijian\\n- Lilian Lee\\n- Liqueur Librazy\\n- Liu Cong\\n- Liu Shaohui\\n- liubo0127\\n- liyanan\\n- lkk2003rty\\n- Louis\\n- louishust\\n- luckcolors\\n- Lynn\\n- Mae Huang\\n- maiyang\\n- maxwell\\n- mengshangqi\\n- Michael Belenchenko\\n- mo2zie\\n- morefreeze\\n- MQ\\n- mxlxm\\n- Neil Shen\\n- netroby\\n- ngaut\\n- Nicole Nie\\n- nolouch\\n- onlymellb\\n- overvenus\\n- PaladinTyrion\\n- paulg\\n- Priya Seth\\n- qgxiaozhan\\n- qhsong\\n- Qiannan\\n- qiukeren\\n- qiuyesuifeng\\n- queenypingcap\\n- qupeng\\n- Rain Li\\n- ranxiaolong\\n- Ray\\n- Rick Yu\\n- shady\\n- ShawnLi\\n- Shen Li\\n- Sheng Tang\\n- Shirly\\n- Shuai Li\\n- ShuNing\\n- ShuYu Wang\\n- siddontang\\n- silenceper\\n- Simon J Mudd\\n- Simon Xia\\n- skimmilk6877\\n- sllt\\n- soup\\n- Sphinx\\n- Steffen\\n- sumBug\\n- sunhao2017\\n- Tao Meng\\n- Tao Zhou\\n- tennix\\n- tiancaiamao\\n- TianGuangyu\\n- Tristan Su\\n- ueizhou\\n- UncP\\n- Unknwon\\n- v01dstar\\n- Van\\n- WangXiangUSTC\\n- wangyanjun\\n- wangyisong1996\\n- weekface\\n- wegel\\n- Wei Fu\\n- Wenbin Xiao\\n- Wenting Li\\n- Wenxuan Shi\\n- winkyao\\n- woodpenker\\n- wuxuelian\\n- Xiang Li\\n- xiaojian cai\\n- Xuanjia Yang\\n- Xuanwo\\n- XuHuaiyu\\n- Yang Zhexuan\\n- Yann Autissier\\n- Yanzhe Chen\\n- Yiding Cui\\n- Yim\\n- youyouhu\\n- Yu Jun\\n- Yuwen Shen\\n- Zejun Li\\n- Zhang Yuning\\n- zhangjinpeng1987\\n- ZHAO Yijun\\n- Zhe-xuan Yang\\n- ZhengQian\\n- ZhengQianFang\\n- zhengwanbo\\n- ZhiFeng Hu\\n- Zhiyuan Zheng\\n- Zhou Tao\\n- Zhoubirdblue\\n- zhouningnan\\n- Ziyi Yan\\n- zs634134578\\n- zxylvlp\\n- zyguan\\n- zz-jason\\n', doc_link='https://docs.pingcap.com/tidb/v8.1/release-1.0-ga'),\n",
       " 30779: DocumentData(id=30779, chunks={}, content=\"---\\ntitle: TiDB Operator 1.0.3 Release Notes\\n---\\n\\n# TiDB Operator 1.0.3 Release Notes\\n\\nRelease date: November 13, 2019\\n\\nTiDB Operator version: 1.0.3\\n\\n## v1.0.3 What's New\\n\\n### Action Required\\n\\nACTION REQUIRED: This release upgrades default TiDB version to `v3.0.5` which fixed a serious [bug](https://github.com/pingcap/tidb/pull/12597) in TiDB. So if you are using TiDB `v3.0.4` or prior versions, you **must** upgrade to `v3.0.5`.\\n\\nACTION REQUIRED: This release adds the `timezone` support for [all charts](https://github.com/pingcap/tidb-operator/tree/master/charts).\\n\\nFor existing TiDB clusters. If the `timezone` in `tidb-cluster/values.yaml` has been customized to other timezones instead of the default `UTC`, then upgrading tidb-operator will trigger a rolling update for the related pods.\\n\\nThe related pods include `pump`, `drainer`, `discovery`, `monitor`, `scheduled backup`, `tidb-initializer`, and `tikv-importer`.\\n\\nThe time zone for all images maintained by `tidb-operator` should be `UTC`. If you use your own images, you need to make sure that the corresponding time zones are `UTC`.\\n\\n### Improvements\\n\\n- Add the `timezone` support for all containers of the TiDB cluster\\n- Support configuring resource requests and limits for all containers of the TiDB cluster\\n\\n## Detailed Bug Fixes and Changes\\n\\n- Upgrade default TiDB version to `v3.0.5` ([#1132](https://github.com/pingcap/tidb-operator/pull/1132))\\n- Add the `timezone` support for all containers of the TiDB cluster ([#1122](https://github.com/pingcap/tidb-operator/pull/1122))\\n- Support configuring resource requests and limits for all containers of the TiDB cluster ([#853](https://github.com/pingcap/tidb-operator/pull/853))\\n\", doc_link='https://docs.pingcap.com/tidb-in-kubernetes/v1.6/release-1.0.3')}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "knowledge_retrieved = {}\n",
    "for action in next_actions:\n",
    "    if action.tool == 'retrieve_knowledge':\n",
    "        data = gkb.retrieve_graph_data(session, action.query)\n",
    "    elif action.tool == 'retrieve_neighbors':\n",
    "        data = gkb.retrieve_neighbors(session, action.entity_ids, action.query)\n",
    "    elif action.tool == 'retrieve_documents':\n",
    "        data = gkb.retrieve_documents(session, action.query, 10)\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid tool: {action.tool}\")\n",
    "\n",
    "    for doc_id, doc in data.documents.items():\n",
    "        if doc_id not in knowledge_retrieved:\n",
    "            knowledge_retrieved[doc_id] = doc\n",
    "        \n",
    "        for chunk_id, chunk in doc.chunks.items():\n",
    "            if chunk_id not in knowledge_retrieved[doc_id].chunks:\n",
    "                knowledge_retrieved[doc_id].chunks[chunk_id] = chunk\n",
    "                continue\n",
    "\n",
    "            existing_chunk = knowledge_retrieved[doc_id].chunks[chunk_id]\n",
    "            rel_dict = {r['id']: r for r in existing_chunk.relationships}\n",
    "            for relationship in chunk.relationships:\n",
    "                rel_id = relationship.id\n",
    "                if rel_id in rel_dict:\n",
    "                    rel_dict[rel_id]['similarity_score'] = max(\n",
    "                        rel_dict[rel_id]['similarity_score'],\n",
    "                        relationship.similarity_score\n",
    "                    )\n",
    "                else:\n",
    "                    rel_dict[rel_id] = relationship.to_dict()\n",
    "\n",
    "            knowledge_retrieved[doc_id].chunks[chunk_id].relationships = list(rel_dict.values())\n",
    "\n",
    "action_history.append(action)\n",
    "\n",
    "knowledge_retrieved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing document(15758, https://docs.pingcap.com/tidb/v8.1/release-1.0.2)\n",
      "Update data: {'updated_answer': \"To find the latest version of TiDB, you should visit the official TiDB GitHub repository or the TiDB website. These platforms typically provide the most up-to-date information on releases. On GitHub, you can navigate to the 'Releases' section to see the latest version available. Additionally, the TiDB website often has a 'Download' or 'Releases' page that lists the current stable version. As of the information available, TiDB 1.0.2 was released on November 13, 2017, but this is likely outdated. Always check the official sources for the most recent version.\", 'changes_made': 'Since the document provided information about TiDB 1.0.2, which is from 2017, I included instructions on how to find the latest version by checking the official TiDB GitHub repository and website. This ensures the user can access the most current version, as the document itself is outdated.', 'temporal_reasoning': 'The document provided information about a version released in 2017, which is significantly outdated given the current year is 2025. Therefore, the answer emphasizes checking official sources for the latest version, as software updates frequently and newer versions are likely available.'}\n",
      "Processing document(15911, https://docs.pingcap.com/tidb/v8.1/release-1.0.3)\n",
      "Update data: {'updated_answer': \"To find the latest version of TiDB, you should visit the official TiDB GitHub repository or the TiDB website. These platforms typically provide the most up-to-date information on releases. On GitHub, you can navigate to the 'Releases' section to see the latest version available. Additionally, the TiDB website often has a 'Download' or 'Releases' page that lists the current stable version. As of the latest information available, TiDB 1.0.3 was released on November 28, 2017. However, this version is likely outdated given the current date. Always check the official sources for the most recent version.\", 'changes_made': 'The previous answer mentioned TiDB 1.0.2 as the latest version, which was outdated. I updated the answer to include TiDB 1.0.3 as the latest known version based on the new document content. I also emphasized the importance of checking official sources for the most current version, as the information from 2017 is likely outdated by 2025.', 'temporal_reasoning': 'The new document provided information about TiDB 1.0.3, which was released on November 28, 2017. This is more recent than the previously mentioned version 1.0.2, so it supersedes the older information. However, given that the current date is 2025, it is highly probable that newer versions have been released since then. Therefore, the answer advises checking the official TiDB sources for the latest version.'}\n",
      "Processing document(15909, https://docs.pingcap.com/tidb/v8.1/release-2.1.16)\n",
      "Update data: {'updated_answer': \"To find the latest version of TiDB, you should visit the official TiDB GitHub repository or the TiDB website. These platforms typically provide the most up-to-date information on releases. On GitHub, you can navigate to the 'Releases' section to see the latest version available. Additionally, the TiDB website often has a 'Download' or 'Releases' page that lists the current stable version. As of the latest information available, TiDB 2.1.16 was released on August 15, 2019. However, this version is likely outdated given the current date. Always check the official sources for the most recent version.\", 'changes_made': 'The previous answer mentioned TiDB 1.0.3 as the latest version, which was outdated. I updated the answer to reflect the information from the new document, which states that TiDB 2.1.16 was released on August 15, 2019. This ensures the answer is more current. I also emphasized the importance of checking official sources for the most recent version, as the provided version might still be outdated given the current date.', 'temporal_reasoning': 'The new document provided information about TiDB 2.1.16, which was released on August 15, 2019. This is more recent than the previously mentioned version 1.0.3 from 2017. Given the current date in 2025, it is likely that even version 2.1.16 is outdated. Therefore, the answer advises checking the official TiDB sources for the latest version, as software updates are frequent and newer versions are likely available.'}\n",
      "Processing document(15854, https://docs.pingcap.com/tidb/v8.1/release-3.0.0-rc.3)\n",
      "Update data: {'updated_answer': \"To find the latest version of TiDB, you should visit the official TiDB GitHub repository or the TiDB website. These platforms typically provide the most up-to-date information on releases. On GitHub, you can navigate to the 'Releases' section to see the latest version available. Additionally, the TiDB website often has a 'Download' or 'Releases' page that lists the current stable version. As of the latest information available, TiDB 3.0.0-rc.3 was released on June 21, 2019. However, this version is likely outdated given the current date. Always check the official sources for the most recent version.\", 'changes_made': 'The previous answer mentioned TiDB 2.1.16 as the latest version, which was incorrect based on the new document. The new document provided information about TiDB 3.0.0-rc.3, which is a more recent release. The answer was updated to reflect this newer version. The mention of TiDB 2.1.16 was removed as it is outdated.', 'temporal_reasoning': 'The new document provided a release note for TiDB 3.0.0-rc.3, dated June 21, 2019. This is a more recent version than the previously mentioned TiDB 2.1.16, which was released on August 15, 2019. Given the current date of January 29, 2025, both versions are likely outdated, but the update was necessary to reflect the most recent information available from the documents provided. The answer emphasizes checking official sources for the latest version, as the information from 2019 is not current.'}\n",
      "Processing document(15868, https://docs.pingcap.com/tidb/v8.1/release-notes)\n",
      "Update data: {'updated_answer': \"To find the latest version of TiDB, you should visit the official TiDB GitHub repository or the TiDB website. These platforms typically provide the most up-to-date information on releases. On GitHub, you can navigate to the 'Releases' section to see the latest version available. Additionally, the TiDB website often has a 'Download' or 'Releases' page that lists the current stable version. According to the latest release notes, the most recent version of TiDB is 8.1.1, which was released on August 27, 2024. Always check the official sources for the most recent version.\", 'changes_made': 'The previous answer mentioned TiDB 3.0.0-rc.3 as the latest version, which was outdated. I updated the answer to reflect the latest version, 8.1.1, based on the new document content. This ensures the user receives the most current information. I also removed the outdated version information and emphasized checking official sources for the latest updates.', 'temporal_reasoning': 'The new document provided a comprehensive list of TiDB versions, including their release dates. By identifying the most recent version, 8.1.1, released on August 27, 2024, I ensured the answer reflects the latest available information. This approach prioritizes the most current data, which is crucial for users seeking the latest software updates.'}\n",
      "Processing document(15821, https://docs.pingcap.com/tidb/v8.1/release-3.0.11)\n",
      "Update data: {'updated_answer': \"To find the latest version of TiDB, you should visit the official TiDB GitHub repository or the TiDB website. These platforms typically provide the most up-to-date information on releases. On GitHub, you can navigate to the 'Releases' section to see the latest version available. Additionally, the TiDB website often has a 'Download' or 'Releases' page that lists the current stable version. According to the latest release notes, the most recent version of TiDB is 8.1.1, which was released on August 27, 2024. Always check the official sources for the most recent version. Note that the document provided is about TiDB 3.0.11, which is an older version released on March 4, 2020, and is not the latest version.\", 'changes_made': 'The answer was updated to clarify that the document provided is about an older version, TiDB 3.0.11, released in 2020. This information was added to ensure that users are aware that the document does not contain the latest version information. The rest of the answer remains focused on guiding users to the official sources for the most current version.', 'temporal_reasoning': 'The temporal reasoning involved recognizing that the document provided is about a version released in 2020, which is significantly older than the latest version mentioned in the current answer (8.1.1, released in 2024). This understanding ensures that the answer directs users to seek the most recent version from official sources, acknowledging that the document does not contain the latest information.'}\n",
      "Processing document(15789, https://docs.pingcap.com/tidb/v8.1/release-timeline)\n",
      "Update data: {'updated_answer': \"To find the latest version of TiDB, you should visit the official TiDB GitHub repository or the TiDB website. These platforms typically provide the most up-to-date information on releases. On GitHub, you can navigate to the 'Releases' section to see the latest version available. Additionally, the TiDB website often has a 'Download' or 'Releases' page that lists the current stable version. According to the latest release notes, the most recent version of TiDB is 7.5.4, which was released on October 15, 2024. Always check the official sources for the most recent version. Note that the document provided is about TiDB 3.0.11, which is an older version released on March 4, 2020, and is not the latest version.\", 'changes_made': 'The previous answer incorrectly stated that the latest version of TiDB was 8.1.1, released on August 27, 2024. Based on the new document content, the latest version is actually 7.5.4, released on October 15, 2024. The answer was updated to reflect this new information. The mention of TiDB 3.0.11 as an older version was retained for context.', 'temporal_reasoning': 'The new document provided a comprehensive list of TiDB releases in reverse chronological order. By examining this list, it was clear that version 7.5.4, released on October 15, 2024, is the most recent version. This supersedes the previously mentioned version 8.1.1, which was released earlier on August 27, 2024. The temporal aspect of the release dates was crucial in determining the most current version.'}\n",
      "Processing document(15777, https://docs.pingcap.com/tidb/v8.1/release-3.0.7)\n",
      "Update data: {'updated_answer': \"To find the latest version of TiDB, you should visit the official TiDB GitHub repository or the TiDB website. These platforms typically provide the most up-to-date information on releases. On GitHub, you can navigate to the 'Releases' section to see the latest version available. Additionally, the TiDB website often has a 'Download' or 'Releases' page that lists the current stable version. According to the latest release notes, the most recent version of TiDB is 7.5.4, which was released on October 15, 2024. Always check the official sources for the most recent version. Note that the document provided is about TiDB 3.0.7, which is an older version released on December 4, 2019, and is not the latest version.\", 'changes_made': 'The new document provided information about TiDB 3.0.7, which is an older version than the previously mentioned TiDB 3.0.11. I clarified that TiDB 3.0.7 is not the latest version and maintained the information about the most recent version, TiDB 7.5.4, as of October 15, 2024. The document content was acknowledged but did not alter the current answer since it pertains to an older release.', 'temporal_reasoning': 'The document provided details about TiDB 3.0.7, released on December 4, 2019. Since the query is about finding the latest version, the information about TiDB 3.0.7 is outdated compared to the latest version, TiDB 7.5.4, released on October 15, 2024. Therefore, the older version information was noted but did not change the current answer, which focuses on the most recent release.'}\n",
      "Processing document(15775, https://docs.pingcap.com/tidb/v8.1/release-2.1-ga)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgraph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mknowledge_synthesizer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KnowledgeSynthesizer\n\u001b[1;32m      3\u001b[0m synthesizer \u001b[38;5;241m=\u001b[39m KnowledgeSynthesizer(llm_client)\n\u001b[0;32m----> 4\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43msynthesizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterative_answer_synthesis\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mknowledge_retrieved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreasoning\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreasoning\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Access the results\u001b[39;00m\n\u001b[1;32m     11\u001b[0m final_answer \u001b[38;5;241m=\u001b[39m result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfinal_answer\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/Work/graph_toolkit/graph/knowledge_synthesizer.py:370\u001b[0m, in \u001b[0;36mKnowledgeSynthesizer.iterative_answer_synthesis\u001b[0;34m(self, query, documents, reasoning)\u001b[0m\n\u001b[1;32m    323\u001b[0m             prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mCurrent Time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent_time\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[1;32m    324\u001b[0m \n\u001b[1;32m    325\u001b[0m \u001b[38;5;124mYour task is to help build and improve an answer to a query by analyzing new information.\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[38;5;124m- Focus on building a clear, accurate answer\u001b[39m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m    368\u001b[0m             \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    369\u001b[0m                 \u001b[38;5;66;03m# Get LLM's analysis and updates\u001b[39;00m\n\u001b[0;32m--> 370\u001b[0m                 raw_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    371\u001b[0m                 json_str \u001b[38;5;241m=\u001b[39m extract_json(raw_response)\n\u001b[1;32m    372\u001b[0m                 update_data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(json_str)\n",
      "File \u001b[0;32m~/Work/graph_toolkit/llm_inference/base.py:255\u001b[0m, in \u001b[0;36mLLMInterface.generate\u001b[0;34m(self, prompt, context, **kwargs)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate\u001b[39m(\n\u001b[1;32m    252\u001b[0m     \u001b[38;5;28mself\u001b[39m, prompt: \u001b[38;5;28mstr\u001b[39m, context: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    253\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m    254\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 255\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprovider\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    257\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLLM generation failed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Work/graph_toolkit/llm_inference/base.py:77\u001b[0m, in \u001b[0;36mOpenAIProvider.generate\u001b[0;34m(self, prompt, context, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate\u001b[39m(\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28mself\u001b[39m, prompt: \u001b[38;5;28mstr\u001b[39m, context: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m     75\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m     76\u001b[0m     full_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcontext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mprompt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m context \u001b[38;5;28;01melse\u001b[39;00m prompt\n\u001b[0;32m---> 77\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_with_exponential_backoff\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msystem\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mYou are a helpful assistant.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfull_prompt\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent\u001b[38;5;241m.\u001b[39mstrip()\n",
      "File \u001b[0;32m~/Work/graph_toolkit/llm_inference/base.py:27\u001b[0m, in \u001b[0;36mBaseLLMProvider._retry_with_exponential_backoff\u001b[0;34m(self, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m attempt \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_retries):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     29\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m attempt \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_retries \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/graph/lib/python3.11/site-packages/openai/_utils/_utils.py:275\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    273\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 275\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/graph/lib/python3.11/site-packages/openai/resources/chat/completions.py:859\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    818\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    819\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    856\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    857\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[1;32m    858\u001b[0m     validate_response_format(response_format)\n\u001b[0;32m--> 859\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maudio\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    866\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    867\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    871\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_completion_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    872\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    873\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodalities\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparallel_tool_calls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprediction\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreasoning_effort\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    882\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mservice_tier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    883\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    884\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    885\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    886\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    887\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    888\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    889\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    891\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    892\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    893\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    894\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    895\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    896\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    897\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    898\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    899\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    900\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    901\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/graph/lib/python3.11/site-packages/openai/_base_client.py:1280\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1266\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1267\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1268\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1275\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1276\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1277\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1278\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1279\u001b[0m     )\n\u001b[0;32m-> 1280\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/graph/lib/python3.11/site-packages/openai/_base_client.py:957\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    954\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    955\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 957\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    958\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    959\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    960\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    961\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    962\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    963\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/graph/lib/python3.11/site-packages/openai/_base_client.py:993\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m    990\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSending HTTP Request: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, request\u001b[38;5;241m.\u001b[39mmethod, request\u001b[38;5;241m.\u001b[39murl)\n\u001b[1;32m    992\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 993\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    994\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    995\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_should_stream_response_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    996\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    997\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    998\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mTimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    999\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncountered httpx.TimeoutException\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/graph/lib/python3.11/site-packages/httpx/_client.py:914\u001b[0m, in \u001b[0;36mClient.send\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m    910\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_timeout(request)\n\u001b[1;32m    912\u001b[0m auth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_request_auth(request, auth)\n\u001b[0;32m--> 914\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    915\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    916\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    918\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    919\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    920\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    921\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "File \u001b[0;32m~/miniconda3/envs/graph/lib/python3.11/site-packages/httpx/_client.py:942\u001b[0m, in \u001b[0;36mClient._send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m    939\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[1;32m    941\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 942\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    943\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    944\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    945\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    947\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    948\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/graph/lib/python3.11/site-packages/httpx/_client.py:979\u001b[0m, in \u001b[0;36mClient._send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m    976\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequest\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    977\u001b[0m     hook(request)\n\u001b[0;32m--> 979\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    980\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    981\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/miniconda3/envs/graph/lib/python3.11/site-packages/httpx/_client.py:1014\u001b[0m, in \u001b[0;36mClient._send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1009\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1010\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1011\u001b[0m     )\n\u001b[1;32m   1013\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39mrequest):\n\u001b[0;32m-> 1014\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mtransport\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, SyncByteStream)\n\u001b[1;32m   1018\u001b[0m response\u001b[38;5;241m.\u001b[39mrequest \u001b[38;5;241m=\u001b[39m request\n",
      "File \u001b[0;32m~/miniconda3/envs/graph/lib/python3.11/site-packages/httpx/_transports/default.py:250\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    237\u001b[0m req \u001b[38;5;241m=\u001b[39m httpcore\u001b[38;5;241m.\u001b[39mRequest(\n\u001b[1;32m    238\u001b[0m     method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[1;32m    239\u001b[0m     url\u001b[38;5;241m=\u001b[39mhttpcore\u001b[38;5;241m.\u001b[39mURL(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    247\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    248\u001b[0m )\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[0;32m--> 250\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[1;32m    255\u001b[0m     status_code\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mstatus,\n\u001b[1;32m    256\u001b[0m     headers\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m    257\u001b[0m     stream\u001b[38;5;241m=\u001b[39mResponseStream(resp\u001b[38;5;241m.\u001b[39mstream),\n\u001b[1;32m    258\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    259\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/graph/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:256\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    253\u001b[0m         closing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_requests_to_connections()\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_connections(closing)\n\u001b[0;32m--> 256\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n",
      "File \u001b[0;32m~/miniconda3/envs/graph/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:236\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    232\u001b[0m connection \u001b[38;5;241m=\u001b[39m pool_request\u001b[38;5;241m.\u001b[39mwait_for_connection(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[0;32m--> 236\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[1;32m    242\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[1;32m    244\u001b[0m     pool_request\u001b[38;5;241m.\u001b[39mclear_connection()\n",
      "File \u001b[0;32m~/miniconda3/envs/graph/lib/python3.11/site-packages/httpcore/_sync/http_proxy.py:343\u001b[0m, in \u001b[0;36mTunnelHTTPConnection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    336\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection \u001b[38;5;241m=\u001b[39m HTTP11Connection(\n\u001b[1;32m    337\u001b[0m                 origin\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_remote_origin,\n\u001b[1;32m    338\u001b[0m                 stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m    339\u001b[0m                 keepalive_expiry\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_keepalive_expiry,\n\u001b[1;32m    340\u001b[0m             )\n\u001b[1;32m    342\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connected \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 343\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_connection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/graph/lib/python3.11/site-packages/httpcore/_sync/http11.py:136\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_closed\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m    135\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_closed()\n\u001b[0;32m--> 136\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[0;32m~/miniconda3/envs/graph/lib/python3.11/site-packages/httpcore/_sync/http11.py:106\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreceive_response_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request, kwargs\n\u001b[1;32m     99\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m    100\u001b[0m     (\n\u001b[1;32m    101\u001b[0m         http_version,\n\u001b[1;32m    102\u001b[0m         status,\n\u001b[1;32m    103\u001b[0m         reason_phrase,\n\u001b[1;32m    104\u001b[0m         headers,\n\u001b[1;32m    105\u001b[0m         trailing_data,\n\u001b[0;32m--> 106\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m     trace\u001b[38;5;241m.\u001b[39mreturn_value \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    108\u001b[0m         http_version,\n\u001b[1;32m    109\u001b[0m         status,\n\u001b[1;32m    110\u001b[0m         reason_phrase,\n\u001b[1;32m    111\u001b[0m         headers,\n\u001b[1;32m    112\u001b[0m     )\n\u001b[1;32m    114\u001b[0m network_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_network_stream\n",
      "File \u001b[0;32m~/miniconda3/envs/graph/lib/python3.11/site-packages/httpcore/_sync/http11.py:177\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_response_headers\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    174\u001b[0m timeout \u001b[38;5;241m=\u001b[39m timeouts\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 177\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11\u001b[38;5;241m.\u001b[39mResponse):\n\u001b[1;32m    179\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/graph/lib/python3.11/site-packages/httpcore/_sync/http11.py:217\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_event\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    214\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mnext_event()\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11\u001b[38;5;241m.\u001b[39mNEED_DATA:\n\u001b[0;32m--> 217\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_network_stream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;241m==\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mtheir_state \u001b[38;5;241m==\u001b[39m h11\u001b[38;5;241m.\u001b[39mSEND_RESPONSE:\n",
      "File \u001b[0;32m~/miniconda3/envs/graph/lib/python3.11/site-packages/httpcore/_backends/sync.py:128\u001b[0m, in \u001b[0;36mSyncStream.read\u001b[0;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39msettimeout(timeout)\n\u001b[0;32m--> 128\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/graph/lib/python3.11/ssl.py:1295\u001b[0m, in \u001b[0;36mSSLSocket.recv\u001b[0;34m(self, buflen, flags)\u001b[0m\n\u001b[1;32m   1291\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1292\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1293\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1294\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1295\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuflen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1296\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1297\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv(buflen, flags)\n",
      "File \u001b[0;32m~/miniconda3/envs/graph/lib/python3.11/ssl.py:1168\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1166\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[1;32m   1167\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1168\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1169\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SSLError \u001b[38;5;28;01mas\u001b[39;00m x:\n\u001b[1;32m   1170\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m SSL_ERROR_EOF \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msuppress_ragged_eofs:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from graph.knowledge_synthesizer import KnowledgeSynthesizer\n",
    "\n",
    "synthesizer = KnowledgeSynthesizer(llm_client)\n",
    "result = synthesizer.iterative_answer_synthesis(\n",
    "    query=query,\n",
    "    documents=knowledge_retrieved,\n",
    "    reasoning=reasoning\n",
    ")\n",
    "\n",
    "# Access the results\n",
    "final_answer = result[\"final_answer\"]\n",
    "evolution = result[\"evolution_history\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_answer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graph",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
