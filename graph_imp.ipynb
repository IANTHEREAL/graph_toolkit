{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from setting.db import SessionLocal\n",
    "from llm_inference.base import LLMInterface\n",
    "from graph.graph_knowledge_base import GraphKnowledgeBase, SearchAction\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "llm_client = LLMInterface(\"openai\", \"o3-mini\")\n",
    "\n",
    "gkb = GraphKnowledgeBase(llm_client, \"entities_150001\", \"relationships_150001\", \"chunks_150001\")\n",
    "session = SessionLocal()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"在 TiDB 中, 如果某个节点发生故障 (down机), 并且该节点的实例一直存在, 那么在故障节点的实例副本全部迁移完成后, down-peer 的数量会减少吗？请详细说明 TiDB 的副本迁移机制和 down-peer 数量变化的过程。\"\n",
    "model_kwargs = {\n",
    "    \"options\": {\n",
    "        \"num_ctx\": 8092,\n",
    "        \"num_gpu\": 80,\n",
    "        \"num_predict\": 10000,\n",
    "        \"temperature\": 0.1,\n",
    "    }\n",
    "}\n",
    "model_kwargs = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gkb.retrieve_documents(session, \"TiDB fault tolerance behavior during node failure\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis Result:\n",
      "Reasoning: At the most basic level, the user's question revolves around understanding the relationship between a node failure and the subsequent behavior of TiDB’s internal metrics—specifically, the number of down-peers—after the system has automatically migrated the data replicas from the failed node. Breaking it down from first principles, we first recognize that a 'down-peer' in TiDB is essentially a replica that is detected as unavailable or unresponsive. The process of replica migration is a core self-healing mechanism in distributed systems: once a node becomes unresponsive (even if its corresponding instance still exists), TiDB is designed to move its data replicas to other healthy nodes to preserve data redundancy and availability. This setup indicates a cause-and-effect relationship, where the failure triggers a recovery mechanism, which in turn should eventually adjust the system metrics (like the down-peer count). Essentially, the fundamental inquiry is about whether and how the metric changes once the system completes its reconciliation of data (all replicas from the failed node having been successfully relocated). The user is not simply asking for a procedural description but is attempting to understand the intrinsic behavior of TiDB's distributed architecture under failure conditions. The fundamental need is to grasp the underlying dynamics of TiDB’s fault tolerance: what processes are in place for handling node failures, how these processes affect reported statuses (like down-peer counts), and whether the system's self-healing mechanisms can lead to a normalization of metrics once the migration is complete.\n",
      "Intent:\n",
      "    Action: explain\n",
      "    Target: TiDB's replica migration mechanism and down-peer metric evolution\n",
      "    Context: Understanding TiDB's self-healing and fault tolerance behavior within a distributed database system during node failure scenarios\n",
      "Initial Queries:\n",
      "    TiDB node failure replica migration process\n",
      "    TiDB down-peer metric behavior after replica migration\n",
      "    How does TiDB handle node failures and update down-peer counts?\n",
      "    TiDB replica migration mechanism and metric normalization\n"
     ]
    }
   ],
   "source": [
    "from graph.query_analyzer import DeepUnderstandingAnalyzer\n",
    "\n",
    "analyzer = DeepUnderstandingAnalyzer(llm_client)\n",
    "analysis_res = analyzer.perform(query)\n",
    "print(analysis_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_history = []\n",
    "current_findings = []\n",
    "docs = {}\n",
    "\n",
    "next_actions = [SearchAction(\n",
    "    tool=\"retrieve_documents\",\n",
    "    query=a\n",
    ") for a in analysis_res.initial_queries]\n",
    "\n",
    "reasoning = analysis_res.reasoning\n",
    "queries = analysis_res.initial_queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SearchAction(tool=retrieve_documents, query=TiDB node failure replica migration process)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:graph.chunk_filter:Filter chunks ['7d9ec6766af442c4a9caf13707ad492a', '34cf7fad4a7747aeb1dba70f006b58df', '60edc9f98e204e9ab2f937c600f9b9ca', '3f95c501d40049cb833bf2ddc19944e8', 'c47725fec0f04f14bce35d3ed8e66720']\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:graph.chunk_filter:Filter Eval Result {'chunk_id': '7d9ec6766af442c4a9caf13707ad492a', 'is_relevant': False, 'confidence': 0.9, 'reasoning': 'This chunk describes migrating data from one TiDB cluster to another and the steps for full data migration via backup/restore. It does not mention node failure or replica migration processes, which is the core of the query.'}\n",
      "INFO:graph.chunk_filter:Filter Eval Result {'chunk_id': '34cf7fad4a7747aeb1dba70f006b58df', 'is_relevant': False, 'confidence': 0.9, 'reasoning': 'This chunk focuses on handling failed DDL statements during TiDB data migration. It does not address node failure or processes specific to replica migration due to a node failure.'}\n",
      "INFO:graph.chunk_filter:Filter Eval Result {'chunk_id': '60edc9f98e204e9ab2f937c600f9b9ca', 'is_relevant': False, 'confidence': 0.9, 'reasoning': 'This chunk details the steps to skip DDL statements when a migration gets interrupted. There is no information regarding node failures or the replica migration process caused by such failures.'}\n",
      "INFO:graph.chunk_filter:Filter Eval Result {'chunk_id': '3f95c501d40049cb833bf2ddc19944e8', 'is_relevant': False, 'confidence': 0.9, 'reasoning': 'This chunk explains the process of migrating full data from one TiDB cluster to another using backup and restore. It does not discuss node failure or replica migration procedures related to node failures.'}\n",
      "INFO:graph.chunk_filter:Filter Eval Result {'chunk_id': 'c47725fec0f04f14bce35d3ed8e66720', 'is_relevant': False, 'confidence': 0.9, 'reasoning': 'This chunk is another example of handling failed DDL statements (skipping them) in the migration context. It does not cover the specific topic of node failure or replica migration resulting from node failures.'}\n",
      "INFO:graph.chunk_filter:Filter chunks ['aa5417b0cd10420399c6ddf6b5bbb80c', '40bde88511df4ce1842260d5b36d43f3', '9160f9054f8b4d9ea0124789a5e7dc7b', '81262ef734f7424cab4488e63d2af531', '26bb80f928004edcacc754111bcb67f9']\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:graph.chunk_filter:Filter Eval Result {'chunk_id': 'aa5417b0cd10420399c6ddf6b5bbb80c', 'is_relevant': False, 'confidence': 0.9, 'reasoning': 'This chunk consists of TiDB 7.4.0 release notes with bug fixes unrelated to node failure or replica migration processes.'}\n",
      "INFO:graph.chunk_filter:Filter Eval Result {'chunk_id': '40bde88511df4ce1842260d5b36d43f3', 'is_relevant': False, 'confidence': 0.9, 'reasoning': 'This chunk covers techniques for bulk data insertion and related tools, with no mention of node failure or replica migration.'}\n",
      "INFO:graph.chunk_filter:Filter Eval Result {'chunk_id': '9160f9054f8b4d9ea0124789a5e7dc7b', 'is_relevant': False, 'confidence': 0.85, 'reasoning': 'Although this chunk discusses handling failed DDL statements in TiDB Data Migration, it does not discuss node failure or the replica migration process.'}\n",
      "INFO:graph.chunk_filter:Filter Eval Result {'chunk_id': '81262ef734f7424cab4488e63d2af531', 'is_relevant': False, 'confidence': 0.9, 'reasoning': 'This chunk is a quick start guide for the TiDB platform and does not include information about node failure or replica migration.'}\n",
      "INFO:graph.chunk_filter:Filter Eval Result {'chunk_id': '26bb80f928004edcacc754111bcb67f9', 'is_relevant': False, 'confidence': 0.9, 'reasoning': 'This chunk consists of TiDB 6.1.3 release notes with various bug fixes that do not address node failure replica migration processes.'}\n",
      "INFO:graph.chunk_filter:Filter chunks ['787ae113a4e64cb1a619498335936bbf', '60ba80257982426cb7e9d9ea0714b41d', 'bfe13c052dfd492594ff07d7985205e7', '6c8e966303804304812060a6a921d4a3', '3c7a21b0cb5d4248a8793b4ae92ec3c7']\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:graph.chunk_filter:Filter Eval Result {'chunk_id': '787ae113a4e64cb1a619498335936bbf', 'is_relevant': False, 'confidence': 0.9, 'reasoning': 'This chunk focuses on data insertion and bulk-import techniques using various tools (Dumpling, TiDB Lightning, DM, BR) and does not mention node failures or replica migration processes.'}\n",
      "INFO:graph.chunk_filter:Filter Eval Result {'chunk_id': '60ba80257982426cb7e9d9ea0714b41d', 'is_relevant': False, 'confidence': 0.9, 'reasoning': 'This chunk contains TiDB 5.3.2 release notes addressing bug fixes (including issues in TiCDC and DM), but it does not address the process for migrating replicas following a node failure.'}\n",
      "INFO:graph.chunk_filter:Filter Eval Result {'chunk_id': 'bfe13c052dfd492594ff07d7985205e7', 'is_relevant': False, 'confidence': 0.9, 'reasoning': 'This chunk is part of the TiDB Troubleshooting Map concerning DM and issues related to replication tasks (such as connection errors and relay log issues), but it does not mention or describe the node failure replica migration process.'}\n",
      "INFO:graph.chunk_filter:Filter Eval Result {'chunk_id': '6c8e966303804304812060a6a921d4a3', 'is_relevant': False, 'confidence': 0.8, 'reasoning': 'This chunk details how to replace nodes in a TiDB cluster on local disks (covering node replacement, scaling processes, and certificate signing), which is related to handling node-level changes. However, it does not specifically address the automatic process of replica migration when a node fails.'}\n",
      "INFO:graph.chunk_filter:Filter Eval Result {'chunk_id': '3c7a21b0cb5d4248a8793b4ae92ec3c7', 'is_relevant': False, 'confidence': 0.9, 'reasoning': 'This chunk explains how to use DM on Kubernetes to migrate MySQL data into TiDB. It is focused on data migration tasks rather than on the node failure replica migration process.'}\n",
      "INFO:graph.chunk_filter:Filter chunks ['3422e2375eeb4ebdb392a04546fe29d0', '9ccee47620124aca895b7660d3d4260a', '0970e7a8a1df490baca5144f29b159bb', '82010e1edfdf43cc91a71fdeb86b4d64']\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:graph.chunk_filter:Filter Eval Result {'chunk_id': '3422e2375eeb4ebdb392a04546fe29d0', 'is_relevant': False, 'confidence': 0.95, 'reasoning': 'The chunk discusses handling failed DDL statements during TiDB data migration, which is not related to the node failure replica migration process.'}\n",
      "INFO:graph.chunk_filter:Filter Eval Result {'chunk_id': '9ccee47620124aca895b7660d3d4260a', 'is_relevant': False, 'confidence': 0.95, 'reasoning': 'This chunk covers migrating and merging MySQL shards into TiDB using DM for incremental replication. It does not address TiDB node failure or the replica migration process.'}\n",
      "INFO:graph.chunk_filter:Filter Eval Result {'chunk_id': '0970e7a8a1df490baca5144f29b159bb', 'is_relevant': False, 'confidence': 0.9, 'reasoning': 'This chunk details migration of TiDB to Kubernetes by scaling in PD nodes and adjusting cluster configuration. It does not specifically discuss handling node failures or replica migration in the context of node failure.'}\n",
      "INFO:graph.chunk_filter:Filter Eval Result {'chunk_id': '82010e1edfdf43cc91a71fdeb86b4d64', 'is_relevant': False, 'confidence': 0.95, 'reasoning': 'This chunk focuses on data checking in sharding scenarios using sync-diff-inspector, which is unrelated to the process of replica migration due to TiDB node failure.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SearchAction(tool=retrieve_documents, query=TiDB down-peer metric behavior after replica migration)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:graph.chunk_filter:Filter chunks ['1f4539140b64445fa4c40ba0d247c183', '657b346a50934f27aa4f687aca9192dc', 'bfe13c052dfd492594ff07d7985205e7', 'fa5f56759f4c43668c9f18253ed1190a', '60ba80257982426cb7e9d9ea0714b41d']\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:graph.chunk_filter:Filter Eval Result {'chunk_id': '1f4539140b64445fa4c40ba0d247c183', 'is_relevant': False, 'confidence': 0.9, 'reasoning': 'This chunk is a release note for TiDB 3.0.0-rc.2 and its components. It does not mention anything about the down-peer metric nor replica migration.'}\n",
      "INFO:graph.chunk_filter:Filter Eval Result {'chunk_id': '657b346a50934f27aa4f687aca9192dc', 'is_relevant': False, 'confidence': 0.9, 'reasoning': 'This is the TiDB 7.6.0 release note focusing on various bug fixes, without any mention of down-peer metrics or replica migration behavior.'}\n",
      "INFO:graph.chunk_filter:Filter Eval Result {'chunk_id': 'bfe13c052dfd492594ff07d7985205e7', 'is_relevant': False, 'confidence': 0.9, 'reasoning': 'This chunk is part of the TiDB Troubleshooting Map and relates to Data Migration issues. It does not reference the down-peer metric or any behavior after replica migration.'}\n",
      "INFO:graph.chunk_filter:Filter Eval Result {'chunk_id': 'fa5f56759f4c43668c9f18253ed1190a', 'is_relevant': False, 'confidence': 0.9, 'reasoning': 'This chunk repeats the TiDB 3.0.0-rc.2 release notes summary. It does not cover any details regarding the down-peer metric or replica migration.'}\n",
      "INFO:graph.chunk_filter:Filter Eval Result {'chunk_id': '60ba80257982426cb7e9d9ea0714b41d', 'is_relevant': False, 'confidence': 0.9, 'reasoning': 'This chunk provides TiDB 5.3.2 release notes with bug fixes for various components, but it does not mention down-peer metrics or replica migration behavior.'}\n",
      "INFO:graph.chunk_filter:Filter chunks ['8befdbd837174633ad01a0c0bed5d153', '96c1b86a8eec4d80b563ee56e9519ddf', '26bb80f928004edcacc754111bcb67f9', '7d9ec6766af442c4a9caf13707ad492a', 'aa5417b0cd10420399c6ddf6b5bbb80c']\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:graph.chunk_filter:Filter Eval Result {'chunk_id': '8befdbd837174633ad01a0c0bed5d153', 'is_relevant': False, 'confidence': 0.8, 'reasoning': 'This chunk is a glossary for TiDB Data Migration and covers terminology (e.g., binlog, checkpoint) without any mention of down-peer metrics or replica migration behavior.'}\n",
      "INFO:graph.chunk_filter:Filter Eval Result {'chunk_id': '96c1b86a8eec4d80b563ee56e9519ddf', 'is_relevant': False, 'confidence': 0.8, 'reasoning': 'The document describes daily checks and monitoring of DM metrics via Prometheus/Grafana, but it does not mention down-peer metric behavior or replica migration.'}\n",
      "INFO:graph.chunk_filter:Filter Eval Result {'chunk_id': '26bb80f928004edcacc754111bcb67f9', 'is_relevant': False, 'confidence': 0.9, 'reasoning': 'This chunk is about the TiDB 6.1.3 Release Notes and bug fixes, with no reference to down-peer metrics or replica migration.'}\n",
      "INFO:graph.chunk_filter:Filter Eval Result {'chunk_id': '7d9ec6766af442c4a9caf13707ad492a', 'is_relevant': False, 'confidence': 0.9, 'reasoning': 'The document explains how to migrate data from one TiDB cluster to another but does not address down-peer metric behavior after replica migration.'}\n",
      "INFO:graph.chunk_filter:Filter Eval Result {'chunk_id': 'aa5417b0cd10420399c6ddf6b5bbb80c', 'is_relevant': False, 'confidence': 0.85, 'reasoning': 'This chunk contains release notes for TiDB 7.4.0 and includes various bug fixes for DM and other components, but there is no mention of down-peer metrics or replica migration.'}\n",
      "INFO:graph.chunk_filter:Filter chunks ['787ae113a4e64cb1a619498335936bbf', '968d59f87a2e46a8815c6da9031f13d0', '17b348762b7e4f36be27698c3f9d0f3d', '214ab45f1be141f5b5dd641b1be13a57', 'f534c65eb2954af69d917fd8ff1db9ac']\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:graph.chunk_filter:Filter Eval Result {'chunk_id': '787ae113a4e64cb1a619498335936bbf', 'is_relevant': False, 'confidence': 1.0, 'reasoning': 'This chunk discusses bulk-insert techniques and data loading tools in TiDB, with no mention of down-peer metrics or replica migration.'}\n",
      "INFO:graph.chunk_filter:Filter Eval Result {'chunk_id': '968d59f87a2e46a8815c6da9031f13d0', 'is_relevant': False, 'confidence': 1.0, 'reasoning': 'This chunk is a release note for TiDB 6.5.11 focusing on bug fixes and improvements, and does not address down-peer metric behavior or replica migration.'}\n",
      "INFO:graph.chunk_filter:Filter Eval Result {'chunk_id': '17b348762b7e4f36be27698c3f9d0f3d', 'is_relevant': False, 'confidence': 1.0, 'reasoning': 'This chunk provides an overview of the TiDB Data Migration tool (DM), covering its features and usage, without relating to down-peer metric behavior after replica migration.'}\n",
      "INFO:graph.chunk_filter:Filter Eval Result {'chunk_id': '214ab45f1be141f5b5dd641b1be13a57', 'is_relevant': False, 'confidence': 1.0, 'reasoning': 'This chunk details TiDB 7.5.4 release notes with various bug fixes, but it does not mention or analyze down-peer metrics or behavior after replica migration.'}\n",
      "INFO:graph.chunk_filter:Filter Eval Result {'chunk_id': 'f534c65eb2954af69d917fd8ff1db9ac', 'is_relevant': False, 'confidence': 1.0, 'reasoning': 'This chunk describes the process of migrating small datasets from MySQL to TiDB using DM. It does not include information regarding down-peer metrics or the behavior following replica migration.'}\n",
      "INFO:graph.chunk_filter:Filter chunks ['40bde88511df4ce1842260d5b36d43f3', '9ccee47620124aca895b7660d3d4260a', '8d87b7d40e124ce28b11515d202eb6ac', '82010e1edfdf43cc91a71fdeb86b4d64']\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:graph.chunk_filter:Filter Eval Result {'chunk_id': '40bde88511df4ce1842260d5b36d43f3', 'is_relevant': False, 'confidence': 0.95, 'reasoning': 'This chunk discusses data insertion techniques and tools (e.g., Dumpling, TiDB Lightning) for bulk-insert in TiDB, but it does not mention down-peer metrics or replica migration behavior.'}\n",
      "INFO:graph.chunk_filter:Filter Eval Result {'chunk_id': '9ccee47620124aca895b7660d3d4260a', 'is_relevant': False, 'confidence': 0.95, 'reasoning': 'This chunk explains steps for using DM for incremental replication and configuring data sources; it does not address down-peer metric behavior or changes after replica migration.'}\n",
      "INFO:graph.chunk_filter:Filter Eval Result {'chunk_id': '8d87b7d40e124ce28b11515d202eb6ac', 'is_relevant': False, 'confidence': 0.9, 'reasoning': 'The content here is release notes for TiDB 6.5.6 that list various bug fixes. There is no mention of down-peer metrics or replica migration behavior.'}\n",
      "INFO:graph.chunk_filter:Filter Eval Result {'chunk_id': '82010e1edfdf43cc91a71fdeb86b4d64', 'is_relevant': False, 'confidence': 0.95, 'reasoning': 'This chunk describes data checking in sharded scenarios using sync-diff-inspector and does not cover down-peer metrics or changes after replica migration.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SearchAction(tool=retrieve_documents, query=How does TiDB handle node failures and update down-peer counts?)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:graph.chunk_filter:Filter chunks ['cdfe5a33d5f6418dab8133d3f08b746b', '9c2dcf083bf7478c9eec150497da5c78', '308096fb1d284787bd62233bbf631960', '83a68645994d43dea55b680c0cf32148', '333969ccdc41465ba39a766f4495f06c']\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:graph.chunk_filter:Filter Eval Result {'chunk_id': 'cdfe5a33d5f6418dab8133d3f08b746b', 'is_relevant': False, 'confidence': 0.7, 'reasoning': 'This chunk lists various bug fixes in TiDB 6.5.4, including handling scenarios when a TiFlash node is down, but it does not mention how TiDB updates down-peer counts or details on overall node failure handling relevant to the query.'}\n",
      "INFO:graph.chunk_filter:Filter Eval Result {'chunk_id': '9c2dcf083bf7478c9eec150497da5c78', 'is_relevant': True, 'confidence': 0.9, 'reasoning': 'This chunk explicitly mentions a fix for the execution of `replace-down-peer`, a command that is directly related to handling down peers (and thus updating down-peer counts) during node failures, making it directly relevant.'}\n",
      "INFO:graph.chunk_filter:Filter Eval Result {'chunk_id': '308096fb1d284787bd62233bbf631960', 'is_relevant': False, 'confidence': 0.75, 'reasoning': 'While this chunk discusses improvements like avoiding sending requests to unhealthy TiKV nodes (i.e. handling node failures), it does not provide any details on how TiDB updates down-peer counts.'}\n",
      "INFO:graph.chunk_filter:Filter Eval Result {'chunk_id': '83a68645994d43dea55b680c0cf32148', 'is_relevant': False, 'confidence': 0.75, 'reasoning': 'This chunk focuses on fault recovery enhancements (e.g., for disk failures and stuck I/O) and overall stability improvements, but it does not detail the process for updating down-peer counts in the event of node failures.'}\n",
      "INFO:graph.chunk_filter:Filter Eval Result {'chunk_id': '333969ccdc41465ba39a766f4495f06c', 'is_relevant': False, 'confidence': 0.75, 'reasoning': 'This chunk includes improvements regarding node failure recovery, such as reducing QPS recovery duration when a TiKV server is in recovery, but it does not mention or explain how down-peer counts are updated.'}\n",
      "INFO:graph.chunk_filter:Filter chunks ['0e718effa59f48f78cd3d43302ab0968', 'cbba5871312b447d8f0fc2fa3ff92a9c', '658d86fbbb2442fbbe6794f5e25ba964', '8360922779c847d49ed7f7a9df07455f', 'fde3e4da2b514a23985c3123eab73a47']\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:graph.chunk_filter:Filter Eval Result {'chunk_id': '0e718effa59f48f78cd3d43302ab0968', 'is_relevant': False, 'confidence': 0.9, 'reasoning': 'This chunk is about high availability configuration for a TiDB cluster on Kubernetes using pod scheduling techniques. It does not mention handling node failures in the context of down-peer counts.'}\n",
      "INFO:graph.chunk_filter:Filter Eval Result {'chunk_id': 'cbba5871312b447d8f0fc2fa3ff92a9c', 'is_relevant': False, 'confidence': 0.95, 'reasoning': 'This chunk is a Sysbench performance test report comparing different versions and does not discuss node failures or the mechanism for updating down-peer counts.'}\n",
      "INFO:graph.chunk_filter:Filter Eval Result {'chunk_id': '658d86fbbb2442fbbe6794f5e25ba964', 'is_relevant': True, 'confidence': 0.85, 'reasoning': \"This chunk, found in the 'Troubleshoot a TiFlash Cluster' document, directly mentions down peers and includes instructions to check for them using 'pd-ctl region check-down-peer'. This discussion of down peers is directly related to how node failures (or issues leading to down peers) might be handled.\"}\n",
      "INFO:graph.chunk_filter:Filter Eval Result {'chunk_id': '8360922779c847d49ed7f7a9df07455f', 'is_relevant': False, 'confidence': 0.8, 'reasoning': 'Although this chunk (TiDB 6.2.0 Release Notes) mentions a node failure scenario (a PD node going down), it does not provide specific details on how node failures are handled or how down-peer counts are updated.'}\n",
      "INFO:graph.chunk_filter:Filter Eval Result {'chunk_id': 'fde3e4da2b514a23985c3123eab73a47', 'is_relevant': False, 'confidence': 0.85, 'reasoning': 'This chunk is from the TiDB 4.0.16 Release Notes and covers various bug fixes unrelated to handling node failures or updating down-peer counts.'}\n",
      "INFO:graph.chunk_filter:Filter chunks ['97b85f20d8d9479481a6afd936535673', '29948f699cae49b6a71397d8011ef1ea', '189f2edb7dc046bfac924d4bf0f2b009', '0400ec0d320a4202b2479f4fbea90d35', '833e66ab4d07409ca040d31c115e9158']\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:graph.chunk_filter:Filter Eval Result {'chunk_id': '97b85f20d8d9479481a6afd936535673', 'is_relevant': False, 'confidence': 0.9, 'reasoning': 'This chunk is a release note for TiDB 7.1.4 that lists various bug fixes. It does not mention node failures or handling/updating down-peer counts, nor does it discuss the specific internals related to node failure management.'}\n",
      "INFO:graph.chunk_filter:Filter Eval Result {'chunk_id': '29948f699cae49b6a71397d8011ef1ea', 'is_relevant': False, 'confidence': 0.9, 'reasoning': 'This chunk contains documentation for upgrading TiDB using TiUP, which is unrelated to node failures or down-peer count updates.'}\n",
      "INFO:graph.chunk_filter:Filter Eval Result {'chunk_id': '189f2edb7dc046bfac924d4bf0f2b009', 'is_relevant': False, 'confidence': 0.9, 'reasoning': 'This chunk is the TiDB 3.0.11 Release Notes and covers compatibility, features, and bug fixes, with no mention of node failure management or down-peer count updates.'}\n",
      "INFO:graph.chunk_filter:Filter Eval Result {'chunk_id': '0400ec0d320a4202b2479f4fbea90d35', 'is_relevant': False, 'confidence': 0.9, 'reasoning': 'This chunk is TiDB 7.5.2 Release Notes that list various bug fixes. It does not address how node failures are handled or the update mechanism for down-peer counts.'}\n",
      "INFO:graph.chunk_filter:Filter Eval Result {'chunk_id': '833e66ab4d07409ca040d31c115e9158', 'is_relevant': False, 'confidence': 0.8, 'reasoning': 'This chunk provides general architecture FAQs for TiDB, focusing on high-level techniques. It does not provide specific details on how TiDB handles node failures or updates down-peer counts.'}\n",
      "INFO:graph.chunk_filter:Filter chunks ['df4d6c5cd38847d1872117d102028935', '0cb569c05bfd4bd3b19701861661c6f2', 'eb83a0c2d1ba4db9a55ffce248af2b62', 'fe754c86be64472abd16cbd4a5889053', '7e9db6f31ec441f583b94c0d2959cf6e']\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:graph.chunk_filter:Filter Eval Result {'chunk_id': 'df4d6c5cd38847d1872117d102028935', 'is_relevant': False, 'confidence': 1.0, 'reasoning': 'This chunk describes determining the TiDB cluster size and configuration of resources (node count, vCPU, RAM) and does not mention node failures or down-peer counts.'}\n",
      "INFO:graph.chunk_filter:Filter Eval Result {'chunk_id': '0cb569c05bfd4bd3b19701861661c6f2', 'is_relevant': False, 'confidence': 1.0, 'reasoning': 'This release notes chunk lists bug fixes related to SQL execution, prepared statements, and various other issues, but does not address node failures or how down-peer counts are updated.'}\n",
      "INFO:graph.chunk_filter:Filter Eval Result {'chunk_id': 'eb83a0c2d1ba4db9a55ffce248af2b62', 'is_relevant': False, 'confidence': 1.0, 'reasoning': 'This chunk provides sample code and instructions for connecting to TiDB with GORM; it does not relate to node failures or down-peer count updates.'}\n",
      "INFO:graph.chunk_filter:Filter Eval Result {'chunk_id': 'fe754c86be64472abd16cbd4a5889053', 'is_relevant': False, 'confidence': 1.0, 'reasoning': 'This release notes chunk focuses on bug fixes in TiDB 7.6.0 records, with no details concerning how node failures are handled or down-peer counts updated.'}\n",
      "INFO:graph.chunk_filter:Filter Eval Result {'chunk_id': '7e9db6f31ec441f583b94c0d2959cf6e', 'is_relevant': False, 'confidence': 1.0, 'reasoning': 'This chunk shows a table of TiDB features and advanced SQL capabilities, without any information on handling node failures or updating down-peer counts.'}\n",
      "INFO:graph.chunk_filter:Filter chunks ['a078cfd1abea425aa3964a693fab1c9b', 'f8c64a4086af4b578e85ee1ef49e5712']\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:graph.chunk_filter:Filter Eval Result {'chunk_id': 'a078cfd1abea425aa3964a693fab1c9b', 'is_relevant': False, 'confidence': 0.9, 'reasoning': 'The text is about replacing and upgrading nodes in a TiDB cluster on cloud disks and does not discuss handling node failures or down-peer counts.'}\n",
      "INFO:graph.chunk_filter:Filter Eval Result {'chunk_id': 'f8c64a4086af4b578e85ee1ef49e5712', 'is_relevant': False, 'confidence': 0.9, 'reasoning': 'The text focuses on TiDB features related to security and data import/export without mentioning node failures or updating down-peer counts.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SearchAction(tool=retrieve_documents, query=TiDB replica migration mechanism and metric normalization)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:graph.chunk_filter:Filter chunks ['40bde88511df4ce1842260d5b36d43f3', '657b346a50934f27aa4f687aca9192dc', '787ae113a4e64cb1a619498335936bbf', '96c1b86a8eec4d80b563ee56e9519ddf', '6c28efc2b22a46358d851e9ba8a3b3ef']\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:graph.chunk_filter:Filter Eval Result {'chunk_id': '40bde88511df4ce1842260d5b36d43f3', 'is_relevant': False, 'confidence': 0.9, 'reasoning': 'This chunk focuses on bulk-insert techniques, data import, and auto-random primary key handling. It only briefly mentions data migration tools (e.g. TiDB Data Migration) without discussing replica migration mechanisms or metric normalization.'}\n",
      "INFO:graph.chunk_filter:Filter Eval Result {'chunk_id': '657b346a50934f27aa4f687aca9192dc', 'is_relevant': False, 'confidence': 0.85, 'reasoning': 'This release notes chunk lists bug fixes for various components including TiDB Data Migration but does not detail the replica migration mechanism or metric normalization aspects requested in the query.'}\n",
      "INFO:graph.chunk_filter:Filter Eval Result {'chunk_id': '787ae113a4e64cb1a619498335936bbf', 'is_relevant': False, 'confidence': 0.9, 'reasoning': 'This chunk is nearly identical to the first one, covering bulk data insertion methods and related topics. It does not contain specific information on the replica migration mechanism or metric normalization.'}\n",
      "INFO:graph.chunk_filter:Filter Eval Result {'chunk_id': '96c1b86a8eec4d80b563ee56e9519ddf', 'is_relevant': True, 'confidence': 0.7, 'reasoning': \"This document explains the daily check procedures for TiDB Data Migration, including how to view DM monitoring metrics in Grafana. While it does not explicitly detail the replica migration mechanism or metric 'normalization', it is the only chunk that references monitoring metrics related to migration, making it partially relevant.\"}\n",
      "INFO:graph.chunk_filter:Filter Eval Result {'chunk_id': '6c28efc2b22a46358d851e9ba8a3b3ef', 'is_relevant': False, 'confidence': 0.85, 'reasoning': 'This release notes chunk describes bug fixes in TiDB and DM components but does not include any specific discussion on the replica migration mechanism or on the normalization of metrics.'}\n",
      "INFO:graph.chunk_filter:Filter chunks ['26bb80f928004edcacc754111bcb67f9', '82010e1edfdf43cc91a71fdeb86b4d64', '44cab24cdabf484ba72b85ce943cca91', 'f534c65eb2954af69d917fd8ff1db9ac', 'aa5417b0cd10420399c6ddf6b5bbb80c']\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:graph.chunk_filter:Filter Eval Result {'chunk_id': '26bb80f928004edcacc754111bcb67f9', 'is_relevant': False, 'confidence': 0.9, 'reasoning': 'The TiDB 6.1.3 release notes only list bug fixes and do not mention any details regarding replica migration mechanisms or metric normalization.'}\n",
      "INFO:graph.chunk_filter:Filter Eval Result {'chunk_id': '82010e1edfdf43cc91a71fdeb86b4d64', 'is_relevant': False, 'confidence': 0.85, 'reasoning': \"This document discusses data checking in a sharding scenario (using sync-diff-inspector) with configuration examples, but it does not cover topics on TiDB's replica migration mechanism or metric normalization.\"}\n",
      "INFO:graph.chunk_filter:Filter Eval Result {'chunk_id': '44cab24cdabf484ba72b85ce943cca91', 'is_relevant': False, 'confidence': 0.9, 'reasoning': 'The TiDB 6.1.2 release notes focus on bug fixes without any mention of replica migration mechanisms or metric normalization.'}\n",
      "INFO:graph.chunk_filter:Filter Eval Result {'chunk_id': 'f534c65eb2954af69d917fd8ff1db9ac', 'is_relevant': False, 'confidence': 0.85, 'reasoning': 'Although this document explains how to migrate small datasets from MySQL to TiDB using DM, it does not specifically address a replica migration mechanism or provide details on metric normalization.'}\n",
      "INFO:graph.chunk_filter:Filter Eval Result {'chunk_id': 'aa5417b0cd10420399c6ddf6b5bbb80c', 'is_relevant': False, 'confidence': 0.9, 'reasoning': 'The TiDB 7.4.0 release notes cover various bug fixes and improvements, including some DM issues, but they do not discuss the replica migration mechanism nor address metric normalization.'}\n",
      "INFO:graph.chunk_filter:Filter chunks ['0911da0fe22a4690b4acf9bb896cad3b', 'd254897aa66f4bf2beb12fe65abe9f2b', '17b348762b7e4f36be27698c3f9d0f3d', '7d9ec6766af442c4a9caf13707ad492a', 'e7a5ca1f1c94440dbd1b4eeaf7c9c108']\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:graph.chunk_filter:Filter Eval Result {'chunk_id': '0911da0fe22a4690b4acf9bb896cad3b', 'is_relevant': False, 'confidence': 0.8, 'reasoning': 'This chunk focuses on MySQL compatibility and lists replication tools (TiCDC and DM) for TiDB, but does not discuss the internal replica migration mechanism or metric normalization.'}\n",
      "INFO:graph.chunk_filter:Filter Eval Result {'chunk_id': 'd254897aa66f4bf2beb12fe65abe9f2b', 'is_relevant': False, 'confidence': 0.9, 'reasoning': 'This chunk consists of TiDB 6.1.0 release notes covering bug fixes. It does not address replica migration mechanisms or metric normalization.'}\n",
      "INFO:graph.chunk_filter:Filter Eval Result {'chunk_id': '17b348762b7e4f36be27698c3f9d0f3d', 'is_relevant': False, 'confidence': 0.8, 'reasoning': 'This chunk provides an overview of the TiDB Data Migration (DM) tool, which handles data migration from MySQL-compatible databases to TiDB, but it does not discuss the replica migration mechanism internally used by TiDB or any details on metric normalization.'}\n",
      "INFO:graph.chunk_filter:Filter Eval Result {'chunk_id': '7d9ec6766af442c4a9caf13707ad492a', 'is_relevant': False, 'confidence': 0.9, 'reasoning': 'This chunk explains how to migrate data from one TiDB cluster to another, covering environment setup and procedural steps. It does not provide information on replica migration mechanisms or metric normalization.'}\n",
      "INFO:graph.chunk_filter:Filter Eval Result {'chunk_id': 'e7a5ca1f1c94440dbd1b4eeaf7c9c108', 'is_relevant': False, 'confidence': 0.9, 'reasoning': 'This chunk describes how to migrate and merge MySQL shards into TiDB Cloud, including handling auto-increment conflicts, but it does not mention the replica migration mechanism or metric normalization.'}\n",
      "INFO:graph.chunk_filter:Filter chunks ['9ccee47620124aca895b7660d3d4260a', 'aad2c63612f94b75b957aac0bb351069', '380f1bd7e2544467b2ec33c28b92ed40', 'f05440dd3ae04291ae1a1fb7646fc6fb', 'bfe13c052dfd492594ff07d7985205e7']\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:graph.chunk_filter:Filter Eval Result {'chunk_id': '9ccee47620124aca895b7660d3d4260a', 'is_relevant': False, 'confidence': 0.9, 'reasoning': 'This chunk explains how to set up DM for incremental replication from MySQL to TiDB. It does not mention any replica migration mechanism details or metric normalization.'}\n",
      "INFO:graph.chunk_filter:Filter Eval Result {'chunk_id': 'aad2c63612f94b75b957aac0bb351069', 'is_relevant': False, 'confidence': 0.9, 'reasoning': 'This chunk focuses on managing internal table schemas during DM migration. It does not address replica migration mechanisms or metric normalization.'}\n",
      "INFO:graph.chunk_filter:Filter Eval Result {'chunk_id': '380f1bd7e2544467b2ec33c28b92ed40', 'is_relevant': False, 'confidence': 0.9, 'reasoning': 'This chunk covers DM best practices and performance limitations, but it does not include details on replica migration mechanisms or how metrics are normalized.'}\n",
      "INFO:graph.chunk_filter:Filter Eval Result {'chunk_id': 'f05440dd3ae04291ae1a1fb7646fc6fb', 'is_relevant': False, 'confidence': 0.9, 'reasoning': 'This FAQ document deals with various DM-related questions and error handling. It does not mention replica migration mechanisms or metric normalization.'}\n",
      "INFO:graph.chunk_filter:Filter Eval Result {'chunk_id': 'bfe13c052dfd492594ff07d7985205e7', 'is_relevant': False, 'confidence': 0.9, 'reasoning': 'This chunk is a troubleshooting map for TiDB Data Migration covering issues like connection errors and relay log problems. It does not provide information on replica migration or metric normalization.'}\n",
      "INFO:graph.chunk_filter:Filter chunks ['34cf7fad4a7747aeb1dba70f006b58df', 'e1a739d8f5ab4ebe8402f21c1ebfba4b']\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:graph.chunk_filter:Filter Eval Result {'chunk_id': '34cf7fad4a7747aeb1dba70f006b58df', 'is_relevant': False, 'confidence': 0.9, 'reasoning': 'The chunk focuses on handling failed DDL statements in TiDB Data Migration (e.g., using the binlog command for error handling) and does not mention replica migration mechanisms or metric normalization, which are the subjects of the query.'}\n",
      "INFO:graph.chunk_filter:Filter Eval Result {'chunk_id': 'e1a739d8f5ab4ebe8402f21c1ebfba4b', 'is_relevant': False, 'confidence': 0.9, 'reasoning': 'This chunk primarily covers how to handle failed DDL statements during data migration with TiDB Data Migration tools, including the use of binlog commands and query-status. It does not address replica migration mechanisms or metric normalization.'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{15833: DocumentData(id=15833, chunks={}, content='---\\ntitle: TiDB 6.6.0 Release Notes\\nsummary: Learn about the new features, compatibility changes, improvements, and bug fixes in TiDB 6.6.0.\\n---\\n\\n# TiDB 6.6.0 Release Notes\\n\\nRelease date: February 20, 2023\\n\\nTiDB version: 6.6.0-[DMR](/releases/versioning.md#development-milestone-releases)\\n\\n> **Note:**\\n>\\n> The TiDB 6.6.0-DMR documentation has been [archived](https://docs-archive.pingcap.com/tidb/v6.6/). PingCAP encourages you to use [the latest LTS version](https://docs.pingcap.com/tidb/stable) of the TiDB database.\\n\\nQuick access: [Quick start](https://docs.pingcap.com/tidb/v6.6/quick-start-with-tidb)\\n\\nIn v6.6.0-DMR, the key new features and improvements are as follows:\\n\\n<table>\\n<thead>\\n  <tr>\\n    <th>Category</th>\\n    <th>Feature</th>\\n    <th>Description</th>\\n  </tr>\\n</thead>\\n<tbody>\\n  <tr>\\n    <td rowspan=\"3\">Scalability and Performance<br /></td>\\n    <td>TiKV supports <a href=\"https://docs.pingcap.com/tidb/v6.6/partitioned-raft-kv\" target=\"_blank\">Partitioned Raft KV storage engine</a> (experimental)</td>\\n    <td>TiKV introduces the Partitioned Raft KV storage engine, and each Region uses an independent RocksDB instance, which can easily expand the storage capacity of the cluster from TB to PB and provide more stable write latency and stronger scalability.</td>\\n  </tr>\\n  <tr>\\n    <td>TiKV supports <a href=\"https://docs.pingcap.com/tidb/v6.6/system-variables#tidb_store_batch_size\" target=\"_blank\">batch aggregating data requests</a></td>\\n    <td>This enhancement significantly reduces total RPCs in TiKV batch-get operations. In situations where data is highly dispersed and the gRPC thread pool has insufficient resources, batching coprocessor requests can improve performance by more than 50%.</td>\\n  </tr>\\n  <tr>\\n    <td>TiFlash supports <a href=\"https://docs.pingcap.com/tidb/v6.6/stale-read\" target=\"_blank\">Stale Read</a> and <a href=\"https://docs.pingcap.com/tidb/v6.6/explain-mpp#mpp-version-and-exchange-data-compression\" target=\"_blank\">compression exchange</a></td>\\n    <td>TiFlash supports the stale read feature, which can improve query performance in scenarios where real-time requirements are not restricted. TiFlash supports data compression to improve the efficiency of parallel data exchange, and the overall TPC-H performance improves by 10%, which can save more than 50% of the network usage.</td>\\n  </tr>\\n  <tr>\\n    <td rowspan=\"2\">Reliability and availability<br /></td>\\n    <td><a href=\"https://docs.pingcap.com/tidb/v6.6/tidb-resource-control\" target=\"_blank\">Resource control</a> (experimental)</td>\\n    <td>Support resource management based on resource groups, which maps database users to the corresponding resource groups and sets quotas for each resource group based on actual needs.</td>\\n  </tr>\\n  <tr>\\n    <td><a href=\"https://docs.pingcap.com/tidb/v6.6/sql-plan-management#create-a-binding-according-to-a-historical-execution-plan\" target=\"_blank\">Historical SQL binding</a></td>\\n    <td>Support binding historical execution plans and quickly binding execution plans on TiDB Dashboard.</td>\\n  </tr>\\n  <tr>\\n    <td rowspan=\"2\">SQL functionalities<br /></td>\\n    <td><a href=\"https://docs.pingcap.com/tidb/v6.6/foreign-key\" target=\"_blank\">Foreign key</a> (experimental)</td>\\n    <td>Support MySQL-compatible foreign key constraints to maintain data consistency and improve data quality.</td>\\n  </tr>\\n  <tr>\\n    <td><a href=\"https://docs.pingcap.com/tidb/v6.6/sql-statement-create-index#multi-valued-indexes\" target=\"_blank\">Multi-valued indexes</a> (experimental)</td>\\n    <td>Introduce MySQL-compatible multi-valued indexes and enhance the JSON type to improve TiDB\\'s compatibility with MySQL 8.0.</td>\\n  </tr>\\n  <tr>\\n    <td>DB operations and observability<br /></td>\\n    <td><a href=\"https://docs.pingcap.com/tidb/v6.6/dm-precheck#check-items-for-physical-import\" target=\"_blank\">DM supports physical import</a> (experimental)</td>\\n    <td>TiDB Data Migration (DM) integrates TiDB Lightning\\'s physical import mode to improve the performance of full data migration, with performance being up to 10 times faster.</td>\\n  </tr>\\n</tbody>\\n</table>\\n\\n## Feature details\\n\\n### Scalability\\n\\n* Support Partitioned Raft KV storage engine (experimental) [#11515](https://github.com/tikv/tikv/issues/11515) [#12842](https://github.com/tikv/tikv/issues/12842) @[busyjay](https://github.com/busyjay) @[tonyxuqqi](https://github.com/tonyxuqqi) @[tabokie](https://github.com/tabokie) @[bufferflies](https://github.com/bufferflies) @[5kbpers](https://github.com/5kbpers) @[SpadeA-Tang](https://github.com/SpadeA-Tang) @[nolouch](https://github.com/nolouch)\\n\\n    Before TiDB v6.6.0, TiKV\\'s Raft-based storage engine used a single RocksDB instance to store the data of all \\'Regions\\' of the TiKV instance. To support larger clusters more stably, starting from TiDB v6.6.0, a new TiKV storage engine is introduced, which uses multiple RocksDB instances to store TiKV Region data, and the data of each Region is independently stored in a separate RocksDB instance. The new engine can better control the number and level of files in the RocksDB instance, achieve physical isolation of data operations between Regions, and support stably managing more data. You can see it as TiKV managing multiple RocksDB instances through partitioning, which is why the feature is named Partitioned-Raft-KV. The main advantage of this feature is better write performance, faster scaling, and larger volume of data supported with the same hardware. It can also support larger cluster scales.\\n\\n    Currently, this feature is experimental and not recommended for use in production environments.\\n\\n    For more information, see [documentation](/partitioned-raft-kv.md).\\n\\n* Support the distributed parallel execution framework for DDL operations (experimental) [#37125](https://github.com/pingcap/tidb/issues/37125) @[zimulala](https://github.com/zimulala)\\n\\n    In previous versions, only one TiDB instance in the entire TiDB cluster was allowed to handle schema change tasks as a DDL owner. To further improve DDL concurrency for large table\\'s DDL operations, TiDB v6.6.0 introduces the distributed parallel execution framework for DDL, through which all TiDB instances in the cluster can concurrently execute the `StateWriteReorganization` phase of the same task to speed up DDL execution. This feature is controlled by the system variable [`tidb_ddl_distribute_reorg`](https://docs.pingcap.com/tidb/v6.6/system-variables#tidb_ddl_distribute_reorg-new-in-v660) and is currently only supported for `Add Index` operations.\\n\\n### Performance\\n\\n* Support a stable wake-up model for pessimistic lock queues [#13298](https://github.com/tikv/tikv/issues/13298) @[MyonKeminta](https://github.com/MyonKeminta)\\n\\n    If an application encounters frequent single-point pessimistic lock conflicts, the existing wake-up mechanism cannot guarantee the time for transactions to acquire locks, which causes high long-tail latency and even lock acquisition timeout. Starting from v6.6.0, you can enable a stable wake-up model for pessimistic locks by setting the value of the system variable [`tidb_pessimistic_txn_aggressive_locking`](https://docs.pingcap.com/tidb/v6.6/system-variables#tidb_pessimistic_txn_aggressive_locking-new-in-v660) to `ON`. In this wake-up model, the wake-up sequence of a queue can be strictly controlled to avoid the waste of resources caused by invalid wake-ups. In scenarios with serious lock conflicts, the stable wake-up model can reduce long-tail latency and the P99 response time.\\n\\n    Tests indicate this reduces tail latency 40-60%.\\n\\n    For more information, see [documentation](https://docs.pingcap.com/tidb/v6.6/system-variables#tidb_pessimistic_txn_aggressive_locking-new-in-v660).\\n\\n* Batch aggregate data requests [#39361](https://github.com/pingcap/tidb/issues/39361) @[cfzjywxk](https://github.com/cfzjywxk) @[you06](https://github.com/you06)\\n\\n    When TiDB sends a data request to TiKV, TiDB compiles the request into different sub-tasks according to the Region where the data is located, and each sub-task only processes the request of a single Region. When the data to be accessed is highly dispersed, even if the size of the data is not large, many sub-tasks will be generated, which in turn will generate many RPC requests and consume extra time. Starting from v6.6.0, TiDB supports partially merging data requests that are sent to the same TiKV instance, which reduces the number of sub-tasks and the overhead of RPC requests. In the case of high data dispersion and insufficient gRPC thread pool resources, batching requests can improve performance by more than 50%.\\n\\n    This feature is enabled by default. You can set the batch size of requests using the system variable [`tidb_store_batch_size`](/system-variables.md#tidb_store_batch_size).\\n\\n* Remove the limit on `LIMIT` clauses [#40219](https://github.com/pingcap/tidb/issues/40219) @[fzzf678](https://github.com/fzzf678)\\n\\n    Starting from v6.6.0, TiDB plan cache supports caching execution plans with a variable as the `LIMIT` parameter, such as `LIMIT ?` or `LIMIT 10, ?`. This feature allows more SQL statements to benefit from plan cache, thus improving execution efficiency. Currently, for security considerations, TiDB can only cache execution plans with `?` not greater than 10000.\\n\\n    For more information, see [documentation](/sql-prepared-plan-cache.md).\\n\\n* TiFlash supports data exchange with compression [#6620](https://github.com/pingcap/tiflash/issues/6620) @[solotzg](https://github.com/solotzg)\\n\\n    To cooperate with multiple nodes for computing, the TiFlash engine needs to exchange data among different nodes. When the size of the data to be exchanged is very large, the performance of data exchange might affect the overall computing efficiency. In v6.6.0, the TiFlash engine introduces a compression mechanism to compress the data that needs to be exchanged when necessary, and then to perform the exchange, thereby improving the efficiency of data exchange.\\n\\n    For more information, see [documentation](/explain-mpp.md#mpp-version-and-exchange-data-compression).\\n\\n* TiFlash supports the Stale Read feature [#4483](https://github.com/pingcap/tiflash/issues/4483) @[hehechen](https://github.com/hehechen)\\n\\n   The Stale Read feature has been generally available (GA) since v5.1.1, which allows you to read historical data at a specific timestamp or within a specified time range. Stale read can reduce read latency and improve query performance by reading data from local TiKV replicas directly. Before v6.6.0, TiFlash does not support Stale Read. Even if a table has TiFlash replicas, Stale Read can only read its TiKV replicas.\\n\\n   Starting from v6.6.0, TiFlash supports the Stale Read feature. When you query the historical data of a table using the [`AS OF TIMESTAMP`](/as-of-timestamp.md) syntax or the [`tidb_read_staleness`](/tidb-read-staleness.md) system variable, if the table has a TiFlash replica, the optimizer now can choose to read the corresponding data from the TiFlash replica, thus further improving query performance.\\n\\n    For more information, see [documentation](/stale-read.md).\\n\\n* Support pushing down the `regexp_replace` string function to TiFlash [#6115](https://github.com/pingcap/tiflash/issues/6115) @[xzhangxian1008](https://github.com/xzhangxian1008)\\n\\n### Reliability\\n\\n* Support resource control based on resource groups (experimental) [#38825](https://github.com/pingcap/tidb/issues/38825) @[nolouch](https://github.com/nolouch) @[BornChanger](https://github.com/BornChanger) @[glorv](https://github.com/glorv) @[tiancaiamao](https://github.com/tiancaiamao) @[Connor1996](https://github.com/Connor1996) @[JmPotato](https://github.com/JmPotato) @[hnes](https://github.com/hnes) @[CabinfeverB](https://github.com/CabinfeverB) @[HuSharp](https://github.com/HuSharp)\\n\\n    Now you can create resource groups for a TiDB cluster, bind different database users to corresponding resource groups, and set quotas for each resource group according to actual needs. When the cluster resources are limited, all resources used by sessions in the same resource group will be limited to the quota. In this way, even if a resource group is over-consumed, the sessions in other resource groups are not affected. TiDB provides a built-in view of the actual usage of resources on Grafana dashboards, assisting you to allocate resources more rationally.\\n\\n    The introduction of the resource control feature is a milestone for TiDB. It can divide a distributed database cluster into multiple logical units. Even if an individual unit overuses resources, it does not crowd out the resources needed by other units.\\n\\n    With this feature, you can:\\n\\n    - Combine multiple small and medium-sized applications from different systems into a single TiDB cluster. When the workload of an application grows larger, it does not affect the normal operation of other applications. When the system workload is low, busy applications can still be allocated the required system resources even if they exceed the set read and write quotas, so as to achieve the maximum utilization of resources.\\n    - Choose to combine all test environments into a single TiDB cluster, or group the batch tasks that consume more resources into a single resource group. It can improve hardware utilization and reduce operating costs while ensuring that critical applications can always get the necessary resources.\\n\\n  In addition, the rational use of the resource control feature can reduce the number of clusters, ease the difficulty of operation and maintenance, and save management costs.\\n\\n  In v6.6, you need to enable both TiDB\\'s global variable [`tidb_enable_resource_control`](/system-variables.md#tidb_enable_resource_control-new-in-v660) and the TiKV configuration item [`resource-control.enabled`](/tikv-configuration-file.md#resource-control) to enable resource control. Currently, the supported quota method is based on \"[Request Unit (RU)](/tidb-resource-control.md#what-is-request-unit-ru)\". RU is TiDB\\'s unified abstraction unit for system resources such as CPU and IO.\\n\\n  For more information, see [documentation](/tidb-resource-control.md).\\n\\n* Binding historical execution plans is GA [#39199](https://github.com/pingcap/tidb/issues/39199) @[fzzf678](https://github.com/fzzf678)\\n\\n    In v6.5.0, TiDB extends the binding targets in the [`CREATE [GLOBAL | SESSION] BINDING`](/sql-statements/sql-statement-create-binding.md) statements and supports creating bindings according to historical execution plans. In v6.6.0, this feature is GA. The selection of execution plans is not limited to the current TiDB node. Any historical execution plan generated by any TiDB node can be selected as the target of [SQL binding](/sql-statements/sql-statement-create-binding.md), which further improves the feature usability.\\n\\n    For more information, see [documentation](/sql-plan-management.md#create-a-binding-according-to-a-historical-execution-plan).\\n\\n* Add several optimizer hints [#39964](https://github.com/pingcap/tidb/issues/39964) @[Reminiscent](https://github.com/Reminiscent)\\n\\n    TiDB adds several optimizer hints in v6.6.0 to control the execution plan selection of `LIMIT` operations.\\n\\n    - [`ORDER_INDEX()`](/optimizer-hints.md#order_indext1_name-idx1_name--idx2_name-): tells the optimizer to use the specified index, to keep the order of the index when reading data, and generates plans similar to `Limit + IndexScan(keep order: true)`.\\n    - [`NO_ORDER_INDEX()`](/optimizer-hints.md#no_order_indext1_name-idx1_name--idx2_name-): tells the optimizer to use the specified index, not to keep the order of the index when reading data, and generates plans similar to `TopN + IndexScan(keep order: false)`.\\n\\n  Continuously introducing optimizer hints provides users with more intervention methods, helps solve SQL performance issues, and improves the stability of overall performance.\\n\\n* Support dynamically managing the resource usage of DDL operations (experimental) [#38025](https://github.com/pingcap/tidb/issues/38025) @[hawkingrei](https://github.com/hawkingrei)\\n\\n    TiDB v6.6.0 introduces resource management for DDL operations to reduce the impact of DDL changes on online applications by automatically controlling the CPU usage of these operations. This feature is effective only after the [DDL distributed parallel execution framework](https://docs.pingcap.com/tidb/v6.6/system-variables#tidb_ddl_distribute_reorg-new-in-v660) is enabled.\\n\\n### Availability\\n\\n* Support configuring `SURVIVAL_PREFERENCE` for [placement rules in SQL](/placement-rules-in-sql.md) [#38605](https://github.com/pingcap/tidb/issues/38605) @[nolouch](https://github.com/nolouch)\\n\\n    `SURVIVAL_PREFERENCES` provides data survival preference settings to increase the disaster survivability of data. By specifying `SURVIVAL_PREFERENCE`, you can control the following:\\n\\n    - For TiDB clusters deployed across cloud regions, when a cloud region fails, the specified databases or tables can survive in another cloud region.\\n    - For TiDB clusters deployed in a single cloud region, when an availability zone fails, the specified databases or tables can survive in another availability zone.\\n\\n  For more information, see [documentation](/placement-rules-in-sql.md#specify-survival-preferences).\\n\\n* Support rolling back DDL operations via the `FLASHBACK CLUSTER TO TIMESTAMP` statement [#14045](https://github.com/tikv/tikv/issues/14045) @[Defined2014](https://github.com/Defined2014) @[JmPotato](https://github.com/JmPotato)\\n\\n    The [`FLASHBACK CLUSTER TO TIMESTAMP`](/sql-statements/sql-statement-flashback-cluster.md) statement supports restoring the entire cluster to a specified point in time within the Garbage Collection (GC) lifetime. In TiDB v6.6.0, this feature adds support for rolling back DDL operations. This can be used to quickly undo a DML or DDL misoperation on a cluster, roll back a cluster within minutes, and roll back a cluster multiple times on the timeline to determine when specific data changes occurred.\\n\\n    For more information, see [documentation](/sql-statements/sql-statement-flashback-cluster.md).\\n\\n### SQL\\n\\n* Support MySQL-compatible foreign key constraints (experimental) [#18209](https://github.com/pingcap/tidb/issues/18209) @[crazycs520](https://github.com/crazycs520)\\n\\n    TiDB v6.6.0 introduces the foreign key constraints feature, which is compatible with MySQL. This feature supports referencing within a table or between tables, constraints validation, and cascade operations. This feature helps to migrate applications to TiDB, maintain data consistency, improve data quality, and facilitate data modeling.\\n\\n    For more information, see [documentation](/foreign-key.md).\\n\\n* Support MySQL-compatible multi-valued indexes (experimental) [#39592](https://github.com/pingcap/tidb/issues/39592) @[xiongjiwei](https://github.com/xiongjiwei) @[qw4990](https://github.com/qw4990)\\n\\n    TiDB introduces MySQL-compatible multi-valued indexes in v6.6.0. Filtering the values of an array in a JSON column is a common operation, but normal indexes cannot help speed up such an operation. Creating a multi-valued index on an array can greatly improve filtering performance. If an array in the JSON column has a multi-valued index, you can use the multi-valued index to filter the retrieval conditions with `MEMBER OF()`, `JSON_CONTAINS()`, `JSON_OVERLAPS()` functions, thereby reducing much I/O consumption and improving operation speed.\\n\\n    Introducing multi-valued indexes further enhances TiDB\\'s support for the JSON data type and also improves TiDB\\'s compatibility with MySQL 8.0.\\n\\n    For more information, see [documentation](/sql-statements/sql-statement-create-index.md#multi-valued-indexes).\\n\\n### DB operations\\n\\n* Support configuring read-only storage nodes for resource-consuming tasks @[v01dstar](https://github.com/v01dstar)\\n\\n    In production environments, some read-only operations might consume a large number of resources regularly and affect the performance of the entire cluster, such as backups and large-scale data reading and analysis. TiDB v6.6.0 supports configuring read-only storage nodes for resource-consuming read-only tasks to reduce the impact on the online application. Currently, TiDB, TiSpark, and BR support reading data from read-only storage nodes. You can configure read-only storage nodes according to [steps](/best-practices/readonly-nodes.md#procedures) and specify where data is read through the system variable `tidb_replica_read`, the TiSpark configuration item `spark.tispark.replica_read`, or the br command line argument `--replica-read-label`, to ensure the stability of cluster performance.\\n\\n    For more information, see [documentation](/best-practices/readonly-nodes.md).\\n\\n* Support dynamically modifying `store-io-pool-size` [#13964](https://github.com/tikv/tikv/issues/13964) @[LykxSassinator](https://github.com/LykxSassinator)\\n\\n    The TiKV configuration item [`raftstore.store-io-pool-size`](/tikv-configuration-file.md#store-io-pool-size-new-in-v530) specifies the allowable number of threads that process Raft I/O tasks, which can be adjusted when tuning TiKV performance. Before v6.6.0, this configuration item cannot be modified dynamically. Starting from v6.6.0, you can modify this configuration without restarting the server, which means more flexible performance tuning.\\n\\n    For more information, see [documentation](/dynamic-config.md).\\n\\n* Support specifying the SQL script executed upon TiDB cluster initialization [#35624](https://github.com/pingcap/tidb/issues/35624) @[morgo](https://github.com/morgo)\\n\\n    When you start a TiDB cluster for the first time, you can specify the SQL script to be executed by configuring the command line parameter `--initialize-sql-file`. You can use this feature when you need to perform such operations as modifying the value of a system variable, creating a user, or granting privileges.\\n\\n    For more information, see [documentation](/tidb-configuration-file.md#initialize-sql-file-new-in-v660).\\n\\n* TiDB Data Migration (DM) integrates with TiDB Lightning\\'s physical import mode for up to a 10x performance boost for full migration (experimental) @[lance6716](https://github.com/lance6716)\\n\\n    In v6.6.0, DM full migration capability integrates with physical import mode of TiDB Lightning, which enables DM to improve the performance of full data migration by up to 10 times, greatly reducing the migration time in large data volume scenarios.\\n\\n    Before v6.6.0, for large data volume scenarios, you were required to configure physical import tasks in TiDB Lightning separately for fast full data migration, and then use DM for incremental data migration, which was a complex configuration. Starting from v6.6.0, you can migrate large data volumes without the need to configure TiDB Lightning tasks; one DM task can accomplish the migration.\\n\\n    For more information, see [documentation](/dm/dm-precheck.md#check-items-for-physical-import).\\n\\n* TiDB Lightning adds a new configuration parameter `\"header-schema-match\"` to address the issue of mismatched column names between the source file and the target table @[dsdashun](https://github.com/dsdashun)\\n\\n    In v6.6.0, TiDB Lightning adds a new profile parameter `\"header-schema-match\"`. The default value is `true`, which means the first row of the source CSV file is treated as the column name, and consistent with that in the target table. If the field name in the CSV table header does not match the column name of the target table, you can set this configuration to `false`. TiDB Lightning will ignore the error and continue to import the data in the order of the columns in the target table.\\n\\n    For more information, see [documentation](/tidb-lightning/tidb-lightning-configuration.md#tidb-lightning-task).\\n\\n* TiDB Lightning supports enabling compressed transfers when sending key-value pairs to TiKV [#41163](https://github.com/pingcap/tidb/issues/41163) @[sleepymole](https://github.com/sleepymole)\\n\\n    Starting from v6.6.0, TiDB Lightning supports compressing locally encoded and sorted key-value pairs for network transfer when sending them to TiKV, thus reducing the amount of data transferred over the network and lowering the network bandwidth overhead. In the earlier TiDB versions before this feature is supported, TiDB Lightning requires relatively high network bandwidth and incurs high traffic charges in case of large data volumes.\\n\\n    This feature is disabled by default. To enable it, you can set the `compress-kv-pairs` configuration item of TiDB Lightning to `\"gzip\"` or `\"gz\"`.\\n\\n    For more information, see [documentation](/tidb-lightning/tidb-lightning-configuration.md#tidb-lightning-task).\\n\\n* The TiKV-CDC tool is now GA and supports subscribing to data changes of RawKV [#48](https://github.com/tikv/migration/issues/48) @[zeminzhou](https://github.com/zeminzhou) @[haojinming](https://github.com/haojinming) @[pingyu](https://github.com/pingyu)\\n\\n    TiKV-CDC is a CDC (Change Data Capture) tool for TiKV clusters. TiKV and PD can constitute a KV database when used without TiDB, which is called RawKV. TiKV-CDC supports subscribing to data changes of RawKV and replicating them to a downstream TiKV cluster in real time, thus enabling cross-cluster replication of RawKV.\\n\\n    For more information, see [documentation](https://tikv.org/docs/latest/concepts/explore-tikv-features/cdc/cdc/).\\n\\n* TiCDC supports scaling out a single table on Kafka changefeeds and distributing the changefeed to multiple TiCDC nodes (experimental) [#7720](https://github.com/pingcap/tiflow/issues/7720) @[overvenus](https://github.com/overvenus)\\n\\n    Before v6.6.0, when a table in the upstream accepts a large amount of writes, the replication capability of this table cannot be scaled out, resulting in an increase in the replication latency. Starting from TiCDC v6.6.0. the changefeed of an upstream table can be distributed to multiple TiCDC nodes in a Kafka sink, which means the replication capability of a single table is scaled out.\\n\\n    For more information, see [documentation](/ticdc/ticdc-sink-to-kafka.md#scale-out-the-load-of-a-single-large-table-to-multiple-ticdc-nodes).\\n\\n* [GORM](https://github.com/go-gorm/gorm) adds TiDB integration tests. Now TiDB is the default database supported by GORM. [#6014](https://github.com/go-gorm/gorm/pull/6014) @[Icemap](https://github.com/Icemap)\\n\\n    - In v1.4.6, [GORM MySQL driver](https://github.com/go-gorm/mysql) adapts to the `AUTO_RANDOM` attribute of TiDB [#104](https://github.com/go-gorm/mysql/pull/104)\\n    - In v1.4.6, [GORM MySQL driver](https://github.com/go-gorm/mysql) fixes the issue that when connecting to TiDB, the `Unique` attribute of the `Unique` field cannot be modified during `AutoMigrate` [#105](https://github.com/go-gorm/mysql/pull/105)\\n    - [GORM documentation](https://github.com/go-gorm/gorm.io) mentions TiDB as the default database [#638](https://github.com/go-gorm/gorm.io/pull/638)\\n\\n    For more information, see [GORM documentation](https://gorm.io/docs/index.html).\\n\\n### Observability\\n\\n* Support quickly creating SQL binding on TiDB Dashboard [#781](https://github.com/pingcap/tidb-dashboard/issues/781) @[YiniXu9506](https://github.com/YiniXu9506)\\n\\n    TiDB v6.6.0 supports creating SQL binding from statement history, which allows you to quickly bind a SQL statement to a specific plan on TiDB Dashboard.\\n\\n    By providing a user-friendly interface, this feature simplifies the process of binding plans in TiDB, reduces the operation complexity, and improves the efficiency and user experience of the plan binding process.\\n\\n    For more information, see [documentation](/dashboard/dashboard-statement-details.md#fast-plan-binding).\\n\\n* Add warning for caching execution plans @[qw4990](https://github.com/qw4990)\\n\\n    When an execution plan cannot be cached, TiDB indicates the reason in warning to make diagnostics easier. For example:\\n\\n    ```sql\\n    mysql> PREPARE st FROM \\'SELECT * FROM t WHERE a<?\\';\\n    Query OK, 0 rows affected (0.00 sec)\\n\\n    mysql> SET @a=\\'1\\';\\n    Query OK, 0 rows affected (0.00 sec)\\n\\n    mysql> EXECUTE st USING @a;\\n    Empty set, 1 warning (0.01 sec)\\n\\n    mysql> SHOW WARNINGS;\\n    +---------+------+----------------------------------------------+\\n    | Level   | Code | Message                                      |\\n    +---------+------+----------------------------------------------+\\n    | Warning | 1105 | skip plan-cache: \\'1\\' may be converted to INT |\\n    +---------+------+----------------------------------------------+\\n    ```\\n\\n    In the preceding example, the optimizer converts a non-INT type to an INT type, and the execution plan might change with the change of the parameter, so TiDB does not cache the plan.\\n\\n    For more information, see [documentation](/sql-prepared-plan-cache.md#diagnostics-of-prepared-plan-cache).\\n\\n* Add a `Warnings` field to the slow query log [#39893](https://github.com/pingcap/tidb/issues/39893) @[time-and-fate](https://github.com/time-and-fate)\\n\\n    TiDB v6.6.0 adds a `Warnings` field to the slow query log to help diagnose performance issues. This field records warnings generated during the execution of a slow query. You can also view the warnings on the slow query page of TiDB Dashboard.\\n\\n    For more information, see [documentation](/identify-slow-queries.md).\\n\\n* Automatically capture the generation of SQL execution plans [#38779](https://github.com/pingcap/tidb/issues/38779) @[Yisaer](https://github.com/Yisaer)\\n\\n    In the process of troubleshooting execution plan issues, `PLAN REPLAYER` can help preserve the scene and improve the efficiency of diagnosis. However, in some scenarios, the generation of some execution plans cannot be reproduced freely, which makes the diagnosis work more difficult.\\n\\n    To address such issues, in TiDB v6.6.0, `PLAN REPLAYER` extends the capability of automatic capture. With the `PLAN REPLAYER CAPTURE` command, you can register the target SQL statement in advance and also specify the target execution plan at the same time. When TiDB detects the SQL statement or the execution plan that matches the registered target, it automatically generates and packages the `PLAN REPLAYER` information. When the execution plan is unstable, this feature can improve diagnostic efficiency.\\n\\n    To use this feature, set the value of [`tidb_enable_plan_replayer_capture`](/system-variables.md#tidb_enable_plan_replayer_capture) to `ON`.\\n\\n    For more information, see [documentation](/sql-plan-replayer.md#use-plan-replayer-capture).\\n\\n* Support persisting statements summary (experimental) [#40812](https://github.com/pingcap/tidb/issues/40812) @[mornyx](https://github.com/mornyx)\\n\\n    Before v6.6.0, statements summary data is kept in memory and would be lost upon a TiDB server restart. Starting from v6.6.0, TiDB supports enabling statements summary persistence, which allows historical data to be written to disks on a regular basis. In the meantime, the result of queries on system tables will derive from disks, instead of memory. After TiDB restarts, all historical data remains available.\\n\\n    For more information, see [documentation](/statement-summary-tables.md#persist-statements-summary).\\n\\n### Security\\n\\n* TiFlash supports automatic rotations of TLS certificates [#5503](https://github.com/pingcap/tiflash/issues/5503) @[ywqzzy](https://github.com/ywqzzy)\\n\\n    In v6.6.0, TiDB supports automatic rotations of TiFlash TLS certificates. For a TiDB cluster with encrypted data transmission between components enabled, when a TLS certificate of TiFlash expires and needs to be reissued with a new one, the new TiFlash TLS certificate can be automatically loaded without restarting the TiDB cluster. In addition, the rotation of a TLS certificate between components within a TiDB cluster does not affect the use of the TiDB cluster, which ensures high availability of the cluster.\\n\\n    For more information, see [documentation](/enable-tls-between-components.md).\\n\\n* TiDB Lightning supports accessing Amazon S3 data via AWS IAM role keys and session tokens [#40750](https://github.com/pingcap/tidb/issues/40750) @[okJiang](https://github.com/okJiang)\\n\\n    Before v6.6.0, TiDB Lightning only supports accessing S3 data via AWS IAM **user\\'s access keys** (each access key consists of an access key ID and a secret access key) so you cannot use a temporary session token to access S3 data. Starting from v6.6.0, TiDB Lightning supports accessing S3 data via AWS IAM **role\\'s access keys + session tokens** as well to improve the data security.\\n\\n    For more information, see [documentation](/tidb-lightning/tidb-lightning-data-source.md#import-data-from-amazon-s3).\\n\\n### Telemetry\\n\\n- Starting from February 20, 2023, the [telemetry feature](/telemetry.md) is disabled by default in new versions of TiDB and TiDB Dashboard (including v6.6.0). If you upgrade from a previous version that uses the default telemetry configuration, the telemetry feature is disabled after the upgrade. For the specific versions, see [TiDB Release Timeline](/releases/release-timeline.md).\\n- Starting from v1.11.3, the telemetry feature is disabled by default in newly deployed TiUP. If you upgrade from a previous version of TiUP to v1.11.3 or a later version, the telemetry feature keeps the same status as before the upgrade.\\n\\n## Compatibility changes\\n\\n> **Note:**\\n>\\n> This section provides compatibility changes you need to know when you upgrade from v6.5.0 to the current version (v6.6.0). If you are upgrading from v6.4.0 or earlier versions to the current version, you might also need to check the compatibility changes introduced in intermediate versions.\\n\\n### MySQL compatibility\\n\\n* Support MySQL-compatible foreign key constraints (experimental) [#18209](https://github.com/pingcap/tidb/issues/18209) @[crazycs520](https://github.com/crazycs520)\\n\\n    For more information, see the [SQL](#sql) section in this document and [documentation](/foreign-key.md).\\n\\n* Support the MySQL-compatible multi-valued indexes (experimental) [#39592](https://github.com/pingcap/tidb/issues/39592) @[xiongjiwei](https://github.com/xiongjiwei) @[qw4990](https://github.com/qw4990)\\n\\n    For more information, see the [SQL](#sql) section in this document and [documentation](/sql-statements/sql-statement-create-index.md#multi-valued-indexes).\\n\\n### System variables\\n\\n| Variable name  | Change type    | Description |\\n|--------|------------------------------|------|\\n| `tidb_enable_amend_pessimistic_txn` | Deleted | Starting from v6.5.0, this variable is deprecated. Starting from v6.6.0, this variable and the `AMEND TRANSACTION` feature are deleted. TiDB will use [meta lock](/metadata-lock.md) to avoid the `Information schema is changed` error. |\\n| `tidb_enable_concurrent_ddl` | Deleted | This variable controls whether to allow TiDB to use concurrent DDL statements. When this variable is disabled, TiDB uses the old DDL execution framework, which provides limited support for concurrent DDL execution. Starting from v6.6.0, this variable is deleted and TiDB no longer supports the old DDL execution framework. |\\n| `tidb_ttl_job_run_interval` | Deleted | This variable is used to control the scheduling interval of TTL jobs in the background. Starting from v6.6.0, this variable is deleted, because TiDB provides the `TTL_JOB_INTERVAL` attribute for every table to control the TTL runtime, which is more flexible than `tidb_ttl_job_run_interval`. |\\n| [`foreign_key_checks`](/system-variables.md#foreign_key_checks) | Modified | This variable controls whether to enable the foreign key constraint check. The default value changes from `OFF` to `ON`, which means enabling the foreign key check by default. |\\n| [`tidb_enable_foreign_key`](/system-variables.md#tidb_enable_foreign_key-new-in-v630) | Modified | This variable controls whether to enable the foreign key feature. The default value changes from `OFF` to `ON`, which means enabling foreign key by default. |\\n| `tidb_enable_general_plan_cache` | Modified | This variable controls whether to enable General Plan Cache. Starting from v6.6.0, this variable is renamed to [`tidb_enable_non_prepared_plan_cache`](/system-variables.md#tidb_enable_non_prepared_plan_cache). |\\n| [`tidb_enable_historical_stats`](/system-variables.md#tidb_enable_historical_stats) | Modified | This variable controls whether to enable historical statistics. The default value changes from `OFF` to `ON`, which means that historical statistics are enabled by default. |\\n| [`tidb_enable_telemetry`](/system-variables.md#tidb_enable_telemetry-new-in-v402-and-deprecated-in-v810) | Modified | The default value changes from `ON` to `OFF`, which means that telemetry is disabled by default in TiDB. |\\n| `tidb_general_plan_cache_size` | Modified | This variable controls the maximum number of execution plans that can be cached by General Plan Cache. Starting from v6.6.0, this variable is renamed to [`tidb_non_prepared_plan_cache_size`](/system-variables.md#tidb_non_prepared_plan_cache_size). |\\n| [`tidb_replica_read`](/system-variables.md#tidb_replica_read-new-in-v40) | Modified | A new value option `learner` is added for this variable to specify the learner replicas with which TiDB reads data from read-only nodes. |\\n| [`tidb_replica_read`](/system-variables.md#tidb_replica_read-new-in-v40) | Modified | A new value option `prefer-leader` is added for this variable to improve the overall read availability of TiDB clusters. When this option is set, TiDB prefers to read from the leader replica. When the performance of the leader replica significantly decreases, TiDB automatically reads from follower replicas. |\\n| [`tidb_store_batch_size`](/system-variables.md#tidb_store_batch_size) | Modified | This variable controls the batch size of the Coprocessor Tasks of the `IndexLookUp` operator. `0` means to disable batch. Starting from v6.6.0, the default value is changed from `0` to `4`, which means 4 Coprocessor tasks will be batched into one task for each batch of requests. |\\n| [`mpp_exchange_compression_mode`](/system-variables.md#mpp_exchange_compression_mode-new-in-v660)  | Newly added |  This variable specifies the data compression mode of the MPP Exchange operator. It takes effect when TiDB selects the MPP execution plan with the version number `1`. The default value `UNSPECIFIED` means that TiDB automatically selects the `FAST` compression mode. |\\n| [`mpp_version`](/system-variables.md#mpp_version-new-in-v660)  | Newly added |  This variable specifies the version of the MPP execution plan. After a version is specified, TiDB selects the specified version of the MPP execution plan. The default value `UNSPECIFIED` means that TiDB automatically selects the latest version `1`. |\\n| [`tidb_ddl_distribute_reorg`](https://docs.pingcap.com/tidb/v6.6/system-variables#tidb_ddl_distribute_reorg-new-in-v660) | Newly added | This variable controls whether to enable distributed execution of the DDL reorg phase to accelerate this phase. The default value `OFF` means not to enable distributed execution of the DDL reorg phase by default. Currently, this variable takes effect only for `ADD INDEX`. |\\n| [`tidb_enable_historical_stats_for_capture`](/system-variables.md#tidb_enable_historical_stats_for_capture) | Newly added | This variable controls whether the information captured by `PLAN REPLAYER CAPTURE` includes historical statistics by default. The default value `OFF` means that historical statistics are not included by default. |\\n| [`tidb_enable_plan_cache_for_param_limit`](/system-variables.md#tidb_enable_plan_cache_for_param_limit-new-in-v660) | Newly added | This variable controls whether Prepared Plan Cache caches execution plans that contain `COUNT` after `Limit`. The default value is `ON`, which means Prepared Plan Cache supports caching such execution plans. Note that Prepared Plan Cache does not support caching execution plans with a `COUNT` condition that counts a number greater than 10000. |\\n| [`tidb_enable_plan_replayer_capture`](/system-variables.md#tidb_enable_plan_replayer_capture) | Newly added | This variable controls whether to enable the [`PLAN REPLAYER CAPTURE` feature](/sql-plan-replayer.md#use-plan-replayer-capture-to-capture-target-plans). The default value `OFF` means to disable the `PLAN REPLAYER CAPTURE` feature. |\\n| [`tidb_enable_resource_control`](/system-variables.md#tidb_enable_resource_control-new-in-v660) | Newly added  | This variable controls whether to enable the resource control feature. The default value is `OFF`. When this variable is set to `ON`, the TiDB cluster supports resource isolation of applications based on resource groups. |\\n| [`tidb_historical_stats_duration`](/system-variables.md#tidb_historical_stats_duration-new-in-v660) | Newly added | This variable controls how long the historical statistics are retained in storage. The default value is 7 days. |\\n| [`tidb_index_join_double_read_penalty_cost_rate`](/system-variables.md#tidb_index_join_double_read_penalty_cost_rate-new-in-v660) | Newly added | This variable controls whether to add some penalty cost to the selection of index join. The default value `0` means that this feature is disabled by default. |\\n| [`tidb_pessimistic_txn_aggressive_locking`](https://docs.pingcap.com/tidb/v6.6/system-variables#tidb_pessimistic_txn_aggressive_locking-new-in-v660) | Newly added | This variable controls whether to use enhanced pessimistic locking wake-up model for pessimistic transactions. The default value `OFF` means not to use such a wake-up model for pessimistic transactions by default. |\\n| [`tidb_stmt_summary_enable_persistent`](/system-variables.md#tidb_stmt_summary_enable_persistent-new-in-v660) | Newly added | This variable is read-only. It controls whether to enable [statements summary persistence](/statement-summary-tables.md#persist-statements-summary). The value of this variable is the same as that of the configuration item [`tidb_stmt_summary_enable_persistent`](/tidb-configuration-file.md#tidb_stmt_summary_enable_persistent-new-in-v660). |\\n| [`tidb_stmt_summary_filename`](/system-variables.md#tidb_stmt_summary_filename-new-in-v660) | Newly added | This variable is read-only. It specifies the file to which persistent data is written when [statements summary persistence](/statement-summary-tables.md#persist-statements-summary) is enabled. The value of this variable is the same as that of the configuration item [`tidb_stmt_summary_filename`](/tidb-configuration-file.md#tidb_stmt_summary_filename-new-in-v660). |\\n| [`tidb_stmt_summary_file_max_backups`](/system-variables.md#tidb_stmt_summary_file_max_backups-new-in-v660) | Newly added | This variable is read-only. It specifies the maximum number of data files that can be persisted when [statements summary persistence](/statement-summary-tables.md#persist-statements-summary) is enabled. The value of this variable is the same as that of the configuration item [`tidb_stmt_summary_file_max_backups`](/tidb-configuration-file.md#tidb_stmt_summary_file_max_backups-new-in-v660). |\\n| [`tidb_stmt_summary_file_max_days`](/system-variables.md#tidb_stmt_summary_file_max_days-new-in-v660) | Newly added | This variable is read-only. It specifies the maximum number of days to keep persistent data files when [statements summary persistence](/statement-summary-tables.md#persist-statements-summary) is enabled. The value of this variable is the same as that of the configuration item [`tidb_stmt_summary_file_max_days`](/tidb-configuration-file.md#tidb_stmt_summary_file_max_days-new-in-v660). |\\n| [`tidb_stmt_summary_file_max_size`](/system-variables.md#tidb_stmt_summary_file_max_size-new-in-v660) | Newly added | This variable is read-only. It specifies the maximum size of a persistent data file when [statements summary persistence](/statement-summary-tables.md#persist-statements-summary) is enabled. The value of this variable is the same as that of the configuration item [`tidb_stmt_summary_file_max_size`](/tidb-configuration-file.md#tidb_stmt_summary_file_max_size-new-in-v660). |\\n\\n### Configuration file parameters\\n\\n| Configuration file | Configuration parameter | Change type | Description |\\n| -------- | -------- | -------- | -------- |\\n| TiKV  | `rocksdb.enable-statistics` | Deleted | This configuration item specifies whether to enable RocksDB statistics. Starting from v6.6.0, this item is deleted. RocksDB statistics are enabled for all clusters by default to help diagnostics. For details, see [#13942](https://github.com/tikv/tikv/pull/13942). |\\n| TiKV  | `raftdb.enable-statistics` | Deleted | This configuration item specifies whether to enable Raft RocksDB statistics. Starting from v6.6.0, this item is deleted. Raft RocksDB statistics are enabled for all clusters by default to help diagnostics. For details, see [#13942](https://github.com/tikv/tikv/pull/13942). |\\n| TiKV | `storage.block-cache.shared` | Deleted | Starting from v6.6.0, this configuration item is deleted, and the block cache is enabled by default and cannot be disabled. For details, see [#12936](https://github.com/tikv/tikv/issues/12936). |\\n| DM | `on-duplicate` |  Deleted | This configuration item controls the methods to resolve conflicts during the full import phase. In v6.6.0, new configuration items `on-duplicate-logical` and `on-duplicate-physical` are introduced to replace `on-duplicate`. |\\n| TiDB | [`enable-telemetry`](/tidb-configuration-file.md#enable-telemetry-new-in-v402-and-deprecated-in-v810) | Modified | Starting from v6.6.0, the default value changes from `true` to `false`, which means that telemetry is disabled by default in TiDB. |\\n| TiKV  | [`rocksdb.defaultcf.block-size`](/tikv-configuration-file.md#block-size) and [`rocksdb.writecf.block-size`](/tikv-configuration-file.md#block-size)  |  Modified  |   The default values change from `64K` to `32K`.  |\\n| TiKV | [`rocksdb.defaultcf.block-cache-size`](/tikv-configuration-file.md#block-cache-size), [`rocksdb.writecf.block-cache-size`](/tikv-configuration-file.md#block-cache-size), [`rocksdb.lockcf.block-cache-size`](/tikv-configuration-file.md#block-cache-size) | Deprecated | Starting from v6.6.0, these configuration items are deprecated. For details, see [#12936](https://github.com/tikv/tikv/issues/12936). |\\n| PD | [`enable-telemetry`](/pd-configuration-file.md#enable-telemetry) | Modified | Starting from v6.6.0, the default value changes from `true` to `false`, which means that telemetry is disabled by default in TiDB Dashboard. |\\n| DM | [`import-mode`](/dm/task-configuration-file-full.md) |  Modified | The possible values of this configuration item are changed from `\"sql\"` and `\"loader\"` to `\"logical\"` and `\"physical\"`. The default value is `\"logical\"`, which means using TiDB Lightning\\'s logical import mode to import data. |\\n| TiFlash |  [`profile.default.max_memory_usage_for_all_queries`](/tiflash/tiflash-configuration.md#configure-the-tiflashtoml-file)  |  Modified  |  Specifies the memory usage limit for the generated intermediate data in all queries. Starting from v6.6.0, the default value changes from `0` to `0.8`, which means the limit is 80% of the total memory. |\\n| TiCDC  | [`consistent.storage`](/ticdc/ticdc-sink-to-mysql.md#prerequisites)  |  Modified  | This configuration item specifies the path under which redo log backup is stored. Two more value options are added for `scheme`, GCS, and Azure. |\\n| TiDB | [`initialize-sql-file`](/tidb-configuration-file.md#initialize-sql-file-new-in-v660) | Newly added | This configuration item specifies the SQL script to be executed when the TiDB cluster is started for the first time. The default value is empty. |\\n| TiDB | [`tidb_stmt_summary_enable_persistent`](/tidb-configuration-file.md#tidb_stmt_summary_enable_persistent-new-in-v660) | Newly added | This configuration item controls whether to enable statements summary persistence. The default value is `false`, which means this feature is not enabled by default. |\\n| TiDB | [`tidb_stmt_summary_file_max_backups`](/tidb-configuration-file.md#tidb_stmt_summary_file_max_backups-new-in-v660) | Newly added | When statements summary persistence is enabled, this configuration specifies the maximum number of data files that can be persisted. `0` means no limit on the number of files. |\\n| TiDB | [`tidb_stmt_summary_file_max_days`](/tidb-configuration-file.md#tidb_stmt_summary_file_max_days-new-in-v660) | Newly added | When statements summary persistence is enabled, this configuration specifies the maximum number of days to keep persistent data files. |\\n| TiDB | [`tidb_stmt_summary_file_max_size`](/tidb-configuration-file.md#tidb_stmt_summary_file_max_size-new-in-v660) | Newly added | When statements summary persistence is enabled, this configuration specifies the maximum size of a persistent data file (in MiB). |\\n| TiDB | [`tidb_stmt_summary_filename`](/tidb-configuration-file.md#tidb_stmt_summary_filename-new-in-v660) | Newly added | When statements summary persistence is enabled, this configuration specifies the file to which persistent data is written. |\\n| TiKV | [`resource-control.enabled`](/tikv-configuration-file.md#resource-control) | Newly added | Whether to enable scheduling for user foreground read/write requests according to the Request Unit (RU) of the corresponding resource groups. The default value is `false`, which means to disable scheduling according to the RU of the corresponding resource groups. |\\n| TiKV | [`storage.engine`](/tikv-configuration-file.md#engine-new-in-v660) | Newly added | This configuration item specifies the type of the storage engine. Value options are `\"raft-kv\"` and `\"partitioned-raft-kv\"`. This configuration item can only be specified when creating a cluster and cannot be modified once being specified. |\\n| TiKV | [`rocksdb.write-buffer-flush-oldest-first`](/tikv-configuration-file.md#write-buffer-flush-oldest-first-new-in-v660) | Newly added | This configuration item specifies the flush strategy used when the memory usage of `memtable` of the current RocksDB reaches the threshold.  |\\n| TiKV | [`rocksdb.write-buffer-limit`](/tikv-configuration-file.md#write-buffer-limit-new-in-v660) | Newly added | This configuration item specifies the limit on total memory used by `memtable` of all RocksDB instances in a single TiKV. The default value is 25% of the total machine memory.  |\\n| PD  | [`pd-server.enable-gogc-tuner`](/pd-configuration-file.md#enable-gogc-tuner-new-in-v660) | Newly added | This configuration item controls whether to enable the GOGC tuner, which is disabled by default. |\\n| PD  | [`pd-server.gc-tuner-threshold`](/pd-configuration-file.md#gc-tuner-threshold-new-in-v660) | Newly added | This configuration item specifies the maximum memory threshold ratio for tuning GOGC. The default value is `0.6`. |\\n| PD  | [`pd-server.server-memory-limit-gc-trigger`](/pd-configuration-file.md#server-memory-limit-gc-trigger-new-in-v660) | Newly added | This configuration item specifies the threshold ratio at which PD tries to trigger GC. The default value is `0.7`. |\\n| PD  | [`pd-server.server-memory-limit`](/pd-configuration-file.md#server-memory-limit-new-in-v660) | Newly added | This configuration item specifies the memory limit ratio for a PD instance. The value `0` means no memory limit. |\\n| TiCDC | [`scheduler.region-per-span`](/ticdc/ticdc-changefeed-config.md#changefeed-configuration-parameters) | Newly added | This configuration item controls whether to split a table into multiple replication ranges based on the number of Regions, and these ranges can be replicated by multiple TiCDC nodes. The default value is `50000`. |\\n| TiDB Lightning | [`compress-kv-pairs`](/tidb-lightning/tidb-lightning-configuration.md#tidb-lightning-task) | Newly added | This configuration item controls whether to enable compression when sending KV pairs to TiKV in the physical import mode. The default value is empty, meaning that the compression is not enabled. |\\n| DM | [`checksum-physical`](/dm/task-configuration-file-full.md) | Newly added | This configuration item controls whether DM performs `ADMIN CHECKSUM TABLE <table>` for each table to verify data integrity after the import. The default value is `\"required\"`, which performs admin checksum after the import. If checksum fails, DM pauses the task and you need to manually handle the failure. |\\n| DM | [`disk-quota-physical`](/dm/task-configuration-file-full.md) | Newly added | This configuration item sets the disk quota. It corresponds to the [`disk-quota` configuration](/tidb-lightning/tidb-lightning-physical-import-mode-usage.md#configure-disk-quota-new-in-v620) of TiDB Lightning. |\\n| DM | [`on-duplicate-logical`](/dm/task-configuration-file-full.md) | Newly added | This configuration item controls how DM resolves conflicting data in the logical import mode. The default value is `\"replace\"`, which means using the new data to replace the existing data. |\\n| DM | [`on-duplicate-physical`](/dm/task-configuration-file-full.md) | Newly added | This configuration item controls how DM resolves conflicting data in the physical import mode. The default value is `\"none\"`, which means not resolving conflicting data. `\"none\"` has the best performance, but might lead to inconsistent data in the downstream database. |\\n| DM | [`sorting-dir-physical`](/dm/task-configuration-file-full.md) | Newly added | This configuration item specifies the directory used for local KV sorting in the physical import mode. The default value is the same as the `dir` configuration. |\\n| sync-diff-inspector | [`skip-non-existing-table`](/sync-diff-inspector/sync-diff-inspector-overview.md#configuration-file-description) | Newly added | This configuration item controls whether to skip checking upstream and downstream data consistency when tables in the downstream do not exist in the upstream.  |\\n| TiSpark | [`spark.tispark.replica_read`](/tispark-overview.md#tispark-configurations) | Newly added | This configuration item controls the type of replicas to be read. The value options are `leader`, `follower`, and `learner`. |\\n| TiSpark | [`spark.tispark.replica_read.label`](/tispark-overview.md#tispark-configurations) | Newly added | This configuration item is used to set labels for the target TiKV node. |\\n\\n### Others\\n\\n- Support dynamically modifying [`store-io-pool-size`](/tikv-configuration-file.md#store-io-pool-size-new-in-v530). This facilitates more flexible TiKV performance tuning.\\n- Remove the limit on `LIMIT` clauses, thus improving the execution performance.\\n- Starting from v6.6.0, BR does not support restoring data to clusters earlier than v6.1.0.\\n- Starting from v6.6.0, TiDB no longer supports modifying column types on partitioned tables because of potential correctness issues.\\n\\n## Improvements\\n\\n+ TiDB\\n\\n    - Improve the scheduling mechanism of TTL background cleaning tasks to allow the cleaning task of a single table to be split into several sub-tasks and scheduled to run on multiple TiDB nodes simultaneously [#40361](https://github.com/pingcap/tidb/issues/40361) @[YangKeao](https://github.com/YangKeao)\\n    - Optimize the column name display of the result returned by running multi-statements after setting a non-default delimiter [#39662](https://github.com/pingcap/tidb/issues/39662) @[mjonss](https://github.com/mjonss)\\n    - Optimize the execution efficiency of statements after warning messages are generated [#39702](https://github.com/pingcap/tidb/issues/39702) @[tiancaiamao](https://github.com/tiancaiamao)\\n    - Support distributed data backfill for `ADD INDEX` (experimental) [#37119](https://github.com/pingcap/tidb/issues/37119) @[zimulala](https://github.com/zimulala)\\n    - Support using `CURDATE()` as the default value of a column [#38356](https://github.com/pingcap/tidb/issues/38356) @[CbcWestwolf](https://github.com/CbcWestwolf)\\n    - `partial order prop push down` now supports the LIST-type partitioned tables [#40273](https://github.com/pingcap/tidb/issues/40273) @[winoros](https://github.com/winoros)\\n    - Add error messages for conflicts between optimizer hints and execution plan bindings [#40910](https://github.com/pingcap/tidb/issues/40910) @[Reminiscent](https://github.com/Reminiscent)\\n    - Optimize the plan cache strategy to avoid non-optimal plans when using plan cache in some scenarios [#40312](https://github.com/pingcap/tidb/pull/40312) [#40218](https://github.com/pingcap/tidb/pull/40218) [#40280](https://github.com/pingcap/tidb/pull/40280) [#41136](https://github.com/pingcap/tidb/pull/41136) [#40686](https://github.com/pingcap/tidb/pull/40686) @[qw4990](https://github.com/qw4990)\\n    - Clear expired region cache regularly to avoid memory leak and performance degradation [#40461](https://github.com/pingcap/tidb/issues/40461) @[sticnarf](https://github.com/sticnarf)\\n    - `MODIFY COLUMN` is not supported on partitioned tables [#39915](https://github.com/pingcap/tidb/issues/39915) @[wjhuang2016](https://github.com/wjhuang2016)\\n    - Disable renaming of columns that partition tables depend on [#40150](https://github.com/pingcap/tidb/issues/40150) @[mjonss](https://github.com/mjonss)\\n    - Refine the error message reported when a column that a partitioned table depends on is deleted [#38739](https://github.com/pingcap/tidb/issues/38739) @[jiyfhust](https://github.com/jiyfhust)\\n    - Add a mechanism that `FLASHBACK CLUSTER` retries when it fails to check the `min-resolved-ts` [#39836](https://github.com/pingcap/tidb/issues/39836) @[Defined2014](https://github.com/Defined2014)\\n\\n+ TiKV\\n\\n    - Optimize the default values of some parameters in partitioned-raft-kv mode: the default value of the TiKV configuration item `storage.block-cache.capacity` is adjusted from 45% to 30%, and the default value of `region-split-size` is adjusted from `96MiB` adjusted to `10GiB`. When using raft-kv mode and `enable-region-bucket` is `true`, `region-split-size` is adjusted to 1 GiB by default. [#12842](https://github.com/tikv/tikv/issues/12842) @[tonyxuqqi](https://github.com/tonyxuqqi)\\n    - Support priority scheduling in Raftstore asynchronous writes [#13730](https://github.com/tikv/tikv/issues/13730) @[Connor1996](https://github.com/Connor1996)\\n    - Support starting TiKV on a CPU with less than 1 core [#13586](https://github.com/tikv/tikv/issues/13586) [#13752](https://github.com/tikv/tikv/issues/13752) [#14017](https://github.com/tikv/tikv/issues/14017) @[andreid-db](https://github.com/andreid-db)\\n    - Optimize the new detection mechanism of Raftstore slow score and add `evict-slow-trend-scheduler` [#14131](https://github.com/tikv/tikv/issues/14131) @[innerr](https://github.com/innerr)\\n    - Force the block cache of RocksDB to be shared and no longer support setting the block cache separately according to CF [#12936](https://github.com/tikv/tikv/issues/12936) @[busyjay](https://github.com/busyjay)\\n\\n+ PD\\n\\n    - Support managing the global memory threshold to alleviate the OOM problem (experimental) [#5827](https://github.com/tikv/pd/issues/5827) @[hnes](https://github.com/hnes)\\n    - Add the GC Tuner to alleviate the GC pressure (experimental) [#5827](https://github.com/tikv/pd/issues/5827) @[hnes](https://github.com/hnes)\\n    - Add the `evict-slow-trend-scheduler` scheduler to detect and schedule abnormal nodes [#5808](https://github.com/tikv/pd/pull/5808) @[innerr](https://github.com/innerr)\\n    - Add the keyspace manager to manage keyspace [#5293](https://github.com/tikv/pd/issues/5293) @[AmoebaProtozoa](https://github.com/AmoebaProtozoa)\\n\\n+ TiFlash\\n\\n    - Support an independent MVCC bitmap filter that decouples the MVCC filtering operations in the TiFlash data scanning process, which provides the foundation for future optimization of the data scanning process [#6296](https://github.com/pingcap/tiflash/issues/6296) @[JinheLin](https://github.com/JinheLin)\\n    - Reduce the memory usage of TiFlash by up to 30% when there is no query [#6589](https://github.com/pingcap/tiflash/pull/6589) @[hongyunyan](https://github.com/hongyunyan)\\n\\n+ Tools\\n\\n    + Backup & Restore (BR)\\n\\n        - Optimize the concurrency of downloading log backup files on the TiKV side to improve the performance of PITR recovery in regular scenarios [#14206](https://github.com/tikv/tikv/issues/14206) @[YuJuncen](https://github.com/YuJuncen)\\n\\n    + TiCDC\\n\\n        - Support batch `UPDATE` DML statements to improve TiCDC replication performance [#8084](https://github.com/pingcap/tiflow/issues/8084) @[amyangfei](https://github.com/amyangfei)\\n        - Implement MQ sink and MySQL sink in the asynchronous mode to improve the sink throughput [#5928](https://github.com/pingcap/tiflow/issues/5928) @[hicqu](https://github.com/hicqu) @[hi-rustin](https://github.com/Rustin170506)\\n\\n    + TiDB Data Migration (DM)\\n\\n        - Optimize DM alert rules and content [#7376](https://github.com/pingcap/tiflow/issues/7376) @[D3Hunter](https://github.com/D3Hunter)\\n\\n             Previously, alerts similar to \"DM_XXX_process_exits_with_error\" were raised whenever a related error occurred. But some alerts are caused by idle database connections, which can be recovered after reconnecting. To reduce these kinds of alerts, DM divides errors into two types: automatically recoverable errors and unrecoverable errors:\\n\\n            - For an error that is automatically recoverable, DM reports the alert only if the error occurs more than 3 times within 2 minutes.\\n            - For an error that is not automatically recoverable, DM maintains the original behavior and reports the alert immediately.\\n\\n        - Optimize relay performance by adding the async/batch relay writer [#4287](https://github.com/pingcap/tiflow/issues/4287) @[GMHDBJD](https://github.com/GMHDBJD)\\n\\n    + TiDB Lightning\\n\\n        - Physical Import Mode supports keyspace [#40531](https://github.com/pingcap/tidb/issues/40531) @[iosmanthus](https://github.com/iosmanthus)\\n        - Support setting the maximum number of conflicts by `lightning.max-error` [#40743](https://github.com/pingcap/tidb/issues/40743) @[dsdashun](https://github.com/dsdashun)\\n        - Support importing CSV data files with BOM headers [#40744](https://github.com/pingcap/tidb/issues/40744) @[dsdashun](https://github.com/dsdashun)\\n        - Optimize the processing logic when encountering TiKV flow-limiting errors and try other available regions instead [#40205](https://github.com/pingcap/tidb/issues/40205) @[lance6716](https://github.com/lance6716)\\n        - Disable checking the table foreign keys during import [#40027](https://github.com/pingcap/tidb/issues/40027) @[sleepymole](https://github.com/sleepymole)\\n\\n    + Dumpling\\n\\n        - Support exporting settings for foreign keys [#39913](https://github.com/pingcap/tidb/issues/39913) @[lichunzhu](https://github.com/lichunzhu)\\n\\n    + sync-diff-inspector\\n\\n        - Add a new parameter `skip-non-existing-table` to control whether to skip checking upstream and downstream data consistency when tables in the downstream do not exist in the upstream [#692](https://github.com/pingcap/tidb-tools/issues/692) @[lichunzhu](https://github.com/lichunzhu) @[liumengya94](https://github.com/liumengya94)\\n\\n## Bug fixes\\n\\n+ TiDB\\n\\n    - Fix the issue that a statistics collection task fails due to an incorrect `datetime` value [#39336](https://github.com/pingcap/tidb/issues/39336) @[xuyifangreeneyes](https://github.com/xuyifangreeneyes)\\n    - Fix the issue that `stats_meta` is not created following table creation [#38189](https://github.com/pingcap/tidb/issues/38189) @[xuyifangreeneyes](https://github.com/xuyifangreeneyes)\\n    - Fix frequent write conflicts in transactions when performing DDL data backfill [#24427](https://github.com/pingcap/tidb/issues/24427) @[mjonss](https://github.com/mjonss)\\n    - Fix the issue that sometimes an index cannot be created for an empty table using ingest mode [#39641](https://github.com/pingcap/tidb/issues/39641) @[tangenta](https://github.com/tangenta)\\n    - Fix the issue that `wait_ts` in the slow query log is the same for different SQL statements within the same transaction [#39713](https://github.com/pingcap/tidb/issues/39713) @[TonsnakeLin](https://github.com/TonsnakeLin)\\n    - Fix the issue that the `Assertion Failed` error is reported when adding a column during the process of deleting a row record [#39570](https://github.com/pingcap/tidb/issues/39570) @[wjhuang2016](https://github.com/wjhuang2016)\\n    - Fix the issue that the `not a DDL owner` error is reported when modifying a column type [#39643](https://github.com/pingcap/tidb/issues/39643) @[zimulala](https://github.com/zimulala)\\n    - Fix the issue that no error is reported when inserting a row after exhaustion of the auto-increment values of the `AUTO_INCREMENT` column [#38950](https://github.com/pingcap/tidb/issues/38950) @[Dousir9](https://github.com/Dousir9)\\n    - Fix the issue that the `Unknown column` error is reported when creating an expression index [#39784](https://github.com/pingcap/tidb/issues/39784) @[Defined2014](https://github.com/Defined2014)\\n    - Fix the issue that data cannot be inserted into a renamed table when the generated expression includes the name of this table [#39826](https://github.com/pingcap/tidb/issues/39826) @[Defined2014](https://github.com/Defined2014)\\n    - Fix the issue that the `INSERT ignore` statement cannot fill in default values when the column is write-only [#40192](https://github.com/pingcap/tidb/issues/40192) @[YangKeao](https://github.com/YangKeao)\\n    - Fix the issue that resources are not released when disabling the resource management module [#40546](https://github.com/pingcap/tidb/issues/40546) @[zimulala](https://github.com/zimulala)\\n    - Fix the issue that TTL tasks cannot trigger statistics updates in time [#40109](https://github.com/pingcap/tidb/issues/40109) @[YangKeao](https://github.com/YangKeao)\\n    - Fix the issue that unexpected data is read because TiDB improperly handles `NULL` values when constructing key ranges [#40158](https://github.com/pingcap/tidb/issues/40158) @[tiancaiamao](https://github.com/tiancaiamao)\\n    - Fix the issue that illegal values are written to a table when the `MODIFY COLUMN` statement also changes the default value of a column [#40164](https://github.com/pingcap/tidb/issues/40164) @[wjhuang2016](https://github.com/wjhuang2016)\\n    - Fix the issue that the adding index operation is inefficient due to invalid Region cache when there are many Regions in a table [#38436](https://github.com/pingcap/tidb/issues/38436) @[tangenta](https://github.com/tangenta)\\n    - Fix data race occurred in allocating auto-increment IDs [#40584](https://github.com/pingcap/tidb/issues/40584) @[Dousir9](https://github.com/Dousir9)\\n    - Fix the issue that the implementation of the not operator in JSON is incompatible with the implementation in MySQL [#40683](https://github.com/pingcap/tidb/issues/40683) @[YangKeao](https://github.com/YangKeao)\\n    - Fix the issue that concurrent view might cause DDL operations to be blocked [#40352](https://github.com/pingcap/tidb/issues/40352) @[zeminzhou](https://github.com/zeminzhou)\\n    - Fix data inconsistency caused by concurrently executing DDL statements to modify columns of partitioned tables [#40620](https://github.com/pingcap/tidb/issues/40620) @[mjonss](https://github.com/mjonss) @[mjonss](https://github.com/mjonss)\\n    - Fix the issue that \"Malformed packet\" is reported when using `caching_sha2_password` for authentication without specifying a password [#40831](https://github.com/pingcap/tidb/issues/40831) @[dveeden](https://github.com/dveeden)\\n    - Fix the issue that a TTL task fails if the primary key of the table contains an `ENUM` column [#40456](https://github.com/pingcap/tidb/issues/40456) @[lcwangchao](https://github.com/lcwangchao)\\n    - Fix the issue that some DDL operations blocked by MDL cannot be queried in `mysql.tidb_mdl_view` [#40838](https://github.com/pingcap/tidb/issues/40838) @[YangKeao](https://github.com/YangKeao)\\n    - Fix the issue that data race might occur during DDL ingestion [#40970](https://github.com/pingcap/tidb/issues/40970) @[tangenta](https://github.com/tangenta)\\n    - Fix the issue that TTL tasks might delete some data incorrectly after the time zone changes [#41043](https://github.com/pingcap/tidb/issues/41043) @[lcwangchao](https://github.com/lcwangchao)\\n    - Fix the issue that `JSON_OBJECT` might report an error in some cases [#39806](https://github.com/pingcap/tidb/issues/39806) @[YangKeao](https://github.com/YangKeao)\\n    - Fix the issue that TiDB might deadlock during initialization [#40408](https://github.com/pingcap/tidb/issues/40408) @[Defined2014](https://github.com/Defined2014)\\n    - Fix the issue that the value of system variables might be incorrectly modified in some cases due to memory reuse [#40979](https://github.com/pingcap/tidb/issues/40979) @[lcwangchao](https://github.com/lcwangchao)\\n    - Fix the issue that data might be inconsistent with the index when a unique index is created in the ingest mode [#40464](https://github.com/pingcap/tidb/issues/40464) @[tangenta](https://github.com/tangenta)\\n    - Fix the issue that some truncate operations cannot be blocked by MDL when truncating the same table concurrently [#40484](https://github.com/pingcap/tidb/issues/40484) @[wjhuang2016](https://github.com/wjhuang2016)\\n    - Fix the issue that the `SHOW PRIVILEGES` statement returns an incomplete privilege list [#40591](https://github.com/pingcap/tidb/issues/40591) @[CbcWestwolf](https://github.com/CbcWestwolf)\\n    - Fix the issue that TiDB panics when adding a unique index [#40592](https://github.com/pingcap/tidb/issues/40592) @[tangenta](https://github.com/tangenta)\\n    - Fix the issue that executing the `ADMIN RECOVER` statement might cause the index data to be corrupted [#40430](https://github.com/pingcap/tidb/issues/40430) @[xiongjiwei](https://github.com/xiongjiwei)\\n    - Fix the issue that a query might fail when the queried table contains a `CAST` expression in the expression index [#40130](https://github.com/pingcap/tidb/issues/40130) @[xiongjiwei](https://github.com/xiongjiwei)\\n    - Fix the issue that a unique index might still produce duplicate data in some cases [#40217](https://github.com/pingcap/tidb/issues/40217) @[tangenta](https://github.com/tangenta)\\n    - Fix the PD OOM issue when there is a large number of Regions but the table ID cannot be pushed down when querying some virtual tables using `Prepare` or `Execute` [#39605](https://github.com/pingcap/tidb/issues/39605) @[djshow832](https://github.com/djshow832)\\n    - Fix the issue that data race might occur when an index is added [#40879](https://github.com/pingcap/tidb/issues/40879) @[tangenta](https://github.com/tangenta)\\n    - Fix the `can\\'t find proper physical plan` issue caused by virtual columns [#41014](https://github.com/pingcap/tidb/issues/41014) @[AilinKid](https://github.com/AilinKid)\\n    - Fix the issue that TiDB cannot restart after global bindings are created for partition tables in dynamic trimming mode [#40368](https://github.com/pingcap/tidb/issues/40368) @[Yisaer](https://github.com/Yisaer)\\n    - Fix the issue that `auto analyze` causes graceful shutdown to take a long time [#40038](https://github.com/pingcap/tidb/issues/40038) @[xuyifangreeneyes](https://github.com/xuyifangreeneyes)\\n    - Fix the panic of the TiDB server when the IndexMerge operator triggers memory limiting behaviors [#41036](https://github.com/pingcap/tidb/pull/41036) @[guo-shaoge](https://github.com/guo-shaoge)\\n    - Fix the issue that the `SELECT * FROM table_name LIMIT 1` query on partitioned tables is slow [#40741](https://github.com/pingcap/tidb/pull/40741) @[solotzg](https://github.com/solotzg)\\n\\n+ TiKV\\n\\n    - Fix an error that occurs when casting the `const Enum` type to other types [#14156](https://github.com/tikv/tikv/issues/14156) @[wshwsh12](https://github.com/wshwsh12)\\n    - Fix the issue that Resolved TS causes higher network traffic [#14092](https://github.com/tikv/tikv/issues/14092) @[overvenus](https://github.com/overvenus)\\n    - Fix the data inconsistency issue caused by network failure between TiDB and TiKV during the execution of a DML after a failed pessimistic DML [#14038](https://github.com/tikv/tikv/issues/14038) @[MyonKeminta](https://github.com/MyonKeminta)\\n\\n+ PD\\n\\n    - Fix the issue that the Region Scatter task generates redundant replicas unexpectedly [#5909](https://github.com/tikv/pd/issues/5909) @[HundunDM](https://github.com/HunDunDM)\\n    - Fix the issue that the Online Unsafe Recovery feature would get stuck and time out in `auto-detect` mode [#5753](https://github.com/tikv/pd/issues/5753) @[Connor1996](https://github.com/Connor1996)\\n    - Fix the issue that the execution `replace-down-peer` slows down under certain conditions [#5788](https://github.com/tikv/pd/issues/5788) @[HundunDM](https://github.com/HunDunDM)\\n    - Fix the PD OOM issue that occurs when the calls of `ReportMinResolvedTS` are too frequent [#5965](https://github.com/tikv/pd/issues/5965) @[HundunDM](https://github.com/HunDunDM)\\n\\n+ TiFlash\\n\\n    - Fix the issue that querying TiFlash-related system tables might get stuck [#6745](https://github.com/pingcap/tiflash/pull/6745) @[lidezhu](https://github.com/lidezhu)\\n    - Fix the issue that semi-joins use excessive memory when calculating Cartesian products [#6730](https://github.com/pingcap/tiflash/issues/6730) @[gengliqi](https://github.com/gengliqi)\\n    - Fix the issue that the result of the division operation on the DECIMAL data type is not rounded [#6393](https://github.com/pingcap/tiflash/issues/6393) @[LittleFall](https://github.com/LittleFall)\\n    - Fix the issue that `start_ts` cannot uniquely identify an MPP query in TiFlash queries, which might cause an MPP query to be incorrectly canceled [#43426](https://github.com/pingcap/tidb/issues/43426) @[hehechen](https://github.com/hehechen)\\n\\n+ Tools\\n\\n    + Backup & Restore (BR)\\n\\n        - Fix the issue that when restoring log backup, hot Regions cause the restore to fail [#37207](https://github.com/pingcap/tidb/issues/37207) @[Leavrth](https://github.com/Leavrth)\\n        - Fix the issue that restoring data to a cluster on which the log backup is running causes the log backup file to be unrecoverable [#40797](https://github.com/pingcap/tidb/issues/40797) @[Leavrth](https://github.com/Leavrth)\\n        - Fix the issue that the PITR feature does not support CA-bundles [#38775](https://github.com/pingcap/tidb/issues/38775) @[YuJuncen](https://github.com/YuJuncen)\\n        - Fix the panic issue caused by duplicate temporary tables during recovery [#40797](https://github.com/pingcap/tidb/issues/40797) @[joccau](https://github.com/joccau)\\n        - Fix the issue that PITR does not support configuration changes for PD clusters [#14165](https://github.com/tikv/tikv/issues/14165) @[YuJuncen](https://github.com/YuJuncen)\\n        - Fix the issue that the connection failure between PD and tidb-server causes PITR backup progress not to advance [#41082](https://github.com/pingcap/tidb/issues/41082) @[YuJuncen](https://github.com/YuJuncen)\\n        - Fix the issue that TiKV cannot listen to PITR tasks due to the connection failure between PD and TiKV [#14159](https://github.com/tikv/tikv/issues/14159) @[YuJuncen](https://github.com/YuJuncen)\\n        - Fix the issue that the frequency of `resolve lock` is too high when there is no PITR backup task in the TiDB cluster [#40759](https://github.com/pingcap/tidb/issues/40759) @[joccau](https://github.com/joccau)\\n        - Fix the issue that when a PITR backup task is deleted, the residual backup data causes data inconsistency in new tasks [#40403](https://github.com/pingcap/tidb/issues/40403) @[joccau](https://github.com/joccau)\\n\\n    + TiCDC\\n\\n        - Fix the issue that `transaction_atomicity` and `protocol` cannot be updated via the configuration file [#7935](https://github.com/pingcap/tiflow/issues/7935) @[CharlesCheung96](https://github.com/CharlesCheung96)\\n        - Fix the issue that precheck is not performed on the storage path of redo log [#6335](https://github.com/pingcap/tiflow/issues/6335) @[CharlesCheung96](https://github.com/CharlesCheung96)\\n        - Fix the issue of insufficient duration that redo log can tolerate for S3 storage failure [#8089](https://github.com/pingcap/tiflow/issues/8089) @[CharlesCheung96](https://github.com/CharlesCheung96)\\n        - Fix the issue that changefeed might get stuck in special scenarios such as when scaling in or scaling out TiKV or TiCDC nodes [#8174](https://github.com/pingcap/tiflow/issues/8174) @[hicqu](https://github.com/hicqu)\\n        - Fix the issue of too high traffic among TiKV nodes [#14092](https://github.com/tikv/tikv/issues/14092) @[overvenus](https://github.com/overvenus)\\n        - Fix the performance issues of TiCDC in terms of CPU usage, memory control, and throughput when the pull-based sink is enabled [#8142](https://github.com/pingcap/tiflow/issues/8142) [#8157](https://github.com/pingcap/tiflow/issues/8157) [#8001](https://github.com/pingcap/tiflow/issues/8001) [#5928](https://github.com/pingcap/tiflow/issues/5928) @[hicqu](https://github.com/hicqu) @[hi-rustin](https://github.com/Rustin170506)\\n\\n    + TiDB Data Migration (DM)\\n\\n        - Fix the issue that the `binlog-schema delete` command fails to execute [#7373](https://github.com/pingcap/tiflow/issues/7373) @[liumengya94](https://github.com/liumengya94)\\n        - Fix the issue that the checkpoint does not advance when the last binlog is a skipped DDL [#8175](https://github.com/pingcap/tiflow/issues/8175) @[D3Hunter](https://github.com/D3Hunter)\\n        - Fix a bug that when the expression filters of both \"update\" and \"non-update\" types are specified in one table, all `UPDATE` statements are skipped [#7831](https://github.com/pingcap/tiflow/issues/7831) @[lance6716](https://github.com/lance6716)\\n        - Fix a bug that when only one of `update-old-value-expr` or `update-new-value-expr` is set for a table, the filter rule does not take effect or DM panics [#7774](https://github.com/pingcap/tiflow/issues/7774) @[lance6716](https://github.com/lance6716)\\n\\n    + TiDB Lightning\\n\\n        - Fix the issue that TiDB Lightning timeout hangs due to TiDB restart in some scenarios [#33714](https://github.com/pingcap/tidb/issues/33714) @[lichunzhu](https://github.com/lichunzhu)\\n        - Fix the issue that TiDB Lightning might incorrectly skip conflict resolution when all but the last TiDB Lightning instance encounters a local duplicate record during a parallel import [#40923](https://github.com/pingcap/tidb/issues/40923) @[lichunzhu](https://github.com/lichunzhu)\\n        - Fix the issue that precheck cannot accurately detect the presence of a running TiCDC in the target cluster [#41040](https://github.com/pingcap/tidb/issues/41040) @[lance6716](https://github.com/lance6716)\\n        - Fix the issue that TiDB Lightning panics in the split-region phase [#40934](https://github.com/pingcap/tidb/issues/40934) @[lance6716](https://github.com/lance6716)\\n        - Fix the issue that the conflict resolution logic (`duplicate-resolution`) might lead to inconsistent checksums [#40657](https://github.com/pingcap/tidb/issues/40657) @[sleepymole](https://github.com/sleepymole)\\n        - Fix a possible OOM problem when there is an unclosed delimiter in the data file [#40400](https://github.com/pingcap/tidb/issues/40400) @[buchuitoudegou](https://github.com/buchuitoudegou)\\n        - Fix the issue that the file offset in the error report exceeds the file size [#40034](https://github.com/pingcap/tidb/issues/40034) @[buchuitoudegou](https://github.com/buchuitoudegou)\\n        - Fix an issue with the new version of PDClient that might cause parallel import to fail [#40493](https://github.com/pingcap/tidb/issues/40493) @[AmoebaProtozoa](https://github.com/AmoebaProtozoa)\\n        - Fix the issue that TiDB Lightning prechecks cannot find dirty data left by previously failed imports [#39477](https://github.com/pingcap/tidb/issues/39477) @[dsdashun](https://github.com/dsdashun)\\n\\n## Contributors\\n\\nWe would like to thank the following contributors from the TiDB community:\\n\\n- [morgo](https://github.com/morgo)\\n- [jiyfhust](https://github.com/jiyfhust)\\n- [b41sh](https://github.com/b41sh)\\n- [sourcelliu](https://github.com/sourcelliu)\\n- [songzhibin97](https://github.com/songzhibin97)\\n- [mamil](https://github.com/mamil)\\n- [Dousir9](https://github.com/Dousir9)\\n- [hihihuhu](https://github.com/hihihuhu)\\n- [mychoxin](https://github.com/mychoxin)\\n- [xuning97](https://github.com/xuning97)\\n- [andreid-db](https://github.com/andreid-db)\\n', doc_link='https://docs.pingcap.com/tidb/v8.1/release-6.6.0'),\n",
       " 16199: DocumentData(id=16199, chunks={}, content='---\\ntitle: Troubleshoot a TiFlash Cluster\\nsummary: Learn common operations when you troubleshoot a TiFlash cluster.\\n---\\n\\n# Troubleshoot a TiFlash Cluster\\n\\nThis section describes some commonly encountered issues when using TiFlash, the reasons, and the solutions.\\n\\n## TiFlash fails to start\\n\\nThe issue might occur due to different reasons. It is recommended that you troubleshoot it following the steps below:\\n\\n1. Check whether your system is RedHat Enterprise Linux 8.\\n\\n    RedHat Enterprise Linux 8 does not have the `libnsl.so` system library. You can manually install it via the following command:\\n\\n    \\n    ```shell\\n    dnf install libnsl\\n    ```\\n\\n2. Check your system\\'s `ulimit` parameter setting.\\n\\n    \\n    ```shell\\n    ulimit -n 1000000\\n    ```\\n\\n3. Use the PD Control tool to check whether there is any TiFlash instance that failed to go offline on the node (same IP and Port) and force the instance(s) to go offline. For detailed steps, refer to [Scale in a TiFlash cluster](/scale-tidb-using-tiup.md#scale-in-a-tiflash-cluster).\\n\\nIf the above methods cannot resolve your issue, save the TiFlash log files and [get support](/support.md) from PingCAP or the community.\\n\\n## TiFlash replica is always unavailable\\n\\nThis is because TiFlash is in an abnormal state caused by configuration errors or environment issues. Take the following steps to identify the faulty component:\\n\\n1. Check whether PD enables the `Placement Rules` feature:\\n\\n    \\n    ```shell\\n    echo \\'config show replication\\' | /path/to/pd-ctl -u http://${pd-ip}:${pd-port}\\n    ```\\n\\n    - If `true` is returned, go to the next step.\\n    - If `false` is returned, [enable the Placement Rules feature](/configure-placement-rules.md#enable-placement-rules) and go to the next step.\\n\\n2. Check whether the TiFlash process is working correctly by viewing `UpTime` on the TiFlash-Summary monitoring panel.\\n\\n3. Check whether the TiFlash proxy status is normal through `pd-ctl`.\\n\\n    \\n    ```shell\\n    echo \"store\" | /path/to/pd-ctl -u http://${pd-ip}:${pd-port}\\n    ```\\n\\n    The TiFlash proxy\\'s `store.labels` includes information such as `{\"key\": \"engine\", \"value\": \"tiflash\"}`. You can check this information to confirm a TiFlash proxy.\\n\\n4. Check whether `pd buddy` can correctly print the logs (the log path is the value of `log` in the [flash.flash_cluster] configuration item; the default log path is under the `tmp` directory configured in the TiFlash configuration file).\\n\\n5. Check whether the number of configured replicas is less than or equal to the number of TiKV nodes in the cluster. If not, PD cannot replicate data to TiFlash:\\n\\n    \\n    ```shell\\n    echo \\'config placement-rules show\\' | /path/to/pd-ctl -u http://${pd-ip}:${pd-port}\\n    ```\\n\\n    Reconfirm the value of `default: count`.\\n\\n    > **Note:**\\n    >\\n    > - When [Placement Rules](/configure-placement-rules.md) are enabled and multiple rules exist, the previously configured [`max-replicas`](/pd-configuration-file.md#max-replicas), [`location-labels`](/pd-configuration-file.md#location-labels), and [`isolation-level`](/pd-configuration-file.md#isolation-level) no longer take effect. To adjust the replica policy, use the interface related to Placement Rules.\\n    > - When [Placement Rules](/configure-placement-rules.md) are enabled and only one default rule exists, TiDB will automatically update this default rule when `max-replicas`, `location-labels`, or `isolation-level` configurations are changed.\\n\\n6. Check whether the remaining disk space of the machine (where `store` of the TiFlash node is) is sufficient. By default, when the remaining disk space is less than 20% of the `store` capacity (which is controlled by the `low-space-ratio` parameter), PD cannot schedule data to this TiFlash node.\\n\\n## Some queries return the `Region Unavailable` error\\n\\nIf the load pressure on TiFlash is too heavy and it causes that TiFlash data replication falls behind, some queries might return the `Region Unavailable` error.\\n\\nIn this case, you can balance the load pressure by adding more TiFlash nodes.\\n\\n## Data file corruption\\n\\nTake the following steps to handle the data file corruption:\\n\\n1. Refer to [Take a TiFlash node down](/scale-tidb-using-tiup.md#scale-in-a-tiflash-cluster) to take the corresponding TiFlash node down.\\n2. Delete the related data of the TiFlash node.\\n3. Redeploy the TiFlash node in the cluster.\\n\\n## TiFlash analysis is slow\\n\\nIf a statement contains operators or functions not supported in the MPP mode, TiDB does not select the MPP mode. Therefore, the analysis of the statement is slow. In this case, you can execute the `EXPLAIN` statement to check for operators or functions not supported in the MPP mode.\\n\\n\\n```sql\\ncreate table t(a datetime);\\nalter table t set tiflash replica 1;\\ninsert into t values(\\'2022-01-13\\');\\nset @@session.tidb_enforce_mpp=1;\\nexplain select count(*) from t where subtime(a, \\'12:00:00\\') > \\'2022-01-01\\' group by a;\\nshow warnings;\\n```\\n\\nIn this example, the warning message shows that TiDB does not select the MPP mode because TiDB 5.4 and earlier versions do not support the `subtime` function.\\n\\n```\\n+---------+------+-----------------------------------------------------------------------------+\\n> | Level   | Code | Message                                                                     |\\n+---------+------+-----------------------------------------------------------------------------+\\n| Warning | 1105 | Scalar function \\'subtime\\'(signature: SubDatetimeAndString, return type: datetime) is not supported to push down to tiflash now.       |\\n+---------+------+-----------------------------------------------------------------------------+\\n```\\n\\n## Data is not replicated to TiFlash\\n\\nAfter deploying a TiFlash node and starting replication (by performing the ALTER operation), no data is replicated to it. In this case, you can identify and address the problem by following the steps below:\\n\\n1. Check whether the replication is successful by running the `ALTER table <tbl_name> set tiflash replica <num>` command and check the output.\\n\\n    - If there is output, go to the next step.\\n    - If there is no output, run the `SELECT * FROM information_schema.tiflash_replica` command to check whether TiFlash replicas have been created. If not, run the `ALTER table ${tbl_name} set tiflash replica ${num}` command again, check whether other statements (for example, `add index`) have been executed, or check whether DDL executions are successful.\\n\\n2. Check whether TiFlash Region replication runs correctly.\\n\\n   Check whether there is any change in `progress`:\\n\\n   - If yes, TiFlash replication runs correctly.\\n   - If no, TiFlash replication is abnormal. In `tidb.log`, search the log saying `Tiflash replica is not available`. Check whether `progress` of the corresponding table is updated. If not, check the `tiflash log` for further information. For example, search `lag_region_info` in `tiflash log` to find out which Region lags behind.\\n\\n3. Check whether the [Placement Rules](/configure-placement-rules.md) function has been enabled by using pd-ctl:\\n\\n    \\n    ```shell\\n    echo \\'config show replication\\' | /path/to/pd-ctl -u http://<pd-ip>:<pd-port>\\n    ```\\n\\n    - If `true` is returned, go to the next step.\\n    - If `false` is returned, [enable the Placement Rules feature](/configure-placement-rules.md#enable-placement-rules) and go to the next step.\\n\\n4. Check whether the `max-replicas` configuration is correct:\\n\\n    - If the value of `max-replicas` does not exceed the number of TiKV nodes in the cluster, go to the next step.\\n    - If the value of `max-replicas` is greater than the number of TiKV nodes in the cluster, the PD does not replicate data to the TiFlash node. To address this issue, change `max-replicas` to an integer fewer than or equal to the number of TiKV nodes in the cluster.\\n\\n    > **Note:**\\n    >\\n    > `max-replicas` is defaulted to 3. In production environments, the value is usually fewer than the number of TiKV nodes. In test environments, the value can be 1.\\n\\n    \\n    ```shell\\n        curl -X POST -d \\'{\\n            \"group_id\": \"pd\",\\n            \"id\": \"default\",\\n            \"start_key\": \"\",\\n            \"end_key\": \"\",\\n            \"role\": \"voter\",\\n            \"count\": 3,\\n            \"location_labels\": [\\n            \"host\"\\n            ]\\n        }\\' <http://172.16.x.xxx:2379/pd/api/v1/config/rule>\\n    ```\\n\\n5. Check whether TiDB has created any placement rule for tables.\\n\\n    Search the logs of TiDB DDL Owner and check whether TiDB has notified PD to add placement rules. For non-partitioned tables, search `ConfigureTiFlashPDForTable`. For partitioned tables, search `ConfigureTiFlashPDForPartitions`.\\n\\n    - If the keyword is found, go to the next step.\\n    - If not, collect logs of the corresponding component for troubleshooting.\\n\\n6. Check whether PD has configured any placement rule for tables.\\n\\n    Run the `curl http://<pd-ip>:<pd-port>/pd/api/v1/config/rules/group/tiflash` command to view  all TiFlash placement rules on the current PD. If a rule with the ID being `table-<table_id>-r` is found, the PD has configured a placement rule successfully.\\n\\n7. Check whether the PD schedules properly.\\n\\n    Search the `pd.log` file for the `table-<table_id>-r` keyword and scheduling behaviors like `add operator`.\\n\\n    - If the keyword is found, the PD schedules properly.\\n    - If not, the PD does not schedule properly.\\n\\n## Data replication gets stuck\\n\\nIf data replication on TiFlash starts normally but then all or some data fails to be replicated after a period of time, you can confirm or resolve the issue by performing the following steps:\\n\\n1. Check the disk space.\\n\\n    Check whether the disk space ratio is higher than the value of `low-space-ratio` (defaulted to 0.8. When the space usage of a node exceeds 80%, the PD stops migrating data to this node to avoid exhaustion of disk space).\\n\\n    - If the disk usage ratio is greater than or equal to the value of `low-space-ratio`, the disk space is insufficient. To relieve the disk space, remove unnecessary files, such as `space_placeholder_file` (if necessary, set `reserve-space` to 0MB after removing the file) under the `${data}/flash/` folder.\\n    - If the disk usage ratio is less than the value of `low-space-ratio`, the disk space is sufficient. Go to the next step.\\n\\n2. Check whether there is any `down peer` (a `down peer` might cause the replication to get stuck).\\n\\n    Run the `pd-ctl region check-down-peer` command to check whether there is any `down peer`. If any, run the `pd-ctl operator add remove-peer <region-id> <tiflash-store-id>` command to remove it.\\n\\n## Data replication is slow\\n\\nThe causes may vary. You can address the problem by performing the following steps.\\n\\n1. Increase [`store limit`](/configure-store-limit.md#usage) to accelerate replication.\\n\\n2. Adjust the load on TiFlash.\\n\\n    Excessively high load on TiFlash can also result in slow replication. You can check the load of TiFlash indicators on the **TiFlash-Summary** panel on Grafana:\\n\\n    - `Applying snapshots Count`: `TiFlash-summary` > `raft` > `Applying snapshots Count`\\n    - `Snapshot Predecode Duration`: `TiFlash-summary` > `raft` > `Snapshot Predecode Duration`\\n    - `Snapshot Flush Duration`: `TiFlash-summary` > `raft` > `Snapshot Flush Duration`\\n    - `Write Stall Duration`: `TiFlash-summary` > `Storage Write Stall` > `Write Stall Duration`\\n    - `generate snapshot CPU`: `TiFlash-Proxy-Details` > `Thread CPU` > `Region task worker pre-handle/generate snapshot CPU`\\n\\n    Based on your service priorities, adjust the load accordingly to achieve optimal performance.\\n', doc_link='https://docs.pingcap.com/tidb/v8.1/troubleshoot-tiflash'),\n",
       " 16482: DocumentData(id=16482, chunks={}, content=\"---\\ntitle: Daily Check for TiDB Data Migration\\nsummary: Learn about the daily check of TiDB Data Migration (DM).\\n---\\n\\n# Daily Check for TiDB Data Migration\\n\\nThis document summarizes how to perform a daily check on TiDB Data Migration (DM).\\n\\n+ Method 1: Execute the `query-status` command to check the running status of the task and the error output (if any). For details, see [Query Status](/dm/dm-query-status.md).\\n\\n+ Method 2: If Prometheus and Grafana are correctly deployed when you deploy the DM cluster using TiUP, you can view DM monitoring metrics in Grafana. For example, suppose that the Grafana's address is `172.16.10.71`, go to <http://172.16.10.71:3000>, enter the Grafana dashboard, and select the DM Dashboard to check monitoring metrics of DM. For more information of these metrics, see [DM Monitoring Metrics](/dm/monitor-a-dm-cluster.md).\\n\\n+ Method 3: Check the running status of DM and the error (if any) using the log file.\\n\\n    - DM-master log directory: It is specified by the `--log-file` DM-master process parameter. If DM is deployed using TiUP, the log directory is `{log_dir}` in the DM-master node.\\n    - DM-worker log directory: It is specified by the `--log-file` DM-worker process parameter. If DM is deployed using TiUP, the log directory is `{log_dir}` in the DM-worker node.\\n\", doc_link='https://docs.pingcap.com/tidb/v8.1/dm-daily-check')}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "knowledge_retrieved = {}\n",
    "for action in next_actions:\n",
    "    print(action)\n",
    "    if action.tool == 'retrieve_knowledge':\n",
    "        data = gkb.retrieve_graph_data(session, action.query)\n",
    "    elif action.tool == 'retrieve_neighbors':\n",
    "        data = gkb.retrieve_neighbors(session, action.entity_ids, action.query)\n",
    "    elif action.tool == 'retrieve_documents':\n",
    "        data = gkb.retrieve_documents(session, action.query, 30)\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid tool: {action.tool}\")\n",
    "\n",
    "    for doc_id, doc in data.documents.items():\n",
    "        if doc_id not in knowledge_retrieved:\n",
    "            knowledge_retrieved[doc_id] = doc\n",
    "        \n",
    "        for chunk_id, chunk in doc.chunks.items():\n",
    "            if chunk_id not in knowledge_retrieved[doc_id].chunks:\n",
    "                knowledge_retrieved[doc_id].chunks[chunk_id] = chunk\n",
    "                continue\n",
    "\n",
    "            existing_chunk = knowledge_retrieved[doc_id].chunks[chunk_id]\n",
    "            rel_dict = {r['id']: r for r in existing_chunk.relationships}\n",
    "            for relationship in chunk.relationships:\n",
    "                rel_id = relationship.id\n",
    "                if rel_id in rel_dict:\n",
    "                    rel_dict[rel_id]['similarity_score'] = max(\n",
    "                        rel_dict[rel_id]['similarity_score'],\n",
    "                        relationship.similarity_score\n",
    "                    )\n",
    "                else:\n",
    "                    rel_dict[rel_id] = relationship.to_dict()\n",
    "\n",
    "            knowledge_retrieved[doc_id].chunks[chunk_id].relationships = list(rel_dict.values())\n",
    "\n",
    "action_history.append(action)\n",
    "\n",
    "knowledge_retrieved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(knowledge_retrieved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing document(15833, https://docs.pingcap.com/tidb/v8.1/release-6.6.0)\n"
     ]
    }
   ],
   "source": [
    "from graph.knowledge_synthesizer import KnowledgeSynthesizer\n",
    "\n",
    "synthesizer = KnowledgeSynthesizer(llm_client)\n",
    "result = synthesizer.iterative_answer_synthesis(\n",
    "    query=query,\n",
    "    documents=knowledge_retrieved,\n",
    "    reasoning=reasoning\n",
    ")\n",
    "\n",
    "# Access the results\n",
    "final_answer = result[\"final_answer\"]\n",
    "evolution = result[\"evolution_history\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'在 TiDB 集群中，当某个节点（TiKV 实例所在机器）发生故障，并且该节点仍然存在于集群中时，会出现部分 Region 的副本处于 down 状态（常称 down-peer）。TiDB（主要由 PD 和 TiKV 协同工作实现副本管理）对副本迁移有专门的机制，以确保集群数据的高可用性。下面详细说明这一过程以及 down-peer 数量如何变化：\\n\\n1. 节点故障检测与标记\\n   • PD 会周期性地从各个 TiKV 实例接收心跳信息，当检测到某个 TiKV 节点长时间未反馈（超过配置的超时时间），该节点所属的所有 Region 中对应的副本就会被标记为 down-peer。\\n   • 这些 down-peer 指示该副本暂时无法提供服务，从而可能影响该 Region 的数据可用性（例如，如果出现超过半数副本不可用，就会影响 Raft 选举）。\\n\\n2. 副本自动迁移（Replica Rebalancing/Replacement）\\n   • 当 PD 检测到某个节点的副本长时间处于 down 状态后，会发起副本调度任务。调度逻辑会选择健康的 TiKV 节点来复制数据，从而在保持 Region 副本数（例如 3 副本）的同时替换失效副本。\\n   • 调度时，PD 会依据集群负载、数据本地性、标签规则等因素选择目标节点，新副本在新节点上被创建后，Raft 协议会进行日志复制，以确保新加入的副本跟上已有副本的数据进度。\\n   • 当新副本同步完成且达到一定状态之后，PD 会发出指令，将 down 状态的副本从 Raft 配置中移除，从而完成一次迁移操作。\\n\\n3. Down-Peer 数量的变化\\n   • 在故障节点存在期间，因检测到无法通信，集群监控与 PD 调度模块会持续统计该节点上拥有的 down-peer 数量。\\n   • 一旦调度完成，并且 Region 的副本风险得到恢复（即用能够正常通信的新副本替换掉原先 down 的副本后），整个 Region 的健康状态恢复正常，此时该 Region 不再计入 down-peer 数量。\\n   • 因此，随着所有故障节点上 down 状态的实例副本成功迁移到健康节点后，监控中所统计的 down-peer 数量就会逐步减少，直至恢复正常（当然前提是故障节点仍然在集群中但其 Region 数据已全部调度完成）。\\n\\n总结来说：\\n   如果某个节点出现故障但实例没有被从集群中摘除，刚开始会因为网络或进程失联等原因，导致该节点上部分 Region 的副本被标记为 down-peer；随后，PD 会自动调度，并在新的健康节点上创建新副本，待新副本同步完毕并替换掉 down 副本之后，该 Region 的状态恢复正常，从而全局统计的 down-peer 数量会减少。\\n\\n补充说明：\\n   新版本 TiDB（例如 TiDB 6.6.0）在许多方面都有改进，但整个副本迁移机制依然依赖于 PD 对 TiKV 状态的监控与自动调度策略，这个基本机制在历次版本中保持了较高的一致性。相关调度策略与监控逻辑确保了在节点部分故障或者故障恢复后，集群能够主动削减 down-peer 数量，从而提高整体数据可用性和系统健康度。'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(final_answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graph",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
