{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from entity_agg import EntityAggregator\n",
    "from setting.db import SessionLocal\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "session = SessionLocal()\n",
    "aggregator = EntityAggregator(session, \"entities_150001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "iteration = 0\n",
    "batch = 5000\n",
    "clusters_info = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start iteration 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing similarity rows: 100%|██████████| 495/495 [00:14<00:00, 34.20it/s]\n",
      "DBSCAN clustering: 100%|██████████| 1/1 [00:00<00:00, 174.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1 (Noise): Prometheus - Used to record detailed information of various operations in components.\n",
      "-1 (Noise): TiDB - TiDB provides statement summary tables similar to MySQL's `events_statements_summary_by_digest` starting from v4.0.0-rc.1 and also provides statement summary tables in `information_schema`.\n",
      "-1 (Noise): Table t - A sample table with columns a, b, c, and d used to illustrate column pruning.\n",
      "-1 (Noise): MySQL - Derived from from relationship: sql_mode -> `sql_mode` controls MySQL compatibility behaviors. -> MySQL\n",
      "-1 (Noise): TiFlash - Derived from from relationship: TiFlash -> TiFlash server uses the security.redact_info_log configuration item to control log redaction. -> security.redact_info_log (TiFlash Server)\n",
      "-1 (Noise): Grafana - A monitoring tool used to visualize TiFlash encryption at rest status.\n",
      "-1 (Noise): TiDB - TiDB interacts with the PD (Placement Driver) component to obtain timestamps (TSOs) and for other cluster management tasks.\n",
      "-1 (Noise): BR - Derived from from relationship: BR -> BR sets the TTL_ENABLE attribute to OFF after restoring data. -> TTL_ENABLE Attribute\n",
      "-1 (Noise): TiDB - TiDB executes root tasks, and root tasks are executed within TiDB.\n",
      "-1 (Noise): MySQL - TiDB's user-defined variable syntax is largely identical to MySQL's, with the exception of the `SELECT ... INTO <variable>` syntax.\n",
      "-1 (Noise): Region - A top-level geographical location in the cluster's topology.\n",
      "-1 (Noise): Region - A physical location, in the context of this document.\n",
      "-1 (Noise): PD - Derived from from relationship: unsafe remove-failed-stores Command -> The `unsafe remove-failed-stores` command interacts with PD to manage and remove failed stores. -> PD\n",
      "-1 (Noise): TiDB - A distributed SQL database compatible with MySQL.  Raft Engine data format compatibility issues exist between versions, and specific procedures are required when downgrading from v5.4.0 or later with Raft Engine enabled.\n",
      "-1 (Noise): MySQL - Derived from from relationship: sync-diff-inspector -> sync-diff-inspector can compare and repair data in MySQL databases. -> MySQL\n",
      "-1 (Noise): Table t - A sample table used in the provided SQL examples, partitioned by hash(x) into 4 partitions, with partition names p0, p1, p2, and p3.\n",
      "-1 (Noise): TiDB - TiDB uses Join Reorder algorithms to optimize query performance, especially for multi-table joins.\n",
      "-1 (Noise): Table - Derived from from relationship: TableFullScan -> TableFullScan performs a full scan of a table. -> Table\n",
      "-1 (Noise): TiDB - A database that can become a data silo if not properly integrated with other platforms.\n",
      "-1 (Noise): TiDB - TiDB supports multiple comment syntaxes.\n",
      "-1 (Noise): Table - A database table that can have TiFlash replicas for analytical queries. Replicas are created and managed using DDL statements, and replication status can be checked using the 'information_schema.tiflash_replica' table.\n",
      "-1 (Noise): Table - Specifies the table name for storing data change records.\n",
      "-1 (Noise): TiDB - Derived from from relationship: TiDB -> TiDB supports BatchPointGet for data retrieval. -> BatchPointGet\n",
      "-1 (Noise): TiDB - Derived from from relationship: tidb_ddl_enable_fast_reorg -> tidb_ddl_enable_fast_reorg affects the performance of adding indexes in TiDB. -> TiDB\n",
      "-1 (Noise): TiDB - TiDB supports authentication using the SM3 algorithm.\n",
      "-1 (Noise): TiKV - Derived from from relationship: TiKV -> TiKV supports encryption at rest using the SM4 algorithm. -> SM4 algorithm\n",
      "-1 (Noise): TiCDC - TiCDC is now compatible with placement rules.\n",
      "-1 (Noise): BR - BR is now compatible with placement rules.\n",
      "-1 (Noise): TiDB - Derived from from relationship: TiDB -> The `new_collations_enabled_on_first_bootstrap` parameter controls the collation rules in TiDB. -> new_collations_enabled_on_first_bootstrap\n",
      "-1 (Noise): TiKV - Derived from from relationship: TiKV -> Leader balancing is performed across TiKV nodes. -> Leader Balancing\n",
      "-1 (Noise): TiDB - Derived from from relationship: TiDB -> TiDB automatically clears the placement rule settings of a table after restoring it using FLASHBACK or RECOVER statements. -> Placement rule settings\n",
      "-1 (Noise): BR - A tool supported by Partitioned Raft KV.\n",
      "-1 (Noise): TiCDC - A tool supported by Partitioned Raft KV.\n",
      "-1 (Noise): TiDB - Derived from from relationship: TiDB -> In v7.1.0, TiDB introduces the `tidb_prefer_broadcast_join_by_exchange_data_size` variable to control MPP Join algorithm selection. -> `tidb_prefer_broadcast_join_by_exchange_data_size`\n",
      "-1 (Noise): TiKV - Derived from from relationship: IndexMerge Query Execution Plan -> An incorrect IndexMerge query execution plan occurs when an expression cannot be pushed down to TiKV. -> TiKV\n",
      "-1 (Noise): PD - Derived from from relationship: Global memory threshold -> PD now supports managing the global memory threshold to alleviate OOM issues. -> PD\n",
      "-1 (Noise): TiFlash - Derived from from relationship: MVCC bitmap filter -> TiFlash now supports an independent MVCC bitmap filter for decoupling MVCC filtering operations. -> TiFlash\n",
      "-1 (Noise): TiFlash - A component of TiDB that supports automatic rotations of TLS certificates.\n",
      "-1 (Noise): PD - PD in TiDB 3.1 Beta supports distributed backup and restore.\n",
      "-1 (Noise): Statistics - Derived from from relationship: Hash partitioned tables -> Statistics for hash partitioned tables are not always updated correctly when partitions are modified. -> Statistics\n",
      "-1 (Noise): TiDB - Derived from from relationship: TiDB -> TiDB prints a warning log for unrecognized configuration options. -> Configuration Option\n",
      "-1 (Noise): Statistics - Data structures used by the query optimizer to estimate the cost of different execution plans. Improved in TiDB 3.0.0-rc.1.\n",
      "-1 (Noise): Table - Derived from from relationship: tidb_indexes -> The `tidb_indexes` system table manages the relationship between Table and Index. -> Table\n",
      "-1 (Noise): TiDB - Derived from from relationship: TiDB -> TiDB now includes schema and table names in the output of the `admin show ddl jobs` statement. -> admin show ddl jobs\n",
      "-1 (Noise): Table - Derived from from relationship: COLLATE -> The table's `COLLATE` is used when creating a table. -> Table\n",
      "-1 (Noise): TiDB - A distributed SQL database compatible with MySQL. Changed the default value of the system variable `tidb_analyze_version` from `2` to `1`.\n",
      "-1 (Noise): Region - Derived from from relationship: tikv-ctl -> A bug fix in TiDB 5.2.4 addresses tikv-ctl not returning correct region-related information. -> Region\n",
      "-1 (Noise): BR - Derived from from relationship: BR -> BR can fail to back up the AUTO_RANDOM ID allocation progress under specific conditions. -> AUTO_RANDOM ID backup failure\n",
      "-1 (Noise): MySQL - Derived from from relationship: Point_Get Plan -> The point_get plan returns a column name that is inconsistent with that of MySQL. -> MySQL\n",
      "-1 (Noise): Grafana - Monitoring component upgraded from V4.6.3 to V6.1.6.\n",
      "-1 (Noise): MySQL - A relational database management system that can be used as a downstream for Reparo.  It can be a destination for Reparo and requires host, port, user, and password configuration.\n",
      "-1 (Noise): MySQL - A relational database management system. Can be a destination for Reparo.\n",
      "-1 (Noise): MySQL - Derived from from relationship: syncer.to -> syncer.to configuration is used when the downstream database is MySQL. -> MySQL\n",
      "-1 (Noise): Table - The specific table on which the job operates.\n",
      "-1 (Noise): TiDB - Derived from from relationship: State -> The 'State' column in TiDB's 'SHOW PROCESSLIST' output is non-descriptive due to parallel query execution. -> TiDB\n",
      "-1 (Noise): Table - A database table from which rows can be deleted using the DELETE statement.\n",
      "-1 (Noise): MySQL - A popular open-source relational database management system. Supports assignment operators like '=' and ':=' but does not support the 'ILIKE' operator.\n",
      "-1 (Noise): TiDB - Derived from from relationship: KILL TIDB -> `KILL TIDB` is used to terminate TiDB server processes. -> TiDB\n",
      "-1 (Noise): TiDB - Derived from from relationship: TiDB -> TiDB supports encryption and compression functions. -> Encryption and Compression Functions\n",
      "-1 (Noise): MySQL - Derived from from relationship: Incorrectly Formatted Date/Time Values -> MySQL often accepts incorrectly formatted date and time values. -> MySQL\n",
      "-1 (Noise): TiDB - A distributed SQL database that can integrate with ProxySQL for query routing. This integration offers benefits such as responding to overloaded databases by rerouting, rewriting, or rejecting queries.  The integration uses container ports 4001 and 4002, with a test table 'test.tidb_server' and test data including 'tidb-server01-port-4001' and 'tidb-server02-port-4002'.\n",
      "-1 (Noise): TiDB - TiDB is a database that can be connected to using Sequelize.\n",
      "-1 (Noise): Table - Database objects that can be added to a database after creation.\n",
      "-1 (Noise): MySQL - Derived from from relationship: COLLATIONS table -> The `COLLATIONS` table is included for compatibility with MySQL. -> MySQL\n",
      "-1 (Noise): Table - Table in the output of tiup dm display command.\n",
      "-1 (Noise): Table - A table belongs to a schema and has a CREATE statement.\n",
      "-1 (Noise): Table - The name of the target table in the configuration.\n",
      "-1 (Noise): TiDB - Derived from from relationship: TiDB -> TiDB calculates the remote checksum after data import. -> Remote Checksum\n",
      "-1 (Noise): Region - A setting in Diag that determines the target Clinic Server and encryption certificate.\n",
      "-1 (Noise): Region - A configurable region setting for Diag.\n",
      "-1 (Noise): Region - The geographical region for data storage and processing.\n",
      "-1 (Noise): PD - Derived from from relationship: PD -> PD has experimental support for encryption at rest. -> Encryption at Rest\n",
      "-1 (Noise): TiKV - Derived from from relationship: TiKV -> TiKV uses `gc.num-threads` to configure the number of GC threads. -> gc.num-threads\n",
      "-1 (Noise): MySQL - Derived from from relationship: MySQL -> MySQL uses local timezone by default and relies on timezone rules in the system, requiring timezone table data import to specify timezone by name. -> Timezone Table Data\n",
      "-1 (Noise): TiDB - Derived from from relationship: TiDB -> TiDB uses Count-Min Sketch for estimating the cardinality of equality and IN queries. -> Count-Min Sketch\n",
      "-1 (Noise): Table - Derived from from relationship: TableRowIDScan -> TableRowIDScan is used to scan a table based on row IDs. -> Table\n",
      "-1 (Noise): TiFlash - A storage engine option for READ_FROM_STORAGE.\n",
      "-1 (Noise): MySQL - The target database for data streaming from TiDB Cloud.\n",
      "-1 (Noise): PD - A component of TiDB, mentioned in the context of direct connection scenarios where VPC peering is recommended.\n",
      "-1 (Noise): MySQL - Derived from from relationship: WordPress -> WordPress uses a MySQL database. -> MySQL\n",
      "-1 (Noise): Table - An existing table in the database to which a column can be added.\n",
      "-1 (Noise): TiDB - The database system in which the ADD COLUMN operation is performed.\n",
      "-1 (Noise): MySQL - Derived from from relationship: MySQL -> MySQL uses ICU for regular expression implementation. -> ICU\n",
      "-1 (Noise): MySQL - A database system that has cast functions and operators, with a specific behavior for casting double-precision floating-point numbers in scientific notation to CHAR that differs from TiDB.\n",
      "-1 (Noise): MySQL - Derived from from relationship: MASTER_POS_WAIT() -> MASTER_POS_WAIT() relates to MySQL replication and is unsupported in TiDB. -> MySQL\n",
      "-1 (Noise): MySQL - A popular open-source relational database management system. Clients and drivers can execute `ALTER TABLE ... COMPACT`.\n",
      "-1 (Noise): Region - The S3 region where the bucket is located.\n",
      "-1 (Noise): TiDB - The database system that implements the SET ROLE statement with compatibility to MySQL 8.0.\n",
      "-1 (Noise): TiKV - Derived from from relationship: TiKV -> The raw-min-ts-outlier-threshold configuration parameter was deleted in TiKV version 6.5.0. -> raw-min-ts-outlier-threshold\n",
      "-1 (Noise): PD - Derived from from relationship: PD -> Supports blocking the Swagger API by default when the Swagger server is not enabled. -> Swagger API\n",
      "-1 (Noise): MySQL - Derived from from relationship: COM_STMT_FETCH -> The COM_STMT_FETCH time record in slow query logs is inconsistent with that in MySQL. -> MySQL\n",
      "-1 (Noise): MySQL - Derived from from relationship: explicit_defaults_for_timestamp -> explicit_defaults_for_timestamp is a session variable in MySQL. -> MySQL\n",
      "-1 (Noise): Region - Region in TiKV had its approximate size and keys count updated after Region merging.\n",
      "-1 (Noise): TiFlash - TiFlash is a component that supports creating node pools on ACK and EKS, and supports scaling.\n",
      "-1 (Noise): TiKV - Derived from from relationship: TiKV -> TiKV supports the encryption-meta command in TiKV Control. -> encryption-meta command\n",
      "-1 (Noise): TiKV - A component in TiDB that returned different results from TiFlash when querying logical operations.\n",
      "-1 (Noise): TiKV - Derived from from relationship: TiKV_space_used_more_than_80% -> The `TiKV_space_used_more_than_80%` alert is about TiKV cluster space usage. -> TiKV\n",
      "-1 (Noise): Table - This entity represents sample tables demonstrating primary key constraints. The 't1 Table' successfully implements a single-column primary key with 'a' as an INT NOT NULL. The 't2 Table' illustrates a failed attempt due to a primary key column being NULL, while the 't3 Table' shows a failure due to defining multiple primary keys.\n",
      "-1 (Noise): Table - This entity represents tables involved in a Cartesian join example and join operations. Table 't1' is the first table involved in the Cartesian join example, performing a full table scan with conditions such as not(isnull(test.t.a)) and eq(test.t1.int_col, 1). Table 't2' is the second table, often used in the probe step of the join operation, with conditions like t2.pad1 = 'value'. Both tables are involved in the join operation and perform full table scans.\n",
      "-1 (Noise): PD - PD supports microservice mode from v8.0.0.\n",
      "-1 (Noise): TiKV - TiKV processes `DECIMAL` arithmetic multiplication truncation, which can lead to inconsistencies with TiDB. A fix in TiKV addressed the decimal operations overflow issue.\n",
      "-1 (Noise): TiDB - TiDB is integrated with Amazon AppFlow for data transfer. It implements the Percolator transaction model and executes root tasks.\n",
      "-1 (Noise): TiDB - A TiDB instance running on 127.0.0.1:4000, used as a database server in the test environment.\n",
      "save cluster iFWJsx6B_iter_1, count 31\n",
      "save cluster grbSNNAY_iter_1, count 60\n",
      "save cluster ycLOCgyU_iter_1, count 29\n",
      "save cluster L0HiZHjN_iter_1, count 16\n",
      "save cluster N8ssyFDY_iter_1, count 42\n",
      "save cluster qlUlRh9g_iter_1, count 8\n",
      "save cluster OxDK2dk9_iter_1, count 57\n",
      "save cluster PpqeOrGJ_iter_1, count 33\n",
      "save cluster z6Vp8pwH_iter_1, count 19\n",
      "save cluster wuetpTY5_iter_1, count 26\n",
      "save cluster WI8jEck8_iter_1, count 23\n",
      "save cluster mESUTR0F_iter_1, count 3\n",
      "save cluster e77eONPE_iter_1, count 18\n",
      "save cluster ZoglYczO_iter_1, count 20\n",
      "save cluster Gt4UtkMP_iter_1, count 3\n",
      "save cluster gNTT5hHo_iter_1, count 4\n",
      "cluster entities finished!\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import string\n",
    "\n",
    "def generate_random_string(length=8):\n",
    "    return ''.join(random.choices(string.ascii_letters + string.digits, k=length))\n",
    "\n",
    "while True:\n",
    "    # entities = aggregator.get_entities(iteration*batch, batch)\n",
    "    names = ['TiKV', 'PD', 'TiCDC', 'MySQL', 'Region', 'TiFlash', 'TiDB', 'Table', 'Table t ', 'Grafana', 'Prometheus', 'BR', 'Statistics']\n",
    "    entities = aggregator.get_entities_by_name(names, batch*iteration, batch)\n",
    "    if len(entities) == 0:\n",
    "        print(\"cluster entities finished!\")\n",
    "        break\n",
    "    iteration += 1\n",
    "\n",
    "    print(\"start iteration\", iteration)\n",
    "\n",
    "    clusters = aggregator.cluster_entities(\n",
    "        entities,\n",
    "        embedding_weight=0.8,\n",
    "        name_weight=0.2,\n",
    "        desc_weight=0, \n",
    "        similarity_threshold=0.75\n",
    "    )\n",
    "    for cluster in clusters:\n",
    "        random_str = generate_random_string()\n",
    "        cluster_name = f\"{random_str}_iter_{iteration}\"\n",
    "        for e in cluster:\n",
    "            clusters_info.append(\n",
    "                {\n",
    "                    'cluster': cluster_name,\n",
    "                    'entity_id': e.id,\n",
    "                    'entity_name': e.name,\n",
    "                    'entity_description': e.description,\n",
    "                    'entity_metadata': e.meta\n",
    "                }\n",
    "            )\n",
    "        print(f\"save cluster {cluster_name}, count {len(cluster)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster: iFWJsx6B_iter_1\n",
      " - ID: 420239, Name: Region, Description: A geographical location where TiDB clusters are deployed, influencing factors such as availability zones, data processing costs, default domains, and cluster creation processes.\n",
      "   - Metadata: {'details': {'influence': 'Affects the cost of data processing based on the location of the secondary cluster relative to the primary cluster'}, 'example': {'Region1': ['AZ1', 'AZ2'], 'Region2': ['AZ3', 'AZ4'], 'Region3': ['AZ5']}, 'labels': ['Region', 'AZ'], 'linked_entities': ['Default Domain'], 'properties': [{'property': 'relation', 'value': 'Determines the default domain'}, {'property': 'Selection', 'value': 'Chosen during cluster creation on the Create Cluster Page'}], 'topic': ['location', 'Data Processing Cost', 'Deployment', 'Region']}\n",
      " - ID: 35984, Name: Region, Description: A geographical area used to group stores, with four regions: North, East, West, and Central.  This grouping is used for partitioning employee data and influences partition naming.\n",
      "   - Metadata: {'properties': [{'property': 'Values', 'value': ['North', 'East', 'West', 'Central']}, {'property': 'Purpose', 'value': 'Group stores and consequently partition employee data'}, {'property': 'Association', 'value': 'Each region is associated with a specific set of Store IDs'}, {'property': 'Partition Naming', 'value': 'Used to name partitions (e.g., pNorth, pEast)'}], 'topic': 'Region'}\n",
      " - ID: 61082, Name: Region, Description: A unit of data storage in TiKV. Hotspot data concentration on a single Region can affect performance.\n",
      "   - Metadata: {'properties': [{'topic': 'Location', 'value': 'TiKV'}, {'topic': 'Performance Impact', 'value': 'Hotspot data concentration can affect performance'}], 'topic': 'Region'}\n",
      " - ID: 3099, Name: Region, Description: A contiguous range of data within TiKV, similar to a shard in other databases.  Regions are allocated by PD during restore and the information within them is fetched by BR during backup.  Balance Region movement is managed by the Balance Region scheduler. Region heartbeat reports are sent via the Region schedule push mechanism.  A Region is also associated with a Syncer Index and a history last index.\n",
      "   - Metadata: {'properties': [{'item': 'Balance Region movement', 'relation': 'managed by'}, {'item': 'Balance Region scheduler', 'relation': 'property of'}, {'item': 'Region heartbeat report', 'relation': 'reported via'}, {'item': 'Region schedule push', 'relation': 'scheduled via'}, {'item': 'Syncer Index', 'relation': 'associated with'}, {'item': 'history last index', 'relation': 'associated with'}, 'Contiguous range of data in TiKV', 'Information fetched by BR during backup', 'Allocated by PD during restore'], 'topic': 'Region'}\n",
      " - ID: 1308, Name: Region, Description: A range of data in TiKV.  Represents a contiguous range of data managed by TiKV. Specific issues related to Region merging include panics and unexpected peer destruction due to invalid target Regions (ID: 12232), and panic issues arising from uninitialized peer replacements during merges (ID: 12048).\n",
      "   - Metadata: {'TiKV bug fixes': [{'id': '12232', 'topic': 'panics and destroys peers unexpectedly because the target Region to be merged is invalid'}, {'id': '12048', 'topic': 'panic issue that occurs when the target peer is replaced with the peer that is destroyed without being initialized when merging a Region'}], 'topic': 'Region'}\n",
      " - ID: 61594, Name: Region, Description: A geographical area where TiKV nodes are located, used for disaster recovery.\n",
      "   - Metadata: {'properties': [{'key': 'Disaster Recovery', 'value': 'Used for disaster recovery'}, {'key': 'Number', 'value': '3 in the example cluster (us-east-1, us-east-2, us-west-1)'}, {'key': 'Contains Zones', 'value': 'Each region contains 3 zones'}, {'key': 'Placement Policy', 'value': \"Considered in placement policies like 'multiaz' for cross-region data isolation\"}], 'topic': 'Region'}\n",
      " - ID: 32544, Name: Region, Description: A unit of data storage in TiKV, typically 96 MB in size. It serves as a basic unit of data storage and is also considered a basic unit in TiKV.  Unhealthy regions (issue #5491) can lead to PD panic.\n",
      "   - Metadata: {'details': 'Basic unit in TiKV', 'issue_number': '5491', 'leads_to': 'PD panic', 'status': 'need-revised', 'topic': 'Region'}\n",
      " - ID: 54818, Name: Region, Description: A basic unit of data storage in TiKV. It serves as a unit of data storage and influences parallel import speed, which is limited by the number of copies and the size of the target cluster.\n",
      "   - Metadata: {'properties': ['Unit of data storage in TiKV'], 'relation': 'import speed limited by the number of copies and the size of the target cluster', 'status': 'need-revised', 'topic': 'Storage Unit'}\n",
      " - ID: 38947, Name: Region, Description: A data partition in TiDB.\n",
      "   - Metadata: {'related_to': 'TiFlash', 'restore_operation': 'TiDB', 'topic': 'Data Distribution'}\n",
      " - ID: 43428, Name: Region, Description: A basic unit of data storage in TiKV. Used for balancing MPP query workload among different TiFlash nodes and invalidating stale data in the cache after MPP query execution. The `health-check` output of a Region can be inconsistent with the Region information returned by querying the Region ID.  Region fetched from PD may not have a Leader when restoring data using BR or importing data using TiDB Lightning in physical import mode.\n",
      "   - Metadata: {'TiDB Improvements': {'stale invalidation': 'supported in MPP queries in TiFlash', 'used for': ['balancing MPP query workload among different TiFlash nodes', 'invalidating stale data in the cache after MPP query execution']}, 'TiKV Concept': {'description': 'Basic unit of data storage'}, 'component': 'TiKV', 'details': [{'issue': 'Region fetched from PD does not have a Leader when restoring data using BR or importing data using TiDB Lightning in physical import mode', 'related_issues': ['#51124', '#50501'], 'subtopic': 'Bug Fixes'}, {'description': 'The `health-check` output of a Region is inconsistent with the Region information returned by querying the Region ID', 'subtopic': 'Health Check'}], 'status': 'need-revised', 'topic': 'Region Management'}\n",
      " - ID: 60336, Name: Region, Description: A unit of data storage in TiFlash.\n",
      "   - Metadata: {'status': 'need-revised'}\n",
      " - ID: 62641, Name: Region, Description: A basic unit of data storage and replication in TiKV. Before Follower Read, only the leader node of a Region could handle read requests. Enabling Follower Read allows follower nodes to handle read requests using the ReadIndex mechanism.\n",
      "   - Metadata: {'description': 'Before Follower Read, only the leader node of a Region could handle read requests.', 'details': {'Follower Read': 'Enables follower nodes to handle read requests using the ReadIndex mechanism.'}, 'status': 'need-revised', 'topic': 'Read Request Handling'}\n",
      " - ID: 35380, Name: Region, Description: A data range in TiDB that becomes unavailable when most or all of its replicas go offline.\n",
      "   - Metadata: {'availability': 'dependent on replica availability', 'topic': 'Data Range', 'unavailable when': 'most or all replicas are offline'}\n",
      " - ID: 38968, Name: Region, Description: A unit of data storage and distribution in TiFlash. Data not matching any region range can remain on a TiFlash node.\n",
      "   - Metadata: {'affected_by': ['range keys'], 'description': 'Data not matching any region range can remain on a TiFlash node', 'link': 'https://github.com/pingcap/tiflash/issues/4414', 'topic': 'data mismatch'}\n",
      " - ID: 36025, Name: Region, Description: A geographical area that groups multiple stores. Used to categorize stores for partitioning.\n",
      "   - Metadata: {'properties': {'Example Values': ['North', 'East', 'West', 'Central'], 'Purpose': 'Groups multiple stores geographically', 'Stores per Region': 5, 'Use in List Partitioning': 'Categorizes stores for partitioning'}, 'topic': 'Region'}\n",
      " - ID: 61121, Name: Region, Description: The smallest unit of data splitting for TTL jobs in TiDB.\n",
      "   - Metadata: {'context': 'TTL Jobs', 'topic': 'Data Splitting Unit'}\n",
      " - ID: 362308, Name: Region, Description: A Region is a fundamental unit of data storage, management, and scheduling in TiKV and TiDB. It represents a contiguous range of key-value pairs and serves as a logical division of data, similar to a shard or partition in other databases. Regions are distributed among multiple TiKV instances to facilitate data distribution, fault tolerance, and high availability. They are managed by the Placement Driver (PD) and can be dynamically split or merged to balance load, improve performance, and manage data distribution across the cluster. Regions are replicated using the Raft consensus algorithm, typically with three replicas located in different data centers, to ensure data consistency and fault tolerance. They have a default size limit of 96 MiB, which is configurable, and are involved in operations like leader election, data replication, and caching. Writing to a Region invalidates its coprocessor cache, and large Regions can cause TiKV Out Of Memory (OOM) issues. Regions are crucial for efficient data access and management, with properties such as REGION_ID, START_KEY, END_KEY, and others.\n",
      "   - Metadata: {'Bug Fixes': ['Scatter range scheduler unable to schedule empty regions'], 'Configuration': 'split-table', 'Creation trigger': 'New table creation', 'PD': ['Hot Region balance scheduling for TiFlash write hotspot', 'More QPS dimensions and adjustable priority for hot Region scheduling'], 'States': ['restarted', 'newly split'], 'TiCDC': ['Resolve lock issue after Region initialization'], 'TiDB': ['MPP workload balancing among TiFlash nodes based on Regions', 'Invalidating stale Regions in MPP query cache'], 'TiFlash': ['Automatic invalidation of stale Regions in MPP queries'], 'actions': ['send heartbeats', 'execute operators (optional)'], 'affected_by': 'shard_row_id_bits', 'analogy': 'shard in other databases', 'attributes': ['id', 'peer_stores'], 'bug_fixes': [{'github_issue': '15919', 'issue': 'stale peers are retained and block resolved-ts after Regions are merged', 'related_entities': ['resolved-ts', 'Stale peers']}, {'github_issue': '13311', 'issue': 'TiKV panics due to inconsistent metadata between Regions', 'related_entities': ['TiKV', 'Inconsistent metadata']}], 'cache_invalidation': 'Writing to a Region invalidates its cache', 'caching': 'Coprocessor Cache operates at Region level', 'characteristics': ['Sharded data', 'Key range based splitting', 'Automatic splitting upon exceeding size threshold'], 'contains': ['Row Data', 'Index Data'], 'count': 'Managed by PD', 'count_after_split': 'ten (five per partition - four row data, one index data)', 'creation': 'A separate Region is created for each newly created table when the `split-table` configuration option is enabled', 'database': 'TiDB', 'description': 'Replicated across multiple stores for redundancy and availability.', 'details': [{'contributor': '5kbpers', 'issue': 'Might be overlapped if Raftstore is busy', 'link': 'https://github.com/tikv/tikv/issues/13160'}, {'contributor': '5kbpers', 'issue': 'Inconsistent size configuration with PD', 'link': 'https://github.com/tikv/tikv/issues/12518'}, 'PD might panic due to empty Regions obtained internally.', {'condition': 'if Raftstore is busy', 'description': 'fixed potential overlap issue'}, \"Limited number of regions for small tables can hinder TiDB's concurrency strength.\", 'Continuous range of data in TiKV', 'Tables are split into multiple regions', 'Used for distributed storage and processing', 'Sharded data', 'Key range based splitting', 'Automatic splitting upon exceeding size threshold'], 'distribution': 'Distributed across stores based on size and other factors', 'distribution_management': 'PD', 'effect': 'stale peers are retained and block resolved-ts', 'end_key': 'Ending key of the region', 'example': ['102', '106', '110', '114', '3', '98'], 'format': '[StartKey, EndKey)', 'forms': 'Raft group', 'function': 'Basic unit of data storage, movement, and replication in TiKV and TiFlash.', 'health_check_command': 'tiup cluster check <cluster-name> --cluster', 'identified_by': 'Region ID', 'improvements': ['batch split command', 'empty split command'], 'information_available_via': 'tikv-ctl (using `get-region-read-progress` command)', 'initial_number': 'Single region for new tables', 'invalidation': 'Writing to a Region invalidates its cache', 'issue': ['TiCDC process might panic when TiKV sends duplicate requests to the same Region'], 'issue_id': '14547', 'issues': [{'issue': 'Querying a region without a leader might cause PD to panic', 'references': 'https://github.com/tikv/pd/issues/7630', 'solution': 'Fixed in TiDB 7.1.4'}, {'description': 'Outdated information causing SQL errors', 'issue': '12934'}], 'leader': 'Balanced across stores', 'leader_distribution': 'Leaders distributed across different stores', 'location': 'TiKV', 'managed by': 'TiKV', 'managed_by': 'Placement Rules', 'management': 'Scheduled by PD', 'matching': 'Rules are matched to Regions based on key range', 'merging': \"Determined by PD's MergeChecker and QPS in heartbeat information\", 'operator_execution': 'Receives and can skip scheduling operators', 'p1_split_count': '2', 'p1_split_range': '[0, 10000]', 'p2_split_count': '2', 'p2_split_range': '[10000, 20000]', 'parameters': ['region-max-size', 'region-max-keys'], 'properties': {'basic unit of data movement': True, 'data stored in RocksDB': True, 'leader': True, 'merge threshold is 20MB (default)': True, 'multiple replicas': True, 'multiple replicas form a Raft group': True, 'replicated across multiple nodes': True, 'represents a range of data in a Store': True, 'split threshold is 144MB (default)': True, 'typical size is 96MB (default)': True}, 'properties_reported': ['safe_ts', 'applied_index', 'resolved_ts', 'number of locks', 'number of transactions'], 'purpose': 'Key-value data movement and storage, redundancy, and availability', 'related_issue': 'scatter range scheduler cannot schedule empty regions', 'related_issues': ['panic when merging with ConfChange and Snapshot', 'incorrect information returned by tikv-ctl', 'invalid target Region causing panic'], 'relation_to_shard_row_id_bits': 'data distributed across multiple regions using SHARD_ROW_ID_BITS', 'relevance': 'Large regions can cause TiKV OOM', 'replica_count': 'three', 'replication': 'Each Region has multiple replicas', 'replication_method': 'Raft', 'reported_information': ['Number of Regions (by TiKV Peer)', 'Data read/write speed (by TiKV Peer)', 'Position of leader and replicas (by Region Leader)', 'Number of offline replicas (by Region Leader)', 'Data read/write speed (by Region Leader)'], 'representation': 'key range (left-closed, right-open interval)', 'role': 'Unit of data storage in TiKV', 'scattering': 'Distribution across different nodes', 'scattering_column': \"Indicates scattering status in 'SHOW TABLE REGIONS' output\", 'scheduling': 'Managed by PD', 'scope': 'Store', 'size': {'default': '96 MiB', 'merge_threshold': '20 MiB', 'split_threshold': '144 MiB'}, 'size_limit': '96 MiB (configurable)', 'source': 'https://github.com/tikv/pd/pull/4118', 'split_command': 'SPLIT', 'split_count_example': '2', 'split_range_example': '[1000, 10000]', 'splitting': ['Dynamically split based on size', 'Manual splitting (previous approach)', 'Automatic splitting via Load Base Split'], 'start_key': 'Starting key of the region', 'status': 'need-revised', 'status_check': \"Can be checked using 'SHOW TABLE REGIONS' command\", 'statuses': ['healthy', 'miss-peer', 'pending-peer'], 'storage': 'TiKV', 'storage_layer': 'TiKV', 'storage_method': 'Independent RocksDB instance in Partitioned-Raft-KV', 'stored_in': 'TiKV', 'subtopics': ['Availability', 'bug fixes'], 'topic': ['Region in TiKV', 'Data Storage Unit', 'Data Storage', 'Data Storage and Movement', 'Region Characteristics', 'Data Unit', 'Region', 'Region Distribution'], 'trigger': 'TiKV node failure', 'used_in': 'TiKV', 'view_command': 'SHOW TABLE REGIONS'}\n",
      " - ID: 2245, Name: Region, Description: A geographic area containing multiple availability zones.\n",
      "   - Metadata: {'contains': 'multiple availability zones', 'topic': 'Geographic Area'}\n",
      " - ID: 63173, Name: Region, Description: A region is a geographical area where AWS resources are located.\n",
      "   - Metadata: {'properties': ['AWS DMS and TiDB cluster are recommended to be in the same region for better performance'], 'topic': 'Region'}\n",
      " - ID: 68426, Name: Region, Description: A unit of data storage in TiDB. Regions can be pre-split during table creation and are distributed after splitting.  Merging is controlled by a scheduler in PD.  For example, a table might have 4 regions for data and 1 for index, covering ranges like [-inf, 1<<61), [1<<61, 2<<61), [2<<61, 3<<61), and [3<<61, +inf).\n",
      "   - Metadata: {'merging': 'Controlled by a scheduler in PD', 'numberForData': '4 (in example)', 'numberForIndex': '1 (in example)', 'purpose': 'Unit of data storage', 'ranges': ['[-inf, 1<<61)', '[1<<61, 2<<61)', '[2<<61, 3<<61)', '[3<<61, +inf)'], 'scattering': 'Distribution of regions after splitting', 'splitting': 'Can be pre-split during table creation', 'status': 'need-revised', 'topic': 'Data Storage'}\n",
      " - ID: 56267, Name: Region, Description: A unit of data storage in TiKV.  The distribution of traffic across regions is visualized in the heatmap.\n",
      "   - Metadata: {'function': 'Unit of data storage in TiKV', 'hotspot_relation': 'Can become hotspots with concentrated traffic', 'topic': 'Region', 'visualization': 'Traffic distribution visualized in heatmap'}\n",
      " - ID: 361292, Name: Region, Description: A Region is a contiguous range of data within a table, partition, or index. It is managed as a unit for data management, distribution, and storage. Multiple Regions can exist within a single partition, facilitating efficient data handling.\n",
      "   - Metadata: {'attributes/properties': ['REGION_ID', 'START_KEY', 'END_KEY', 'LEADER_ID', 'LEADER_STORE_ID', 'PEERS', 'SCATTERING', 'WRITTEN_BYTES', 'READ_BYTES', 'APPROXIMATE_SIZE(MB)', 'APPROXIMATE_KEYS', 'SCHEDULING_CONSTRAINTS', 'SCHEDULING_STATE'], 'characteristic': 'Continuous range', 'location': 'Within a table/index/partition', 'number_after_split': 'ten Regions, each partition with five Regions, four of which are the row data and one is the index data', 'split_command': 'SPLIT', 'split_example_even': 'SPLIT [PARTITION] TABLE t [PARTITION] [(partition_name_list...)] [INDEX index_name] BETWEEN (lower_value) AND (upper_value) REGIONS region_num', 'split_example_uneven': 'SPLIT [PARTITION] TABLE table_name [PARTITION (partition_name_list...)] [INDEX index_name] BY (value_list) [, (value_list)] ...', 'status': 'need-revised', 'topic': ['Data Storage/Range/Region', 'Region Information'], 'view_command': 'SHOW TABLE REGIONS', 'viewable_via': 'SHOW TABLE REGIONS statement'}\n",
      " - ID: 390220, Name: Region, Description: A basic data storage unit in TiKV. Backups are managed through processes where TiKV handles backing up regions and provides information to BR. Backup progress is tracked using region checkpoint timestamps. Each Region generates backup files on its Leader node, with pre-splitting also handled by BR.\n",
      "   - Metadata: {'backup_process': ['Backed up by TiKV', 'Backup information returned to BR'], 'backup_tracking': 'Region Checkpoint Timestamp', 'id': 'REGION_ID', 'log_query_method': \"searching logs according to the 'region id'\", 'properties': [{'property': 'backup files', 'value': 'generated on Leader node of each Region'}, {'property': 'pre-splitting', 'value': 'backed up and restored by BR'}], 'topic': ['Region', 'Data Storage Unit']}\n",
      " - ID: 66006, Name: Region, Description: A geographical area where cloud resources are located.\n",
      "   - Metadata: {'details': {'condition': 'IAM role created with Account ID and External ID', 'restriction': 'Only TiDB clusters within the same region can access the S3 bucket.'}, 'topic': 'Context for Access Restriction'}\n",
      " - ID: 52827, Name: Region, Description: The basic unit of data distribution in TiDB.  It has multiple replicas (leader and followers), and data changes on the leader are synchronously updated to followers.\n",
      "   - Metadata: {'details': ['Has multiple replicas (leader and followers).', 'Data changes on the leader are synchronously updated to followers.'], 'properties': ['Basic unit of data distribution', 'Has multiple replicas (leader and followers)', 'Data changes on leader are synchronously updated to followers'], 'topic': 'Data Distribution and Region Properties'}\n",
      " - ID: 56028, Name: Region, Description: A Region is the basic unit of data storage and scheduling in TiDB and TiKV. It represents a contiguous range of keys and their associated values stored in a Store (in TiKV).  Regions are replicated across multiple nodes (TiKV peers) for fault tolerance and handle data for a specific key range.  A leader Region reports RegionState to PD.\n",
      "   - Metadata: {'details': ['Replicated across multiple TiKV peers.', 'Has a leader that reports RegionState to PD.', 'Basic unit of data movement in TiKV.', 'Basic unit of scheduling in TiDB and TiKV.'], 'status': 'need-revised', 'topic': 'Data Storage Unit'}\n",
      " - ID: 30686, Name: Region, Description: A contiguous block of data in a distributed database.\n",
      "   - Metadata: {'details': ['REGION', 'REGIONS', 'PRE_SPLIT_REGIONS'], 'keywords': ['REGION', 'REGIONS', 'PRE_SPLIT_REGIONS'], 'topic': 'Region-related Keywords'}\n",
      " - ID: 33379, Name: Region, Description: A unit of data storage in TiDB that splits after reaching a certain size.\n",
      "   - Metadata: {'merging': 'Controllable via table attributes', 'splitting': 'Splits after reaching a certain size', 'topic': 'data_storage'}\n",
      " - ID: 44136, Name: Region, Description: A unit of data storage in TiDB. Transactions involving only one Region benefit from one-phase commit.\n",
      "   - Metadata: {'description': 'A unit of data storage in TiDB.', 'details': {'1PC benefit': 'Transactions involving only one Region benefit from one-phase commit.'}, 'topic': 'Region'}\n",
      " - ID: 362741, Name: Region, Description: A Region is a basic unit of data storage and distribution in the TiDB ecosystem, specifically within TiKV. It is managed by the Placement Driver (PD) and the Raft consensus algorithm. Regions are crucial for data partitioning and distribution, and they can become unavailable due to various issues. They are involved in data replication processes, such as applying Region snapshots and ingesting SST files. During restore operations, metadata is read from each TiKV, and a leader is selected for each Region using a leader selection algorithm.\n",
      "   - Metadata: {'details': {'function': 'Data partition in TiKV.', 'issues': 'Can become unavailable due to various issues.'}, 'features': ['Add more metrics for Region and store heartbeat', 'Add Grafana panels for data replication (`apply Region snapshots` and `ingest SST files`)'], 'issues_fixed': ['Region stuck in multi-time merging', 'Exception when resolving locks for Regions', 'Check for Region meta for correct Region Split/Region Merge'], 'properties': [{'property': 'Metadata', 'value': 'Read by BR from each TiKV during restore'}, {'property': 'Leader Selection', 'value': 'BR selects the leader of each Region with a leader selection algorithm'}], 'status': 'need-revised', 'subtopic': ['Distribution fetched from PD by BR'], 'topic': ['Region', 'storage', 'Bug Fixes', 'Data Partitioning', 'Data Distribution Unit']}\n",
      " - ID: 390267, Name: Region, Description: In TiKV, a Region is a contiguous segment of the key-value space, represented by a left-closed and right-open interval [StartKey, EndKey). It serves as the basic unit for data distribution and Raft replication. A Region consists of a series of adjacent keys and is replicated using the Raft algorithm to form a Raft Group.\n",
      "   - Metadata: {'context': 'TiKV', 'definition': 'a segment of the key-value space, consisting of a series of adjacent keys', 'details': ['Consecutive segment of Key-Value pairs.', 'Represented by [StartKey, EndKey).', 'Default size limit is 96 MiB (configurable)', 'Basic unit for data distribution and Raft replication.'], 'further_information': 'TiDB Internal (I) - Data Storage (https://www.pingcap.com/blog/tidb-internal-data-storage/)', 'location': 'TiKV', 'note': 'Incorrect key counting in some cases', 'properties': ['basic unit of data distribution in TiKV', 'represents a range of keys', 'replicated using Raft algorithm', 'multiple replicas form a Raft Group'], 'topic': ['Characteristics', 'Key Range', 'Region']}\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "from entity_agg import merge_entities, group_mergeable_entities\n",
    "\n",
    "cluster_mapping = {}\n",
    "for row in clusters_info:\n",
    "    cluster_name = row['cluster']\n",
    "    entity = aggregator._entity_model(\n",
    "        id=row['entity_id'],\n",
    "        name=row['entity_name'],\n",
    "        description=row['entity_description'],\n",
    "        meta=row['entity_metadata']\n",
    "    )\n",
    "    \n",
    "    if cluster_name not in cluster_mapping:\n",
    "        cluster_mapping[cluster_name] = set()\n",
    "    \n",
    "    cluster_mapping[cluster_name].add(entity)\n",
    "\n",
    "if cluster_mapping:\n",
    "    first_cluster = next(iter(cluster_mapping))\n",
    "    print(f\"Cluster: {first_cluster}\")\n",
    "    for entity in cluster_mapping[first_cluster]:\n",
    "        print(f\" - ID: {entity.id}, Name: {entity.name}, Description: {entity.description}\")\n",
    "        print(f\"   - Metadata: {entity.meta}\")\n",
    "\n",
    "print(len(cluster_mapping))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merge entities cluster iFWJsx6B_iter_1, count 31\n",
      "prompt token 7390\n",
      "merge entities cluster grbSNNAY_iter_1, count 60\n",
      "prompt token exceeds 16384 33293\n",
      "prompt token exceeds 20000, reduced to 30\n",
      "prompt token exceeds 16384 22432\n",
      "prompt token exceeds 20000, reduced to 15\n",
      "prompt token 10635\n",
      "merge entities cluster ycLOCgyU_iter_1, count 29\n",
      "prompt token 4912\n",
      "merge entities cluster L0HiZHjN_iter_1, count 16\n",
      "prompt token 3534\n",
      "merge entities cluster N8ssyFDY_iter_1, count 42\n",
      "prompt token exceeds 16384 19662\n",
      "prompt token exceeds 20000, reduced to 21\n",
      "prompt token 9097\n",
      "merge entities cluster qlUlRh9g_iter_1, count 8\n",
      "prompt token 2106\n",
      "merge entities cluster OxDK2dk9_iter_1, count 57\n",
      "prompt token 8511\n",
      "merge entities cluster PpqeOrGJ_iter_1, count 33\n",
      "prompt token 6018\n",
      "merge entities cluster z6Vp8pwH_iter_1, count 19\n",
      "prompt token 3329\n",
      "merge entities cluster wuetpTY5_iter_1, count 26\n",
      "prompt token 7428\n",
      "merge entities cluster WI8jEck8_iter_1, count 23\n",
      "prompt token 5200\n",
      "[ERROR in group_mergeable_entities]: No valid JSON array found in the response. </think>\n",
      "\n",
      "```json\n",
      "{\n",
      "    \"groups\": [\n",
      "        {\n",
      "            \"ids\": [94218, 362899, 50461, 46632, 50733, 63407, 56245, 56757, 94133, 362307, 93385, 56526, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574, 362574,\n",
      "merge entities cluster mESUTR0F_iter_1, count 3\n",
      "prompt token 744\n",
      "merge entities cluster e77eONPE_iter_1, count 18\n",
      "prompt token 1959\n",
      "merge entities cluster ZoglYczO_iter_1, count 20\n",
      "prompt token 5652\n",
      "merge entities cluster Gt4UtkMP_iter_1, count 3\n",
      "prompt token 645\n",
      "merge entities cluster gNTT5hHo_iter_1, count 4\n",
      "prompt token 744\n"
     ]
    }
   ],
   "source": [
    "from llm_inference.base import LLMInterface\n",
    "\n",
    "new_clusters_info = []\n",
    "llm_client = LLMInterface(\"ollama\", \"deepseek-qwen-32b\")\n",
    "\n",
    "for cluster_name, entities in cluster_mapping.items():\n",
    "    print(f\"merge entities cluster {cluster_name}, count {len(entities)}\")\n",
    "\n",
    "    processed_entities = entities\n",
    "    while True:\n",
    "        token_count = merge_entities(llm_client, processed_entities, only_count_token=True)\n",
    "        if token_count <= 16384:\n",
    "            break\n",
    "        print(\"prompt token exceeds 16384\", token_count)\n",
    "        processed_entities = set(list(processed_entities)[:len(processed_entities)//2])\n",
    "        print(\"prompt token exceeds 20000, reduced to\", len(processed_entities))\n",
    "\n",
    "    model_args = {}\n",
    "    if token_count > 7000:\n",
    "        model_args[\"options\"]={\n",
    "            \"num_ctx\": token_count+1500,\n",
    "            \"num_gpu\": 80,\n",
    "            \"num_predict\": 8192,\n",
    "            \"temperature\": 0.1,\n",
    "        }\n",
    "    else:\n",
    "        model_args[\"options\"]={\n",
    "            \"num_ctx\": 8192,\n",
    "            \"num_gpu\": 80,\n",
    "            \"num_predict\": 8192,\n",
    "            \"temperature\": 0.1,\n",
    "        }\n",
    "\n",
    "    print(\"prompt token\", token_count)\n",
    "    try:\n",
    "        merged_group =  group_mergeable_entities(llm_client, processed_entities, **model_args)\n",
    "        cluster_idx = 0\n",
    "        for entities in merged_group:\n",
    "            cluster_idx += 1\n",
    "            new_cluster_name = f\"{cluster_name}_idx{cluster_idx}\"\n",
    "            for e in entities:\n",
    "                new_clusters_info.append(\n",
    "                    {\n",
    "                        'cluster': new_cluster_name,\n",
    "                        'entity_id': e.id,\n",
    "                        'entity_name': e.name,\n",
    "                        'entity_description': e.description,\n",
    "                        'entity_metadata': e.meta\n",
    "                    }\n",
    "                )\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing cluster {cluster_name}: {e}\", exc_info=True)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_clusters_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "cluster_info_df = pd.DataFrame(new_clusters_info)\n",
    "cluster_info_df['processed'] = False\n",
    "cluster_info_df.to_pickle(\"cluster_entities.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cluster               142\n",
       "entity_id             142\n",
       "entity_name           142\n",
       "entity_description    142\n",
       "entity_metadata       142\n",
       "processed             142\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_info_df.count()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graph",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
