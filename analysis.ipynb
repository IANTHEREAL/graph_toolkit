{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from entity_agg import EntityAggregator\n",
    "from setting.db import SessionLocal\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "session = SessionLocal()\n",
    "aggregator = EntityAggregator(session, \"entities_150001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "iteration = 0\n",
    "batch = 5000\n",
    "clusters_info = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import string\n",
    "\n",
    "def generate_random_string(length=8):\n",
    "    return ''.join(random.choices(string.ascii_letters + string.digits, k=length))\n",
    "\n",
    "while True:\n",
    "    # entities = aggregator.get_entities(iteration*batch, batch)\n",
    "    entities = aggregator.get_entities_by_name_groups(5, batch*iteration, batch)\n",
    "    if len(entities) == 0:\n",
    "        print(\"cluster entities finished!\")\n",
    "        break\n",
    "    iteration += 1\n",
    "\n",
    "    print(\"start iteration\", iteration)\n",
    "\n",
    "    clusters = aggregator.cluster_entities(\n",
    "        entities,\n",
    "        embedding_weight=0.8,\n",
    "        name_weight=0.2,\n",
    "        desc_weight=0, \n",
    "        similarity_threshold=0.75\n",
    "    )\n",
    "    for cluster in clusters:\n",
    "        random_str = generate_random_string()\n",
    "        cluster_name = f\"{random_str}_iter_{iteration}\"\n",
    "        for e in cluster:\n",
    "            clusters_info.append(\n",
    "                {\n",
    "                    'cluster': cluster_name,\n",
    "                    'entity_id': e.id,\n",
    "                    'entity_name': e.name,\n",
    "                    'entity_description': e.description,\n",
    "                    'entity_metadata': e.meta\n",
    "                }\n",
    "            )\n",
    "        print(f\"save cluster {cluster_name}, count {len(cluster)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from entity_agg import merge_entities, group_mergeable_entities\n",
    "\n",
    "cluster_mapping = {}\n",
    "for row in clusters_info:\n",
    "    cluster_name = row['cluster']\n",
    "    entity = aggregator._entity_model(\n",
    "        id=row['entity_id'],\n",
    "        name=row['entity_name'],\n",
    "        description=row['entity_description'],\n",
    "        meta=row['entity_metadata']\n",
    "    )\n",
    "    \n",
    "    if cluster_name not in cluster_mapping:\n",
    "        cluster_mapping[cluster_name] = set()\n",
    "    \n",
    "    cluster_mapping[cluster_name].add(entity)\n",
    "\n",
    "if cluster_mapping:\n",
    "    first_cluster = next(iter(cluster_mapping))\n",
    "    print(f\"Cluster: {first_cluster}\")\n",
    "    for entity in cluster_mapping[first_cluster]:\n",
    "        print(f\" - ID: {entity.id}, Name: {entity.name}, Description: {entity.description}\")\n",
    "        print(f\"   - Metadata: {entity.meta}\")\n",
    "\n",
    "print(len(cluster_mapping))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_inference.base import LLMInterface\n",
    "\n",
    "splitting_clusters_info = {}\n",
    "llm_client = LLMInterface(\"ollama\", \"deepseek-qwen-32b\")\n",
    "\n",
    "idx = 0\n",
    "def split_entities_by_token(cluster_name, large_entities):\n",
    "    global idx\n",
    "    if len(large_entities) == 0:\n",
    "        return\n",
    "\n",
    "    token_count = merge_entities(llm_client, large_entities, only_count_token=True)\n",
    "    if token_count <= 16384:\n",
    "        idx += 1\n",
    "        splitting_clusters_info[f\"{cluster_name}_idx{idx}\"] = large_entities\n",
    "        return\n",
    "    \n",
    "    if len(large_entities) == 2:\n",
    "        left_group = set(list(large_entities)[:1])\n",
    "        right_group = set(list(large_entities)[1:])\n",
    "    else:\n",
    "        split_point = len(large_entities)//2\n",
    "        left_group = set(list(large_entities)[:split_point])\n",
    "        right_group = set(list(large_entities)[split_point:])\n",
    "\n",
    "    split_entities_by_token(cluster_name, left_group)\n",
    "    split_entities_by_token(cluster_name, right_group)\n",
    "\n",
    "\n",
    "for cluster_name, entities in cluster_mapping.items():\n",
    "    print(f\"merge entities cluster {cluster_name}, count {len(entities)}\")\n",
    "    idx = 0\n",
    "    split_entities_by_token(cluster_name, entities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_inference.base import LLMInterface\n",
    "\n",
    "new_clusters_info = []\n",
    "llm_client = LLMInterface(\"ollama\", \"deepseek-qwen-32b\")\n",
    "\n",
    "for cluster_name, entities in splitting_clusters_info.items():\n",
    "    print(f\"merge entities cluster {cluster_name}, count {len(entities)}\")\n",
    "\n",
    "    token_count = merge_entities(llm_client, entities, only_count_token=True)\n",
    "    model_args = {}\n",
    "    if token_count > 7000:\n",
    "        model_args[\"options\"]={\n",
    "            \"num_ctx\": token_count+1500,\n",
    "            \"num_gpu\": 60,\n",
    "            \"num_predict\": 8192,\n",
    "            \"temperature\": 0.1,\n",
    "        }\n",
    "    else:\n",
    "        model_args[\"options\"]={\n",
    "            \"num_ctx\": 8192,\n",
    "            \"num_gpu\": 60,\n",
    "            \"num_predict\": 8192,\n",
    "            \"temperature\": 0.1,\n",
    "        }\n",
    "\n",
    "    print(\"prompt token\", token_count)\n",
    "    try:\n",
    "        merged_group =  group_mergeable_entities(llm_client, entities, **model_args)\n",
    "        cluster_idx = 0\n",
    "        for entities in merged_group:\n",
    "            cluster_idx += 1\n",
    "            new_cluster_name = f\"{cluster_name}_idx{cluster_idx}\"\n",
    "            for e in entities:\n",
    "                new_clusters_info.append(\n",
    "                    {\n",
    "                        'cluster': new_cluster_name,\n",
    "                        'entity_id': e.id,\n",
    "                        'entity_name': e.name,\n",
    "                        'entity_description': e.description,\n",
    "                        'entity_metadata': e.meta\n",
    "                    }\n",
    "                )\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing cluster {cluster_name}: {e}\", exc_info=True)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_clusters_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "cluster_info_df = pd.DataFrame(new_clusters_info)\n",
    "cluster_info_df['processed'] = False\n",
    "cluster_info_df.to_pickle(\"cluster_entities.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_info_df.count()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graph",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
